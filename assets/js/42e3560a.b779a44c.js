"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[75],{1504:(e,t,a)=>{a.d(t,{Z:()=>r});var n=a(7294),i=a(7273);i.Z.initialize({startOnLoad:!0});const r=e=>{let{chart:t}=e;return(0,n.useEffect)((()=>{i.Z.contentLoaded()}),[]),n.createElement("div",{className:"mermaid"},t)}},5040:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var n=a(7462),i=(a(7294),a(3905)),r=a(1504);const o={},l=void 0,s={unversionedId:"reference/Connectors/materialization-connectors/Rockset",id:"reference/Connectors/materialization-connectors/Rockset",title:"Rockset",description:"This Flow connector materializes delta updates of your Flow collections into Rockset collections.",source:"@site/docs/reference/Connectors/materialization-connectors/Rockset.md",sourceDirName:"reference/Connectors/materialization-connectors",slug:"/reference/Connectors/materialization-connectors/Rockset",permalink:"/reference/Connectors/materialization-connectors/Rockset",draft:!1,editUrl:"https://github.com/estuary/flow/edit/master/site/docs/reference/Connectors/materialization-connectors/Rockset.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"PostgreSQL",permalink:"/reference/Connectors/materialization-connectors/PostgreSQL"},next:{title:"Snowflake",permalink:"/reference/Connectors/materialization-connectors/Snowflake"}},c={},p=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Properties",id:"properties",level:3},{value:"Endpoint",id:"endpoint",level:4},{value:"Bindings",id:"bindings",level:4},{value:"Sample",id:"sample",level:3},{value:"Delta updates and reduction strategies",id:"delta-updates-and-reduction-strategies",level:2},{value:"Bulk ingestion for large backfills of historical data",id:"bulk-ingestion-for-large-backfills-of-historical-data",level:2},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"How to perform a bulk ingestion",id:"how-to-perform-a-bulk-ingestion",level:3},{value:"Changelog",id:"changelog",level:2},{value:"V2: 2022-12-06",id:"v2-2022-12-06",level:4}],m={toc:p};function d(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"This Flow connector materializes ",(0,i.kt)("a",{parentName:"p",href:"/concepts/materialization#delta-updates"},"delta updates")," of your Flow collections into Rockset collections."),(0,i.kt)("p",null,"It is available for use in the Flow web application. For local development or open-source workflows, ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/estuary/connectors/pkgs/container/materialize-rockset"},(0,i.kt)("inlineCode",{parentName:"a"},"ghcr.io/estuary/materialize-rockset:dev"))," provides the latest version of the connector as a Docker image. You can also follow the link in your browser to see past image versions."),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("p",null,"To use this connector, you'll need:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A Rockset ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/rest-api/#createapikey"},"API key generated"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"The API key must have the ",(0,i.kt)("strong",{parentName:"li"},"Member")," or ",(0,i.kt)("strong",{parentName:"li"},"Admin")," ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/iam/#users-api-keys-and-roles"},"role"),"."))),(0,i.kt)("li",{parentName:"ul"},"A Rockset workspace",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Optional; if none exist, one will be created by the connector."))),(0,i.kt)("li",{parentName:"ul"},"A Rockset collection",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Optional; if none exist, one will be created by the connector."))),(0,i.kt)("li",{parentName:"ul"},"At least one Flow collection")),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"If you haven't yet captured your data from its external source, start at the beginning of the ",(0,i.kt)("a",{parentName:"p",href:"/guides/create-dataflow"},"guide to create a dataflow"),". You'll be referred back to this connector-specific documentation at the appropriate steps."))),(0,i.kt)("h2",{id:"configuration"},"Configuration"),(0,i.kt)("p",null,"To use this connector, begin with data in one or more Flow collections.\nUse the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections."),(0,i.kt)("h3",{id:"properties"},"Properties"),(0,i.kt)("h4",{id:"endpoint"},"Endpoint"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Property"),(0,i.kt)("th",{parentName:"tr",align:null},"Title"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Required/Default"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("strong",{parentName:"td"},(0,i.kt)("inlineCode",{parentName:"strong"},"/api_key"))),(0,i.kt)("td",{parentName:"tr",align:null},"Rockset API Key"),(0,i.kt)("td",{parentName:"tr",align:null},"The key used to authenticate to the Rockset API. Must have role of admin or member."),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("strong",{parentName:"td"},(0,i.kt)("inlineCode",{parentName:"strong"},"/region_base_url"))),(0,i.kt)("td",{parentName:"tr",align:null},"Region Base URL"),(0,i.kt)("td",{parentName:"tr",align:null},"The base URL to connect to your Rockset deployment. Example: api.usw2a1.rockset.com (do not include the protocol). ",(0,i.kt)("a",{parentName:"td",href:"https://rockset.com/docs/rest-api/"},"See supported options and how to find yours"),"."),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")))),(0,i.kt)("h4",{id:"bindings"},"Bindings"),(0,i.kt)("p",null,"The binding configuration for this connector includes two optional sections.\n",(0,i.kt)("strong",{parentName:"p"},"Backfill from S3")," allows you to backfill historical data from an S3 bucket, ",(0,i.kt)("a",{parentName:"p",href:"#bulk-ingestion-for-large-backfills-of-historical-data"},"as detailed below"),".\n",(0,i.kt)("strong",{parentName:"p"},"Advanced collection settings")," includes settings that may help optimize your resulting Rockset collections:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Clustering fields"),": You can specify clustering fields\nfor your Rockset collection's columnar index to help optimize specific query patterns.\nSee the ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/query-composition/#data-clustering"},"Rockset docs")," for more information."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Retention period"),": Amount of time before data is purged, in seconds.\nA low value will keep the amount of data indexed in Rockset smaller.")),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Property"),(0,i.kt)("th",{parentName:"tr",align:null},"Title"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Required/Default"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/advancedCollectionSettings")),(0,i.kt)("td",{parentName:"tr",align:null},"Advanced Collection Settings"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/advancedCollectionSettings/clustering_key")),(0,i.kt)("td",{parentName:"tr",align:null},"Clustering Key"),(0,i.kt)("td",{parentName:"tr",align:null},"List of clustering fields"),(0,i.kt)("td",{parentName:"tr",align:null},"array"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("em",{parentName:"td"},(0,i.kt)("inlineCode",{parentName:"em"},"/advancedCollectionSettings/clustering_key/-/field_name"))),(0,i.kt)("td",{parentName:"tr",align:null},"Field Name"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of a field"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/advancedCollectionSettings/retention_secs")),(0,i.kt)("td",{parentName:"tr",align:null},"Retention Period"),(0,i.kt)("td",{parentName:"tr",align:null},"Number of seconds after which data is purged based on event time"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("strong",{parentName:"td"},(0,i.kt)("inlineCode",{parentName:"strong"},"/collection"))),(0,i.kt)("td",{parentName:"tr",align:null},"Rockset Collection"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of the Rockset collection (will be created if it does not exist)"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3")),(0,i.kt)("td",{parentName:"tr",align:null},"Backfill from S3"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3/bucket")),(0,i.kt)("td",{parentName:"tr",align:null},"Bucket"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of the S3 bucket to load data from."),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3/integration")),(0,i.kt)("td",{parentName:"tr",align:null},"Integration Name"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of the integration that was previously created in the Rockset UI"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3/pattern")),(0,i.kt)("td",{parentName:"tr",align:null},"Pattern"),(0,i.kt)("td",{parentName:"tr",align:null},"A regex that is used to match objects to be ingested"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3/prefix")),(0,i.kt)("td",{parentName:"tr",align:null},"Prefix"),(0,i.kt)("td",{parentName:"tr",align:null},"Prefix of the data within the S3 bucket. All files under this prefix will be loaded. Optional. Must not be set if ","'","pattern","'"," is defined."),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"/initializeFromS3/region")),(0,i.kt)("td",{parentName:"tr",align:null},"Region"),(0,i.kt)("td",{parentName:"tr",align:null},"The AWS region in which the bucket resides. Optional."),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("strong",{parentName:"td"},(0,i.kt)("inlineCode",{parentName:"strong"},"/workspace"))),(0,i.kt)("td",{parentName:"tr",align:null},"Workspace"),(0,i.kt)("td",{parentName:"tr",align:null},"The name of the Rockset workspace (will be created if it does not exist)"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"Required")))),(0,i.kt)("h3",{id:"sample"},"Sample"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  ${PREFIX}/${mat_name}:\n      endpoint:\n      connector:\n            config:\n               region_base_url: api.usw2a1.rockset.com\n               api_key: supersecret\n            # Path to the latest version of the connector, provided as a Docker image\n            image: ghcr.io/estuary/materialize-rockset:dev\n    # If you have multiple collections you need to materialize, add a binding for each one\n    # to ensure complete data flow-through\n    bindings:\n    - resource:\n        workspace: ${namespace_name}\n        collection: ${table_name}\n    source: ${PREFIX}/${source_collection}\n")),(0,i.kt)("h2",{id:"delta-updates-and-reduction-strategies"},"Delta updates and reduction strategies"),(0,i.kt)("p",null,"The Rockset connector operates only in ",(0,i.kt)("a",{parentName:"p",href:"/concepts/materialization#delta-updates"},"delta updates")," mode.\nThis means that Rockset, rather than Flow, performs the document merge.\nIn some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates."),(0,i.kt)("p",null,"Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of ",(0,i.kt)("a",{parentName:"p",href:"https://datatracker.ietf.org/doc/html/rfc7396"},"RFC 7396 - JSON merge"),".\nThis differs from how Flow would reduce documents, most notably in that Rockset will ",(0,i.kt)("em",{parentName:"p"},"not")," honor any reduction strategies defined in your Flow schema.\nFor consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations\nin Flow mirror Rockset's semantics."),(0,i.kt)("p",null,"To accomplish this, ensure that your collection schema has the following ",(0,i.kt)("a",{parentName:"p",href:"/concepts/schemas#reductions"},"data reductions")," defined in its schema:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A top-level reduction strategy of ",(0,i.kt)("a",{parentName:"li",href:"/reference/reduction-strategies/merge"},"merge")),(0,i.kt)("li",{parentName:"ul"},"A strategy of ",(0,i.kt)("a",{parentName:"li",href:"/reference/reduction-strategies/firstwritewins-and-lastwritewins"},"lastWriteWins")," for all nested values (this is the default)")),(0,i.kt)("h2",{id:"bulk-ingestion-for-large-backfills-of-historical-data"},"Bulk ingestion for large backfills of historical data"),(0,i.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"Beta")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"This feature is currently being updated as part of Flow's beta release,\nand may not work as expected at this time. If you require this feature,\n",(0,i.kt)("a",{parentName:"p",href:"mailto:support@estuary.dev"},"contact the Estuary team"),"."))),(0,i.kt)("p",null,"You can backfill large amounts of historical data into Rockset using a ",(0,i.kt)("em",{parentName:"p"},"bulk ingestion"),". Bulk ingestion must originate in S3 and requires additional steps in your dataflow.\nThis workflow is supported using the ",(0,i.kt)("a",{parentName:"p",href:"/concepts/flowctl"},"flowctl")," CLI."),(0,i.kt)("h3",{id:"prerequisites-1"},"Prerequisites"),(0,i.kt)("p",null,"This is an intermediate workflow: after you use the web app's GUI to create the initial materialization, you must manually\nupdate the ",(0,i.kt)("a",{parentName:"p",href:"/concepts/materialization#specification"},"materialization specification"),"."),(0,i.kt)("p",null,"To do this, you'll need :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A local ",(0,i.kt)("a",{parentName:"li",href:"/concepts/flowctl#installation-and-setup"},"installation of ",(0,i.kt)("inlineCode",{parentName:"a"},"flowctl")),"."),(0,i.kt)("li",{parentName:"ul"},"Familiarity with ",(0,i.kt)("a",{parentName:"li",href:"/concepts/flowctl#working-with-drafts"},"working with drafts in flowctl"),". If you're not familiar, read through ",(0,i.kt)("a",{parentName:"li",href:"/guides/create-derivation"},"this tutorial")," as a guide.")),(0,i.kt)("h3",{id:"how-to-perform-a-bulk-ingestion"},"How to perform a bulk ingestion"),(0,i.kt)("p",null,"A bulk ingestion from a Flow collection into Rockset is essentially a two-step process. First, Flow writes your historical data into an S3 bucket using Estuary's ",(0,i.kt)("a",{parentName:"p",href:"/reference/Connectors/materialization-connectors/Parquet"},"S3-Parquet materialization")," connector. Once the data is caught up, it uses the Rockset connector to backfill the data from S3 into Rockset and then switches to the Rockset Write API for the continuous materialization of new data."),(0,i.kt)(r.Z,{chart:"\n\tgraph TD\n    A[Create an S3 integration in Rockset] --\x3e B\n    B[Create Flow materialization into S3 bucket] --\x3e C\n    C[Wait for S3 materialization to catch up with historical data] --\x3e|When ready to bulk ingest into Rockset| D\n    D[Disable S3 materialization shards] --\x3e E\n    E[Update same materialization to use the Rockset connector with the integration created in first step] --\x3e F\n    F[Rockset connector automatically continues materializing after the bulk ingestion completes]\n",mdxType:"Mermaid"}),(0,i.kt)("p",null,"To set this up, use the following procedure as a guide, substituting ",(0,i.kt)("inlineCode",{parentName:"p"},"example/flow/collection")," for your collection:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"You'll need an ",(0,i.kt)("a",{parentName:"p",href:"https://rockset.com/docs/amazon-s3/"},"S3 integration")," in Rockset. To create one, follow the ",(0,i.kt)("a",{parentName:"p",href:"https://rockset.com/docs/amazon-s3/#create-an-s3-integration"},"instructions here"),", but ",(0,i.kt)("em",{parentName:"p"},"do not create the Rockset collection yet"),".")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"In the Flow web app, use the ",(0,i.kt)("a",{parentName:"p",href:"/reference/Connectors/materialization-connectors/Parquet"},"S3-Parquet materialization")," to create and publish a materialization of ",(0,i.kt)("inlineCode",{parentName:"p"},"example/flow/collection")," into a unique prefix within an S3 bucket of your choosing.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"On the ",(0,i.kt)("strong",{parentName:"p"},"Materializations")," tab, note the full name of the materialization; for example ",(0,i.kt)("inlineCode",{parentName:"p"},"myOrg/example/toRockset"),".")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Switch to flowctl. Create a new draft and add the materialization into your draft."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-console"},"flowctl draft create\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-console"},"flowctl catalog draft --name myOrg/example/toRockset\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Pull the draft locally."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"flowctl draft develop\n")),(0,i.kt)("p",{parentName:"li"}," In a directory that shares the name of your materialization, you'll find a file called ",(0,i.kt)("inlineCode",{parentName:"p"},"flow.yaml"),".\nOpen it in your preferred editor."),(0,i.kt)("p",{parentName:"li"}," It will look similar to:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-s3-parquet:dev\n        config:\n          bucket: example-s3-bucket\n          region: us-east-1\n          awsAccessKeyId: <your key>\n          awsSecretAccessKey: <your secret>\n          uploadIntervalInSeconds: 300\n    bindings:\n      - resource:\n          pathPrefix: example/s3-prefix/\n        source: example/flow/collection\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"By now, this  S3 materialization has caught up with your historical data, so you'll switch to the Rockset write API for your future data."),(0,i.kt)("p",{parentName:"li"}," To make the switch, first disable the S3 materialization by setting shards to disabled in the definition. This is necessary to ensure correct ordering of documents written to Rockset."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    shards:\n      disable: true\n    # ...the remainder of the materialization yaml remains the same as above\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Re-deploy the materialization."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-console"},"flowctl draft author --source flow.yaml\n")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-console"},"flowctl draft publish\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Back in your local editor, update the materialization to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"materialize-rockset")," connector, and re-enable the shards. Here you'll provide the name of the Rockset S3 integration you created above, as well as the bucket and prefix that you previously materialized into. ",(0,i.kt)("strong",{parentName:"p"},"It's critical that the name of the materialization remains the same as it was for materializing into S3.")),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-rockset:dev\n        config:\n           region_base_url: api.usw2a1.rockset.com\n           api_key: <your rockset API key here>\n    bindings:\n      - resource:\n          workspace: <your rockset workspace name>\n          collection: <your rockset collection name>\n          initializeFromS3:\n            integration: <rockset integration name>\n            bucket: example-s3-bucket\n            region: us-east-1\n            prefix: example/s3-prefix/\n        source: example/flow/collection\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Re-deploy the materialization as described in step 7."),(0,i.kt)("p",{parentName:"li"},"When you activate the new materialization, the connector will create the Rockset collection using the given integration, and wait for it to ingest all of the historical data from S3 before it continues. Once this completes, the Rockset connector will automatically switch over to the incoming stream of new data."))),(0,i.kt)("h2",{id:"changelog"},"Changelog"),(0,i.kt)("p",null,"The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Proceed with caution when editing materializations created with previous versions of this connector;\nediting always upgrades your materialization to the latest connector version.")),(0,i.kt)("h4",{id:"v2-2022-12-06"},"V2: 2022-12-06"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Region Base URL was added and is now required as part of the endpoint configuration."),(0,i.kt)("li",{parentName:"ul"},"Event Time fields and the Insert Only option were removed from the advanced collection settings.")))}d.isMDXComponent=!0}}]);