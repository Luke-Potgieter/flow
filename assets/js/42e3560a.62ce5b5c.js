"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[75],{1504:function(e,t,a){var n=a(7294),o=a(7273);o.Z.initialize({startOnLoad:!0});t.Z=function(e){var t=e.chart;return(0,n.useEffect)((function(){o.Z.contentLoaded()}),[]),n.createElement("div",{className:"mermaid"},t)}},5040:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return c},metadata:function(){return u},toc:function(){return m},default:function(){return k}});var n=a(7462),o=a(3366),i=(a(7294),a(3905)),r=a(1504),l=["components"],s={},c=void 0,u={unversionedId:"reference/Connectors/materialization-connectors/Rockset",id:"reference/Connectors/materialization-connectors/Rockset",title:"Rockset",description:"This Flow connector materializes delta updates of your Flow collections into Rockset collections.",source:"@site/docs/reference/Connectors/materialization-connectors/Rockset.md",sourceDirName:"reference/Connectors/materialization-connectors",slug:"/reference/Connectors/materialization-connectors/Rockset",permalink:"/reference/Connectors/materialization-connectors/Rockset",editUrl:"https://github.com/estuary/flow/edit/master/site/docs/reference/Connectors/materialization-connectors/Rockset.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Materialization connectors",permalink:"/reference/Connectors/materialization-connectors/"}},m=[{value:"Prerequisites",id:"prerequisites",children:[],level:2},{value:"Configuration",id:"configuration",children:[{value:"Values",id:"values",children:[],level:3},{value:"Sample",id:"sample",children:[],level:3}],level:2},{value:"Bulk ingestion for large backfills of historical data",id:"bulk-ingestion-for-large-backfills-of-historical-data",children:[{value:"Prerequisites",id:"prerequisites-1",children:[],level:3},{value:"How to perform a bulk ingestion",id:"how-to-perform-a-bulk-ingestion",children:[],level:3}],level:2}],p={toc:m};function k(e){var t=e.components,a=(0,o.Z)(e,l);return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"This Flow connector materializes ",(0,i.kt)("a",{parentName:"p",href:"/concepts/materialization#how-materializations-work-"},"delta updates")," of your Flow collections into Rockset collections."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"ghcr.io/estuary/materialize-rockset:dev")," provides the latest connector image when using the Flow GitOps environment. You can also follow the link in your browser to see past image versions."),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("p",null,"To use this connector, you'll need :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"An existing catalog spec that includes at least one collection with its schema specified"),(0,i.kt)("li",{parentName:"ul"},"A Rockset account with an ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/rest-api/#createapikey"},"API key generated")," from the web UI"),(0,i.kt)("li",{parentName:"ul"},"A Rockset workspace",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Optional; if none exist, one will be created by the connector."))),(0,i.kt)("li",{parentName:"ul"},"A Rockset collection",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Optional; if none exist, one will be created by the connector.")))),(0,i.kt)("h2",{id:"configuration"},"Configuration"),(0,i.kt)("p",null,"To use this connector, begin with a Flow catalog that has at least one ",(0,i.kt)("strong",{parentName:"p"},"collection"),". You'll add a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections. Follow the basic ",(0,i.kt)("a",{parentName:"p",href:"/concepts/materialization"},"materialization setup")," and add the required Rockset configuration values per the table below."),(0,i.kt)("h3",{id:"values"},"Values"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Value"),(0,i.kt)("th",{parentName:"tr",align:null},"Name"),(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Required/Default"),(0,i.kt)("th",{parentName:"tr",align:null},"Details"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"api_key")),(0,i.kt)("td",{parentName:"tr",align:null},"API Key"),(0,i.kt)("td",{parentName:"tr",align:null},"String"),(0,i.kt)("td",{parentName:"tr",align:null},"Required"),(0,i.kt)("td",{parentName:"tr",align:null},"Rockset API key generated from the web UI.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"HttpLogging")),(0,i.kt)("td",{parentName:"tr",align:null},"HTTP Logging"),(0,i.kt)("td",{parentName:"tr",align:null},"bool"),(0,i.kt)("td",{parentName:"tr",align:null},"false"),(0,i.kt)("td",{parentName:"tr",align:null},"Enable verbose logging of the HTTP calls to the Rockset API")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"MaxConcurrentRequests")),(0,i.kt)("td",{parentName:"tr",align:null},"Maximum Concurrent Requests"),(0,i.kt)("td",{parentName:"tr",align:null},"int"),(0,i.kt)("td",{parentName:"tr",align:null},"1"),(0,i.kt)("td",{parentName:"tr",align:null},"The upper limit on how many concurrent requests will be sent to Rockset.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"workspace")),(0,i.kt)("td",{parentName:"tr",align:null},"Workspace"),(0,i.kt)("td",{parentName:"tr",align:null},"String"),(0,i.kt)("td",{parentName:"tr",align:null},"Required"),(0,i.kt)("td",{parentName:"tr",align:null},"For each binding, name of the Rockset workspace")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"collection")),(0,i.kt)("td",{parentName:"tr",align:null},"Rockset collection"),(0,i.kt)("td",{parentName:"tr",align:null},"String"),(0,i.kt)("td",{parentName:"tr",align:null},"Required"),(0,i.kt)("td",{parentName:"tr",align:null},"For each binding, the name of the destination Rockset table")))),(0,i.kt)("h3",{id:"sample"},"Sample"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"# If this is the first materialization, add the section to your catalog spec\nmaterializations:\n  ${tenant}/${mat_name}:\n      endpoint:\n      connector:\n            config:\n               api_key: supersecret\n            # Path to the latest version of the connector, provided as a Docker image\n            image: ghcr.io/estuary/materialize-rockset:dev\n    # If you have multiple collections you need to materialize, add a binding for each one\n    # to ensure complete data flow-through\n    bindings:\n    - resource:\n        workspace: ${namespace_name}\n        collection: ${table_name}\n    source: ${tenant}/${source_collection}\n")),(0,i.kt)("h2",{id:"bulk-ingestion-for-large-backfills-of-historical-data"},"Bulk ingestion for large backfills of historical data"),(0,i.kt)("p",null,"You can backfill large amounts of historical data into Rockset using a ",(0,i.kt)("em",{parentName:"p"},"bulk ingestion"),". Bulk ingestion must originate in S3 and requires additional steps in your dataflow. Flow's Rockset connector supports this through the GitOps workflow."),(0,i.kt)("h3",{id:"prerequisites-1"},"Prerequisites"),(0,i.kt)("p",null,"Before completing this workflow, make sure you have:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A working catalog spec including at least one Flow collection."),(0,i.kt)("li",{parentName:"ul"},"A production or ",(0,i.kt)("a",{parentName:"li",href:"/getting-started/installation"},"development")," environment\n::: tip\nThe following is an intermediate workflow. As needed, refer to ",(0,i.kt)("a",{parentName:"li",href:"/guides/create-dataflow"},"this guide")," for the basic steps to create and deploy a catalog spec using the GitOps workflow.\n:::")),(0,i.kt)("h3",{id:"how-to-perform-a-bulk-ingestion"},"How to perform a bulk ingestion"),(0,i.kt)("p",null,"A bulk ingestion from a Flow collection into Rockset is essentially a two-step process. First, Flow writes your historical data into an S3 bucket using Estuary's ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/estuary/connectors/pkgs/container/materialize-s3-parquet"},"materialize-s3-parquet")," connector. Once the data is caught up, it uses the Rockset connector to backfill the data from S3 into Rockset and then switch to the Rockset Write API for the continuous materialization of new data."),(0,i.kt)(r.Z,{chart:"\n\tgraph TD\n    A[Create an S3 integration in Rockset] --\x3e B\n    B[Create Flow materialization into S3 bucket] --\x3e C\n    C[Wait for S3 materialization to catch up with historical data] --\x3e|When ready to bulk ingest into Rockset| D\n    D[Disable S3 materialization shards] --\x3e E\n    E[Update same materialization to use the Rockset connector with the integration created in first step] --\x3e F\n    F[Rockset connector automatically continues materializing after the bulk ingestion completes]\n",mdxType:"Mermaid"}),(0,i.kt)("p",null,"To set this up, use the following procedure as a guide, substituting ",(0,i.kt)("inlineCode",{parentName:"p"},"example/flow/collection")," for your collection:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"You'll need an ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/amazon-s3/"},"S3 integration")," in Rockset. To create one, follow the ",(0,i.kt)("a",{parentName:"li",href:"https://rockset.com/docs/amazon-s3/#create-an-s3-integration"},"instructions here"),", but ",(0,i.kt)("em",{parentName:"li"},"do not create the Rockset collection yet"),"."),(0,i.kt)("li",{parentName:"ol"},"Create and activate a materialization of ",(0,i.kt)("inlineCode",{parentName:"li"},"example/flow/collection")," into a unique prefix within an S3 bucket of your choosing.",(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-s3-parquet:dev\n        config:\n          bucket: example-s3-bucket\n          region: us-east-1\n          awsAccessKeyId: <your key>\n          awsSecretAccessKey: <your secret>\n          uploadIntervalInSeconds: 300\n    bindings:\n      - resource:\n          pathPrefix: example/s3-prefix/\n        source: example/flow/collection\n"))),(0,i.kt)("li",{parentName:"ol"},"Once the  S3 materialization is caught up with your historical data, you'll switch to the Rockset write API for your future data. To make the switch, first disable the S3 materialization by setting shards to disabled in the definition, and re-deploy the catalog. This is necessary to ensure correct ordering of documents written to Rockset.",(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    shards:\n      disable: true\n    # ...the remainder of the materialization yaml remains the same as above\n"))),(0,i.kt)("li",{parentName:"ol"},"Update the materialization to use the ",(0,i.kt)("inlineCode",{parentName:"li"},"materialize-rockset")," connector, and re-enable the shards. Here you'll provide the name of the Rockset S3 integration you created above, as well as the bucket and prefix that you previously materialized into. ",(0,i.kt)("strong",{parentName:"li"},"It's critical that the name of the materialization remains the same as it was for materializing into S3."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-yaml"},"materializations:\n  example/toRockset:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-rockset:dev\n        config:\n          api_key: <your rockset API key here>\n          max_concurrent_requests: 5\n    bindings:\n      - resource:\n          workspace: <your rockset workspace name>\n          collection: <your rockset collection name>\n          initializeFromS3:\n            integration: <rockset integration name>\n            bucket: example-s3-bucket\n            region: us-east-1\n            prefix: example/s3-prefix/\n        source: example/flow/collection\n"))),(0,i.kt)("li",{parentName:"ol"},"When you activate the new materialization, the connector will create the Rockset collection using the given integration, and wait for it to ingest all of the historical data from S3 before it continues. Once this completes, the Rockset connector will automatically switch over to the incoming stream of new data.")))}k.isMDXComponent=!0}}]);