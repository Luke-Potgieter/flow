[{"title":"Concepts","type":0,"sectionRef":"#","url":"concepts/","content":"","keywords":""},{"title":"Working with Flow​","type":1,"pageTitle":"Concepts","url":"concepts/#working-with-flow","content":"There are two main environments in which you can work with Flow: the web application or your preferred local environment using the flowctl command-line tool. "},{"title":"Web application​","type":1,"pageTitle":"Concepts","url":"concepts/#web-application","content":"The Flow web application is where you'll most likely create, monitor, and update your Data Flows. You can find it at dashboard.estuary.dev. The application is designed for intuitive use, and only requires a few basic concepts to get started. For a complete walk-through, see the guide to create a dataflow. The app is backed by secure, cloud-hosted infrastructure that Estuary manages. "},{"title":"flowctl​","type":1,"pageTitle":"Concepts","url":"concepts/#flowctl","content":"flowctl is a command-line interface for working with Flow's public API. Using flowctl, developers can inspect, edit, test, and publish Data Flows — just as with the web application. For example, you can create a Data Flow with the web app, and then use flowctl to fetch it into files that you manage within a Git repo. Learn more about flowctl "},{"title":"Essential concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#essential-concepts","content":"In this section, you'll find the most important Flow terms and concepts. If you're new to Flow, start here. A complete end-to-end Data Flow between two systems has three components: Capture: Flow ingests data from an outside source.Collections: Flow maintains the captured data in cloud storage.Materialization: Flow pushes data to an outside destination. graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; It may also include: Derivations: You apply a transformation to data in a collection, resulting in a new collection. All of these entities are described in the catalog. "},{"title":"Catalog​","type":1,"pageTitle":"Concepts","url":"concepts/#catalog","content":"The catalog is the set of active entities that comprise all Data Flows: captures, materializations, derivations, collections, schemas, tests, and more. All catalog entities are defined in Flow specification files — either manually, by you, or generated by the Flow web app. You create the specifications as drafts, and publish them to add them to the catalog. You can mix and match catalog entities to create a variety of Data Flows. Learn more about the catalog  "},{"title":"Collections​","type":1,"pageTitle":"Concepts","url":"concepts/#collections","content":"Collections represent datasets within Flow. All captured documents are written to a collection, and all materialized documents are read from a collection. Collections are a real-time data lake. Like a traditional data lake, the documents that make up a collection are stored as plain JSON in your cloud storage bucket. Unlike a traditional data lake, updates to the collection are reflected downstream in the data flow within milliseconds. Documents in collections are stored indefinitely in your cloud storage bucket (or may be managed with your regular bucket lifecycle policies). This means that the full historical content of a collection is available to support future data operations and perform backfills without going back to the source. Each collection has a keyed schema against which incoming documents are validated. This ensures that data is always clean and organized. Learn more about collections  "},{"title":"Captures​","type":1,"pageTitle":"Concepts","url":"concepts/#captures","content":"A capture is a Flow task that ingests data from an external source into one or more Flow collections. Documents continuously move from the source into Flow; as new documents become available at the source, Flow validates their schema and adds them to their corresponding collection. Captures interface with source systems using connectors. Learn more about captures  "},{"title":"Materializations​","type":1,"pageTitle":"Concepts","url":"concepts/#materializations","content":"A materialization is a Flow task that pushes data from one or more collections to an external destination. Documents continuously moves from each Flow collection to the destination. Materializations are the conceptual inverse of captures. As new documents become available within bound collections, the materialization keeps the destination up to date within milliseconds, or as fast as that system allows. Materializations interface with destinations using connectors. Learn more about materializations  "},{"title":"Endpoints​","type":1,"pageTitle":"Concepts","url":"concepts/#endpoints","content":"Endpoints are the source systems from which Flow captures data and the destination systems to which Flow materializes data. All kinds of data systems can be endpoints, including databases, key/value stores, streaming pub/sub systems, SaaS products, and cloud storage locations. Flow connects to this wide variety of endpoints using connectors.  "},{"title":"Connectors​","type":1,"pageTitle":"Concepts","url":"concepts/#connectors","content":"Connectors are plugin components that allow Flow to interface with endpoint data systems. They power captures and materializations. Flow uses an open-source connector model. Many connectors are made by Estuary, and others are made by third parties. Because connectors are open-source and kept separate from Flow itself, new integrations can be added and updated quickly. This is important, as the landscape of data systems and platforms is constantly evolving. All currently supported connectors are ready to use in the Flow web application. They're also available as Docker images, each encapsulating the details of working with a particular source or destination system. Learn more about connectors  "},{"title":"Intermediate concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#intermediate-concepts","content":"In this section, you'll find important concepts that are optional for basic usage. Read this to unlock more powerful workflows. "},{"title":"Derivations​","type":1,"pageTitle":"Concepts","url":"concepts/#derivations","content":"A derivation is a collection that results from the transformation of one or more other collections, which is continuously updated in sync with its source collection(s). You can use derivations to map, reshape, and filter documents. They can also be used to tackle complex stateful streaming workflows, including joins and aggregations, without windowing and scaling limitations. Learn more about derivations  "},{"title":"Schemas​","type":1,"pageTitle":"Concepts","url":"concepts/#schemas","content":"All collections in Flow have an associatedJSON schemaagainst which documents are validated every time they're written or read. Schemas are critical to how Flow ensures the integrity of your data. Flow validates your documents to ensure that bad data doesn't make it into your collections — or worse, into downstream data products! JSON schema is a flexible standard for representing structure, invariants, and other constraints over your documents. Schemas can be very permissive, highly exacting, or somewhere in between. Flow pauses catalog tasks when documents don't match the collection schema, alerting you to the mismatch and allowing you to fix it before it creates a bigger problem. Learn more about schemas  "},{"title":"Reductions​","type":1,"pageTitle":"Concepts","url":"concepts/#reductions","content":"Every Flow collection schema includes a key. The key is used to identify collection documents and determine how they are grouped. When a collection is materialized into a database table, for example, its key becomes the SQL primary key of the materialized table. Flow also uses the key to reduce documents in collections, making storage and materializations more efficient. If multiple documents of a given key are added to a collection, by default, the most recent document supersedes all previous documents of that key. You can exert more control over your data by changing the default reduction strategy. By doing so, you can deeply merge documents, maintain running counts, and achieve other complex aggregation behaviors. Learn more about reductions  "},{"title":"Tests​","type":1,"pageTitle":"Concepts","url":"concepts/#tests","content":"Tests become an important part of your Data Flows when you add derivations and customized reduction behavior. You use tests to verify the end-to-end behavior of your collections and derivations. A test is a sequence of ingestion or verification steps. Ingestion steps ingest one or more document fixtures into a collection, and verification steps assert that the contents of another derived collection match a test expectation. Learn more about tests  "},{"title":"Tasks​","type":1,"pageTitle":"Concepts","url":"concepts/#tasks","content":"Captures, derivations, and materializations are collectively referred to as catalog tasks. They are the &quot;active&quot; components of a Data Flow, each running continuously and reacting to documents as they become available. Collections, by way of comparison, are inert. They reflect data at rest, and are acted upon by catalog tasks: A capture adds documents to a collection pulled from a source endpoint.A derivation updates a collection by applying transformations to other collections.A materialization reacts to changes of a collection to update a destination endpoint.  "},{"title":"Resources and bindings​","type":1,"pageTitle":"Concepts","url":"concepts/#resources-and-bindings","content":"A resource is an addressable collection of data within a source or destination system. The exact meaning of a resource is up to the endpoint and its connector. For example: Resources of a database endpoint might be its individual tables.Resources of a Kafka cluster might be its topics.Resources of a SaaS connector might be its various API feeds. When you create capture or materialization, it connects a collection to a resource through a binding. A given capture or materialization may have multiple bindings, which connect multiple collections to different resources.  "},{"title":"Storage mappings​","type":1,"pageTitle":"Concepts","url":"concepts/#storage-mappings","content":"Flow collections use cloud storage buckets for the durable storage of data. Storage mappings define how Flow maps your various collections into your storage buckets and prefixes. Learn more about storage mappings "},{"title":"Advanced concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#advanced-concepts","content":"This section discusses advanced Flow concepts. The information here unlocks a more technical understanding of how Flow works, and may be helpful in advanced use cases. "},{"title":"Journals​","type":1,"pageTitle":"Concepts","url":"concepts/#journals","content":"Journals provide the low-level storage for Flow collections. Each logical and physical partition of a collection is backed by a journal. Task shards also use journals to provide for their durability and fault tolerance. Each shard has an associated recovery log, which is a journal into which internal checkpoint states are written. Learn more about journals "},{"title":"Task shards​","type":1,"pageTitle":"Concepts","url":"concepts/#task-shards","content":"Task shards are the unit of execution for a catalog task. A single task can have many shards, which allow the task to scale across many machines to achieve more throughput and parallelism. Shards are created and managed by the Flow runtime. Each shard represents a slice of the overall work of the catalog task, including its processing status and associated internal checkpoints. Catalog tasks are created with a single shard, which can be repeatedly subdivided at any time — with no downtime — to increase the processing capacity of the task. Learn more about shards "},{"title":"Projections​","type":1,"pageTitle":"Concepts","url":"concepts/#projections","content":"Flow leverages your JSON schemas to produce other types of schemas as needed, such as TypeScript types and SQL CREATE TABLE statements. In many cases these projections provide comprehensive end-to-end type safety of Data Flows and their TypeScript transformations, all statically verified at build time. Learn more about projections "},{"title":"Journals","type":0,"sectionRef":"#","url":"concepts/advanced/journals/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#specification","content":"Flow collections can control some aspects of how their contents are mapped into journals through the journals stanza: collections: acmeCo/orders: schema: orders.schema.yaml key: [/id] journals: # Configuration for journal fragments. # Required, type: object. fragments: # Codec used to compress fragment files. # One of ZSTANDARD, SNAPPY, GZIP, or NONE. # Optional. Default is GZIP. compressionCodec: GZIP # Maximum flush delay before in-progress fragment buffers are closed # and persisted. Default uses no flush interval. # Optional. Given as a time duration. flushInterval: 15m # Desired content length of each fragment, in megabytes before compression. # Default is 512MB. # Optional, type: integer. length: 512 # Duration for which historical files of the collection should be kept. # Default is forever. # Optional. Given as a time duration. retention: 720h  Your storage mappings determine which of your cloud storage buckets is used for storage of collection fragment files. "},{"title":"Physical partitions​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#physical-partitions","content":"Every logical partition of a Flow collection is created with a single physical partition. Later and as required, new physical partitions are added in order to increase the write throughput of the collection. Each physical partition is responsible for all new writes covering a range of keys occurring in collection documents. Conceptually, if keys range from [A-Z] then one partition might cover [A-F] while another covers [G-Z]. The pivot of a partition reflects the first key in its covered range. One physical partition is turned into more partitions by subdividing its range of key ownership. For instance, a partition covering [A-F]is split into partitions [A-C] and [D-F]. Physical partitions are journals. The relationship between the journal and its specific collection and logical partition are encoded withinits journal specification. "},{"title":"Fragment files​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#fragment-files","content":"Journal fragment files each hold a slice of your collection's content, stored as a compressed file of newline-delimited JSON documents in your cloud storage bucket. Files are flushed to cloud storage periodically, typically after they reach a desired size threshold. They use a content-addressed naming scheme which allows Flow to understand how each file stitches into the overall journal. Consider a fragment file path like: s3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/0000000000000000-00000000201a3f27-1ec69e2de187b7720fb864a8cd6d50bb69cc7f26.gz This path has the following components: Component\tExampleStorage prefix of physical partition\ts3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/ Supplemental time pseudo-partitions\tutc_date=2022-01-07/utc_hour=19/ Beginning content offset\t0000000000000000 Ending content offset\t00000000201a3f27 SHA content checksum\t1ec69e2de187b7720fb864a8cd6d50bb69cc7f26 Compression codec\t.gz The supplemental time pseudo-partitions are not logical partitions, but are added to each fragment file path to facilitate integration with external tools that understand Hive layouts. "},{"title":"Hive layouts​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#hive-layouts","content":"As we've seen, collection fragment files are written to cloud storage with path components like/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/. If you've used tools within the Apache Hive ecosystem, this layout should feel familiar. Flow organizes files in this way to make them directly usable by tools that understand Hive partitioning, like Spark and Hive itself. Collections can also be integrated as Hive-compatible external tables in tools likeSnowflakeandBigQueryfor ad-hoc analysis. SQL queries against these tables can even utilize predicate push-down, taking query predicates over category, utc_date, and utc_hourand pushing them down into the selection of files that must be read to answer the query — often offering much faster and more efficient query execution because far less data must be read. "},{"title":"Logs and statistics","type":0,"sectionRef":"#","url":"concepts/advanced/logs-stats/","content":"","keywords":""},{"title":"Logs​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#logs","content":"Each organization that uses Flow has a logs collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/logs. These can be thought of as standard application logs: they store information about events that occur at runtime. They’re distinct from recovery logs, which track the state of various task shards. Regardless of how many Flow catalogs your organization has, all logs are stored in the same collection, which is read-only and logically partitioned on tasks. Logs are collected from events that occur within the Flow runtime, as well as the capture and materialization connectors your catalog is using. "},{"title":"Log level​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#log-level","content":"You can set the log level for each catalog task to control the level of detail at which logs are collected for that task. The available levels, listed from least to most detailed, are: error: Non-recoverable errors from the Flow runtime or connector that are critical to know aboutwarn: Errors that can be re-tried, but likely require investigationinfo: Task lifecycle events, or information you might want to collect on an ongoing basisdebug: Details that will help debug an issue with a tasktrace: Maximum level of detail that may yield gigabytes of logs The default log level is info. You can change a task’s log level by adding the shards keyword to its definition in the catalog spec: materializations: acmeCo/debugMaterialization: shards: logLevel: debug endpoint: {}  To learn more about working with logs and statistics, see their reference documentation. "},{"title":"Task shards","type":0,"sectionRef":"#","url":"concepts/advanced/shards/","content":"","keywords":""},{"title":"Shard splits​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#shard-splits","content":"When a task is first created, it is initialized with a single shard. Later and as required, shards may be split into two shards. This is done by the service operator on your behalf, depending on the size of your task. Shard splitting doesn't require downtime; your task will continue to run as normal on the old shard until the split occurs and then shift seamlessly to the new, split shards. This process can be repeated as needed until your required throughput is achieved. If you have questions about how shards are split for your tasks, contact your Estuary account representative. "},{"title":"Transactions​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#transactions","content":"Shards process messages in dynamic transactions. Whenever a message is ready to be processed by the task (when new documents appear at the source endpoint or collection), a new transaction is initiated. The transaction will continue so long as further messages are available for processing. When no more messages are immediately available, the transaction closes. A new transaction is started whenever the next message is available. In general, shorter transaction durations decrease latency, while longer transaction durations increase efficiency. Flow automatically balances these two extremes to optimize each task, but it may be useful in some cases to control transaction duration. For example, materializations to large analytical warehouses may benefit from longer transactions, which can reduce cost by performing more data reduction before landing data in the warehouse. Some endpoint systems, like BigQuery, limit the number of table operations you can perform. Longer transaction durations ensure that you don't exceed these limits. You can set the minimum and maximum transaction duration in a task's shards configuration. "},{"title":"Recovery logs​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#recovery-logs","content":"All task shards have associated state, which is managed in the shard's store. Capture tasks must track incremental checkpoints of their endpoint connectors.Derivation tasks manage a potentially very large index of registers, as well as read checkpoints of sourced collection journals.Materialization tasks track incremental checkpoints of their endpoint connectors, as well as read checkpoints of sourced collection journals. Shard stores userecovery logsto replicate updates and implement transaction semantics. Recovery logs are regular journals, but hold binary data and are not intended for direct use. However, they can hold your user data. Recovery logs of derivations hold your derivation register values. Recovery logs are stored in your cloud storage bucket, and must have a configured storage mapping. "},{"title":"Projections","type":0,"sectionRef":"#","url":"concepts/advanced/projections/","content":"","keywords":""},{"title":"Logical partitions​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#logical-partitions","content":"Projections can also be used to logically partition a collection, specified as a longer-form variant of a projection definition: collections: acmeCo/user-sessions: schema: session.schema.yaml key: [/user/id, /timestamp] projections: country: location: /country partition: true device: location: /agent/type partition: true network: location: /agent/network partition: true  Logical partitions isolate the storage of documents by their differing values for partitioned fields. Flow extracts partitioned fields from each document, and every unique combination of partitioned fields is a separate logical partition. Every logical partition has one or more physical partitionsinto which their documents are written, which in turn controls how files are arranged within cloud storage. For example, a document of &quot;acmeCo/user-sessions&quot; like: {&quot;country&quot;: &quot;CA&quot;, &quot;agent&quot;: {&quot;type&quot;: &quot;iPhone&quot;, &quot;network&quot;: &quot;LTE&quot;}, ...}  Might produce files in cloud storage like: s3://bucket/example/sessions/country=CA/device=iPhone/network=LTE/pivot=00/utc_date=2020-11-04/utc_hour=16/&lt;name&gt;.gz  info country, device, and network together identify a logical partition, while pivot identifies a physical partition.utc_date and utc_hour is the time at which the journal fragment was created. Learn more about physical partitions. "},{"title":"Partition selectors​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#partition-selectors","content":"When reading from a collection, Flow catalog entities like derivations, materializations, and tests can provide a partition selector, which identifies the subset of partitions that should be read from a source collection: # Partition selectors are included as part of a larger entity, # such as a derivation or materialization. partitions: # `include` selects partitioned fields and corresponding values that # must be matched in order for a partition to be processed. # All of the included fields must be matched. # Default: All partitions are included. type: object include: # Include partitions from North America. country: [US, CA] # AND where the device is a mobile phone. device: [iPhone, Android] # `exclude` selects partitioned fields and corresponding values which, # if matched, exclude the partition from being processed. # A match of any of the excluded fields will exclude the partition. # Default: No partitions are excluded. type: object exclude: # Skip sessions which were over a 3G network. network: [&quot;3G&quot;]  Partition selectors are efficient as they allow Flow to altogether avoid reading documents that aren’t needed. "},{"title":"Captures","type":0,"sectionRef":"#","url":"concepts/captures/","content":"","keywords":""},{"title":"Pull captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#pull-captures","content":"Pull captures pull data from an endpoint using a connector. "},{"title":"Estuary sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#estuary-sources","content":"Estuary builds and maintains many real-time connectors for various technology systems, such as database change data capture (CDC) connectors. See the source connector reference documentation. "},{"title":"Airbyte sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#airbyte-sources","content":"Flow also natively supports Airbyte source connectors. These connectors tend to focus on SaaS APIs, and do not offer real-time streaming integrations. Flow runs the connector at regular intervals to capture updated documents. Airbyte source connectors are independently reviewed and sometime updated for compatibility with Flow. Estuary's source connectors documentation includes actively supported Airbyte connectors. A full list of Airbyte's connectors is available at Airbyte docker hub. If you see a connector you'd like to prioritize for access in the Flow web app, contact us. "},{"title":"Discovery​","type":1,"pageTitle":"Captures","url":"concepts/captures/#discovery","content":"To help you configure new pull captures, Flow offers the guided discovery workflow in the Flow web application. To begin discovery, you tell Flow the connector you'd like to use and basic information about the endpoint. Flow automatically generates a capture configuration for you. It identifies one or moreresources — tables, data streams, or the equivalent — and generates bindings so that each will be mapped to a data collection in Flow. You may then modify the generated configuration as needed before publishing the capture. "},{"title":"Specification​","type":1,"pageTitle":"Captures","url":"concepts/captures/#specification","content":"Pull captures are defined in Flow specification files per the following format: # A set of captures to include in the catalog. # Optional, type: object captures: # The name of the capture. acmeCo/example/source-s3: # Endpoint defines how to connect to the source of the capture. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the capture connector. image: ghcr.io/estuary/source-s3:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how collections are populated from the data source. # A capture may bind multiple resources to different collections. # Required, type: array bindings: - # The target collection to capture into. # This may be defined in a separate, imported specification file. # Required, type: string target: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and capture a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: stream: a-bucket/and-prefix # syncMode should be set to incremental for all Estuary connectors syncMode: incremental - target: acmeCo/example/another-collection resource: stream: a-bucket/another-prefix syncMode: incremental  "},{"title":"Push captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#push-captures","content":"Push captures expose an endpoint to which documents may be pushed using a supported ingestion protocol. Beta Push captures are under development. Estuary intends to offer Webhook, Websocket, and Kafka-compatible APIs for capturing into collections. Join the Estuary Slack for more information on this and other ongoing development work. "},{"title":"Specification​","type":1,"pageTitle":"Captures","url":"concepts/captures/#specification-1","content":"Push capture configurations use the following general format: captures: # The name of the capture. acmeCo/example/webhook-ingest: endpoint: # This endpoint is an ingestion. ingest: {} bindings: - # The target collection to capture into. target: acmeCo/example/webhooks # The resource configures the specific behavior of the ingestion endpoint. resource: name: webhooks  "},{"title":"Catalog","type":0,"sectionRef":"#","url":"concepts/catalogs/","content":"","keywords":""},{"title":"Data Flows​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#data-flows","content":"You can mix and match catalog entities to create a variety of Data Flows. The simplest Data Flow has just three entities. graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; It may also be more complex, combining multiple entities of each type. graph LR; capture/two--&gt;collection/D; capture/one--&gt;collection/C; capture/one--&gt;collection/A; collection/A--&gt;derivation/B; collection/D--&gt;derivation/E; collection/C--&gt;derivation/E; derivation/B--&gt;derivation/E; collection/D--&gt;materialization/one; derivation/E--&gt;materialization/two; "},{"title":"Flow specification files​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#flow-specification-files","content":"Catalog entities are defined and described in Flow specification files.These YAML files contain the configuration details that each entity requires. You work on specification files as drafts before you publish them to a catalog. There are two ways to create and work with specification files. "},{"title":"In the Flow web app​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#in-the-flow-web-app","content":"You don't need to write or edit the specification files directly — the web app is designed to generate them for you. You do have the option to review and edit the generated specification as you create captures and materializations using the Catalog Editor. "},{"title":"With flowctl​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#with-flowctl","content":"If you prefer a developer workflow, you can also work with specification files directly in your local environment using flowctl. You then publish them back to the catalog. A given Data Flow may be described by one specification file, or by many, so long as a top-level file imports all the others. The files use the extension *.flow.yaml or are simply named flow.yaml by convention. Using this extension activates Flow's VS Code integration and auto-complete. Flow integrates with VS Code for development environment support, like auto-complete, tooltips, and inline documentation. Depending on your Data Flow, you may also have TypeScript modules, JSON schemas, or test fixtures. "},{"title":"Namespace​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#namespace","content":"All catalog entities (captures, materializations, and collections) are identified by a namesuch as acmeCo/teams/manufacturing/anvils. Names have directory-like prefixes and every name within Flow is globally unique. If you've ever used database schemas to organize your tables and authorize access, you can think of name prefixes as being akin to database schemas with arbitrary nesting. All catalog entities exist together in a single namespace. As a Flow customer, you're provisioned one or more high-level prefixes for your organization. Further division of the namespace into prefixes is up to you. Prefixes of the namespace, like acmeCo/teams/manufacturing/, are the foundation for Flow's authorization model. "},{"title":"Collections","type":0,"sectionRef":"#","url":"concepts/collections/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Collections","url":"concepts/collections/#specification","content":"Collections are defined in Flow specification files per the following format: # A set of collections to include in the catalog. # Optional, type: object collections: # The unique name of the collection. acmeCo/products/anvils: # The schema of the collection, against which collection documents # are validated. This may be an inline definition or a relative URI # reference. # Required, type: string (relative URI form) or object (inline form) schema: anvils.schema.yaml # The key of the collection, specified as JSON pointers of one or more # locations within collection documents. If multiple fields are given, # they act as a composite key, equivalent to a SQL table PRIMARY KEY # with multiple table columns. # Required, type: array key: [/product/id] # Projections and logical partitions for this collection. # Optional, type: object projections: # Derivation that builds this collection from others through transformations. # See the &quot;Derivations&quot; concept page to learn more. # Optional, type: object derivation:  "},{"title":"Schemas​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schemas","content":"Every Flow collection must declare a schema, and will never accept documents that do not validate against the schema. This helps ensure the quality of your data products and the reliability of your derivations and materializations. Schema specifications are flexible: yours could be exactingly strict, extremely permissive, or somewhere in between. For many source types, Flow is able to generate a basic schema during discovery. Schemas may either be declared inline, or provided as a reference to a file. References can also include JSON pointers as a URL fragment to name a specific schema of a larger schema document: InlineFile referenceReference with pointer collections: acmeCo/collection: schema: type: object required: [id] properties: id: string key: [/id]  Learn more about schemas "},{"title":"Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#keys","content":"Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. For example: flow.yamlschema.yaml collections: acmeCo/users: schema: schema.yaml key: [/userId]  Suppose the following JSON documents are captured into acmeCo/users: {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;William&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;}  As its key is [/userId], a materialization of the collection into a database table will reduce to a single row: userId | name 1 | Will  If its key were instead [/name], there would be two rows in the table: userId | name 1 | Will 1 | William  "},{"title":"Schema restrictions​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schema-restrictions","content":"Keyed document locations may be of a limited set of allowed types: booleanintegerstring Excluded types are: arraynullobjectFractional number Keyed fields also must always exist in collection documents. Flow performs static inference of the collection schema to verify the existence and types of all keyed document locations, and will report an error if the location could not exist, or could exist with the wrong type. Flow itself doesn't mind if a keyed location has multiple types, so long as they're each of the allowed types: an integer or string for example. Some materialization connectors, however, may impose further type restrictions as required by the endpoint. For example, SQL databases do not support multiple types for a primary key. "},{"title":"Composite Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#composite-keys","content":"A collection may have multiple locations which collectively form a composite key. This can include locations within nested objects and arrays: flow.yamlschema.yaml collections: acmeCo/compound-key: schema: schema.yaml key: [/foo/a, /foo/b, /foo/c/0, /foo/c/1]  "},{"title":"Key behaviors​","type":1,"pageTitle":"Collections","url":"concepts/collections/#key-behaviors","content":"A collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. Flow also performs opportunistic local reductions over windows of documents to improve its performance and reduce the volumes of data at each processing stage. An important subtlety is that the underlying storage of a collection will potentially retain many documents of a given key. In the acmeCo/users example, each of the &quot;Will&quot; or &quot;William&quot; variants is likely represented in the collection's storage — so long as they didn't arrive so closely together that they were locally combined by Flow. If desired, a derivation could re-key the collection on [/userId, /name] to materialize the various /names seen for a /userId. This property makes keys less lossy than they might otherwise appear, and it is generally good practice to chose a key that reflects how you wish to query a collection, rather than an exhaustive key that's certain to be unique for every document. "},{"title":"Empty keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#empty-keys","content":"When a specification is automatically generated, there may not be an unambiguously correct key for all collections. This could occur, for example, when a SQL database doesn't have a primary key defined for some table. In cases like this, the generated specification will contain an empty collection key. However, every collection must have a non-empty key, so you'll need to manually edit the generated specification and specify keys for those collections before publishing to the catalog. "},{"title":"Projections​","type":1,"pageTitle":"Collections","url":"concepts/collections/#projections","content":"Projections are named locations within a collection document that may be used for logical partitioning or directly exposed to databases into which collections are materialized. Many projections are automatically inferred from the collection schema. The projections stanza can be used to provide additional projections, and to declare logical partitions: collections: acmeCo/products/anvils: schema: anvils.schema.yaml key: [/product/id] # Projections and logical partitions for this collection. # Keys name the unique projection field, and values are its JSON Pointer # location within the document and configure logical partitioning. # Optional, type: object projections: # Short form: define a field &quot;product_id&quot; with document pointer /product/id. product_id: &quot;/product/id&quot; # Long form: define a field &quot;metal&quot; with document pointer /metal_type # which is a logical partition of the collection. metal: location: &quot;/metal_type&quot; partition: true  Learn more about projections. "},{"title":"Storage​","type":1,"pageTitle":"Collections","url":"concepts/collections/#storage","content":"Collections are real-time data lakes. Historical documents of the collection are stored as an organized layout of regular JSON files in your cloud storage bucket. Reads of that history are served by directly reading files from your bucket. Your storage mappingsdetermine how Flow collections are mapped into your cloud storage buckets. Unlike a traditional data lake, however, it's very efficient to read collection documents as they are written. Derivations and materializations that source from a collection are notified of its new documents within milliseconds of their being published. Learn more about journals, which provide storage for collections "},{"title":"Connectors","type":0,"sectionRef":"#","url":"concepts/connectors/","content":"","keywords":""},{"title":"Using connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#using-connectors","content":"Most — if not all — of your Data Flows will use at least one connector. You configure connectors within capture or materialization specifications. When you publish one of these entities, you're also deploying all the connectors it uses. You can interact with connectors using either the Flow web application or the flowctl CLI. "},{"title":"Flow web application​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#flow-web-application","content":"The Flow web application is designed to assist you with connector configuration and deployment. It's a completely no-code experience, but it's compatible with Flow's command line tools, discussed below. When you add a capture or materialization in the Flow web app, choose the desired data system from the Connector drop-down menu. The required fields for the connector appear below the drop-down. When you fill in the fields and click Discover Endpoint, Flow automatically &quot;discovers&quot; the data streams or tables — known as resources — associated with the endpoint system. From there, you can refine the configuration, save, and publish the resulting Flow specification. "},{"title":"GitOps and flowctl​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#gitops-and-flowctl","content":"Connectors are packaged as Open Container (Docker) images, and can be tagged, and pulled usingDocker Hub,GitHub Container registry, or any other public image registry provider. To interface with a connector, the Flow runtime needs to know: The specific image to use, through an image name such as ghcr.io/estuary/source-postgres:dev. Notice that the image name also conveys the specific image registry and version tag to use. Endpoint configuration such as a database address and account, with meaning that is specific to the connector. Resource configuration such as a specific database table to capture, which is also specific to the connector. To integrate a connector into your dataflow, you must define all three components within your Flow specification. The web application is intended to help you generate the Flow specification. From there, you can use flowctl to refine it in your local environment. It's also possible to manually write your Flow specification files, but this isn't the recommended workflow. materializations: acmeCo/postgres-views: endpoint: connector: # 1: Provide the image that implements your endpoint connector. # The `dev` tag uses the most recent version (the web app chooses this tag automatically) image: ghcr.io/estuary/materialize-postgres:dev # 2: Provide endpoint configuration that the connector requires. config: address: localhost:5432 password: password database: postgres user: postgres bindings: - source: acmeCo/products/anvils # 3: Provide resource configuration for the binding between the Flow # collection and the endpoint resource. This connector interfaces # with a SQL database and its resources are database tables. Here, # we provide a table to create and materialize which is bound to the # `acmeCo/products/anvils` source collection. resource: table: anvil_products # Multiple resources can be configured through a single connector. # Bind additional collections to tables as part of this connector instance: - source: acmeCo/products/TNT resource: table: tnt_products - source: acmeCo/customers resource: table: customers  Configuration​ Because connectors interface with external systems, each requires a slightly different endpoint configuration. Here you specify information such as a database hostname or account credentials — whatever that specific connector needs to function. If you're working directly with Flow specification files, you have the option of including the configuration inline or storing it in separate files: InlineReferenced file my.flow.yaml materializations: acmeCo/postgres-views: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: address: localhost:5432 password: password database: postgres user: postgres bindings: []  Storing configuration in separate files serves two important purposes: Re-use of configuration across multiple captures or materializationsThe ability to protect sensitive credentials "},{"title":"Protecting secrets​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#protecting-secrets","content":"Most endpoint systems require credentials of some kind, such as a username or password. Sensitive credentials should be protected while not in use. The only time a credential needs to be directly accessed is when Flow initiates the connector. Flow integrates with Mozilla’s sops tool, which can encrypt and protect credentials. It stores a sops-protected configuration in its encrypted form, and decrypts it only when invoking a connector on the your behalf. sops, short for “Secrets Operations,” is a tool that encrypts the values of a JSON or YAML document against a key management system (KMS) such as Google Cloud Platform KMS, Azure Key Vault, or Hashicorp Vault. Encryption or decryption of a credential with sops is an active process: it requires that the user (or the Flow runtime identity) have a current authorization to the required KMS, and creates a request trace which can be logged and audited. It's also possible to revoke access to the KMS, which immediately and permanently removes access to the protected credential. When you use the Flow web application, Flow automatically adds sops protection to sensitive fields on your behalf. You can also implement sops manually if you are writing a Flow specification locally. The examples below provide a useful reference. Example: Protect a configuration​ Suppose you're given a connector configuration: config.yaml host: my.hostname password: &quot;this is sensitive!&quot; user: my-user  You can protect it using a Google KMS key that you own: # Login to Google Cloud and initialize application default credentials used by `sops`. $ gcloud auth application-default login # Use `sops` to re-write the configuration document in place, protecting its values. $ sops --encrypt --in-place --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping each value in an encrypted envelope and adding a sops metadata section: config.yaml host: ENC[AES256_GCM,data:K/clly65pThTg2U=,iv:1bNmY8wjtjHFBcXLR1KFcsNMGVXRl5LGTdREUZIgcEU=,tag:5GKcguVPihXXDIM7HHuNnA==,type:str] password: ENC[AES256_GCM,data:IDDY+fl0/gAcsH+6tjRdww+G,iv:Ye8st7zJ9wsMRMs6BoAyWlaJeNc9qeNjkkjo6BPp/tE=,tag:EPS9Unkdg4eAFICGujlTfQ==,type:str] user: ENC[AES256_GCM,data:w+F7MMwQhw==,iv:amHhNCJWAJnJaGujZgjhzVzUZAeSchEpUpBau7RVeCg=,tag:62HguhnnSDqJdKdwYnj7mQ==,type:str] sops: # Some items omitted for brevity: gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T15:49:45Z&quot; enc: CiQAW8BC2GDYWrJTp3ikVGkTI2XaZc6F4p/d/PCBlczCz8BZiUISSQCnySJKIptagFkIl01uiBQp056c lastmodified: &quot;2022-01-05T15:49:45Z&quot; version: 3.7.1  You then use this config.yaml within your Flow specification. The Flow runtime knows that this document is protected by sopswill continue to store it in its protected form, and will attempt a decryption only when invoking a connector on your behalf. If you need to make further changes to your configuration, edit it using sops config.yaml. It's not required to provide the KMS key to use again, as sops finds it within its metadata section. important When deploying catalogs onto the managed Flow runtime, you must grant access to decrypt your GCP KMS key to the Flow runtime service agent, which is: flow-258@helpful-kingdom-273219.iam.gserviceaccount.com  Example: Protect portions of a configuration​ Endpoint configurations are typically a mix of sensitive and non-sensitive values. It can be cumbersome when sops protects an entire configuration document as you lose visibility into non-sensitive values, which you might prefer to store as cleartext for ease of use. You can use the encrypted-suffix feature of sops to selectively protect credentials: config.yaml host: my.hostname password_sops: &quot;this is sensitive!&quot; user: my-user  Notice that password in this configuration has an added _sops suffix. Next, encrypt only values which have that suffix: $ sops --encrypt --in-place --encrypted-suffix &quot;_sops&quot; --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping only values having a &quot;_sops&quot; suffix and adding its sops metadata section: config.yaml host: my.hostname password_sops: ENC[AES256_GCM,data:dlfidMrHfDxN//nWQTPCsjoG,iv:DHQ5dXhyOOSKI6ZIzcUM67R6DD/2MSE4LENRgOt6GPY=,tag:FNs2pTlzYlagvz7vP/YcIQ==,type:str] user: my-user sops: # Some items omitted for brevity: encrypted_suffix: _sops gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T16:06:36Z&quot; enc: CiQAW8BC2Au779CGdMFUjWPhNleCTAj9rL949sBvPQ6eyAC3EdESSQCnySJKD3eWX8XrtrgHqx327 lastmodified: &quot;2022-01-05T16:06:37Z&quot; version: 3.7.1  You then use this config.yaml within your Flow specification. Flow looks for and understands the encrypted_suffix, and will remove this suffix from configuration keys before passing them to the connector. "},{"title":"Connecting to endpoints on secure networks​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#connecting-to-endpoints-on-secure-networks","content":"In some cases, your source or destination endpoint may be within a secure network, and you may not be able to allow direct access to its port due to your organization's security policy. tip If permitted by your organization, a quicker solution is to whitelist the Estuary IP address, 34.121.207.128. For help completing this task on different cloud hosting platforms, see the documentation for the connector you're using. SHH tunneling, or port forwarding, provides a means for Flow to access the port indirectly through an SSH server. SSH tunneling is available in Estuary connectors for endpoints that use a network address for connection. To set up and configure the SSH server, see the guide. Then, add the appropriate properties when you define the capture or materialization in the Flow web app, or add the networkTunnel stanza directly to the YAML, as shown below. Sample​ source-postgres-ssh-tunnel.flow.yaml captures: acmeCo/postgres-capture-ssh: endpoint: connector: image: ghcr.io/estuary/source-postgres:dev config: address: 127.0.0.1:5432 database: flow user: flow_user password: secret networkTunnel: sshForwarding: # Location of the remote SSH server that supports tunneling. # Formatted as ssh://user@hostname[:port]. sshEndpoint: ssh://sshUser@198.21.98.1:22 # Private key to connect to the SSH server, formatted as multiline plaintext. # Use the YAML literal block style with the indentation indicator. # See https://yaml-multiline.info/ for details. privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: []  "},{"title":"Why an open connector architecture?​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#why-an-open-connector-architecture","content":"Historically, data platforms have directly implemented integrations to external systems with which they interact. Today, there are simply so many systems and APIs that companies use, that it’s not feasible for a company to provide all possible integrations. Users are forced to wait indefinitely while the platform works through their prioritized integration list. An open connector architecture removes Estuary — or any company — as a bottleneck in the development of integrations. Estuary contributes open-source connectors to the ecosystem, and in turn is able to leverage connectors implemented by others. Users are empowered to write their own connectors for esoteric systems not already covered by the ecosystem. Furthermore, implementing a Docker-based community specification brings other important qualities to Estuary connectors: Cross-platform interoperability between Flow, Airbyte, and any other platform that supports the protocolThe abilities to write connectors in any language and run them on any machineBuilt-in solutions for version management (through image tags) and distributionThe ability to integrate connectors from different sources at will, without the centralized control of a single company, thanks to container image registries info In order to be reflected in the Flow web app and used on the managed Flow platform, connectors must be reviewed and added by the Estuary team. Have a connector you'd like to add?Contact us. "},{"title":"Available connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#available-connectors","content":"Learn about available connectors in the reference section "},{"title":"flowctl","type":0,"sectionRef":"#","url":"concepts/flowctl/","content":"","keywords":""},{"title":"Installation and setup​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#installation-and-setup","content":"flowctl binaries for MacOS and Linux are available. Copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flowctl/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flowctl/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl Alternatively, you can find the source files on GitHub here. To connect to your Flow account and start a session, use an authentication token from the web app. "},{"title":"flowctl subcommands​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#flowctl-subcommands","content":"flowctl includes several top-level subcommands representing different functional areas. Each of these include multiple nested subcommands. Important top-level flowctl subcommands are described below. auth allows you to authenticate your development session in your local development environment. It's also how you provision Flow roles and users. Learn more about authentication. catalog allows you to work with your organization's current active catalog entities. You can investigate the current Data Flows, or add their specification files to a draft, where you can develop them further. draft allows you to work with drafts. You can create, test, develop locally, and then publish, or deploy, them to the catalog. You can access full documentation of all flowctl subcommands from the command line by passing the --help or -h flag, for example: flowctl --help lists top-level flowctl subcommands. flowctl catalog --help lists subcommands of catalog. "},{"title":"Working with drafts​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#working-with-drafts","content":"flowctl draft allows you to work with Flow specification files in the draft state and deploy changes to the catalog.draft is an essential flowctl subcommand that you'll use often. With draft, you: Create new drafts or convert active Data Flows into drafts.Pull a draft created in the web app or on the command line into your current working directory.Develop the draft locally.Author your local changes to the draft. This is equivalent to syncing changes.Test and publish the draft to publish to the catalog. graph LR; a((Start)); s[Selected, synced draft]; d[Local draft]; c[Active catalog]; s-- flowctl draft develop --&gt;d; d-- flowctl draft author --&gt;s; s-- flowctl draft publish --&gt;c; a-- flowctl draft select --&gt;s; d-- Work locally --&gt;d; "},{"title":"Development directories​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#development-directories","content":"Most of the work you perform with flowctl takes place remotely on Estuary infrastructure. You'll only see files locally when you are actively developing a draft. These files are created within your current working directory when you run flowctl draft develop. They typically include: flow.yaml: The main specification file that imports all other Flow specification files in the current draft. As part of local development, you may add new specifications that you create as imports.flow_generated/: Directory of generated files, including TypeScript classes and interfaces. See TypeScript code generation.estuary/: Directory of the draft's current specifications. Its contents will vary, but it may contain various YAML files and subdirectories.package.json and package-lock.json: Files used by npm to manage dependencies and your Data Flow's associated JavaScript project. You may customize package.json, but its dependencies stanza will be overwritten by thenpmDependenciesof your Flow specification source files, if any exist. "},{"title":"TypeScript code generation​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#typescript-code-generation","content":"TypeScript files are used in the Flow catalog both as part of the automatic build process, and to define lambdas functions for derivations, which requires your input. As part of the Data Flow build process, Flow translates yourschemasinto equivalent TypeScript types on your behalf. These definitions live within flow_generated/ in your Data Flow's build directory , and are frequently over-written by invocations of flowctl. Files in this subdirectory are human-readable and stable. You may want to commit them as part of a GitOps-managed project, but this isn't required. Whenever you define a derivation that uses a lambda, you must define the lambda in an accompanying TypeScript module, and reference that module in the derivation's definition. To facilitate this, you can generate a stub of the module using flowctl typescript generateand simply write the function bodies.Learn more about this workflow. If a TypeScript module exists, flowctl will never overwrite it, even if you update or expand your specifications such that the required interfaces have changed. "},{"title":"Imports","type":0,"sectionRef":"#","url":"concepts/import/","content":"","keywords":""},{"title":"Fetch behavior​","type":1,"pageTitle":"Imports","url":"concepts/import/#fetch-behavior","content":"Flow resolves, fetches, and validates all imports in your local environment during the catalog build process, and then includes their fetched contents within the published catalog on the Estuary servers. The resulting catalog entities are thus self-contained snapshots of all resourcesas they were at the time of publication. This means it's both safe and recommended to directly reference an authoritative source of a resource, such as a third-party JSON schema, as well as resources within your private network. It will be fetched and verified locally at build time, and thereafter that fetched version will be used for execution, regardless of whether the authority URL itself later changes or errors. "},{"title":"Import types​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-types","content":"Almost always, the import stanza is used to import other Flow specification files. This is the default when given a string path: import: - path/to/source/catalog.flow.yaml  A long-form variant also accepts a content type of the imported resource: import: - url: path/to/source/catalog.flow.yaml contentType: CATALOG  Other permitted content types include JSON_SCHEMA, but these are not typically used and are needed only for advanced use cases. "},{"title":"JSON Schema $ref​","type":1,"pageTitle":"Imports","url":"concepts/import/#json-schema-ref","content":"Certain catalog entities, like collections, commonly reference JSON schemas. It's not necessary to explicitly add these to the import section; they are automatically resolved and treated as an import. You can think of this as an analog to the JSON Schema $ref keyword, which is used to reference a schema that may be contained in another file. The one exception is schemas that use the $id keyword at their root to define an alternative canonical URL. In this case, the schema must be referenced through its canonical URL, and then explicitly added to the import section with JSON_SCHEMA content type. "},{"title":"Importing derivation resources​","type":1,"pageTitle":"Imports","url":"concepts/import/#importing-derivation-resources","content":"In many cases, derivations in your catalog will need to import resources. Usually, these are Typescript modules that define the lambda functions of a transformation, and, in certain cases, the NPM dependencies of that Typescript module. These imports are specified in the derivation specification, not in the import section of the catalog spec. For more information, see Derivation specification and creating TypeScript modules. "},{"title":"Import paths​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-paths","content":"If a catalog source file foo.flow.yaml references a collection in bar.flow.yaml, for example as a target of a capture, there must be an import path where either foo.flow.yamlimports bar.flow.yaml or vice versa. Import paths can be direct: graph LR; foo.flow.yaml--&gt;bar.flow.yaml; Or they can be indirect: graph LR; bar.flow.yaml--&gt;other.flow.yaml; other.flow.yaml--&gt;foo.flow.yaml; The sources must still have an import path even if referenced from a common parent. The following would not work: graph LR; parent.flow.yaml--&gt;foo.flow.yaml; parent.flow.yaml--&gt;bar.flow.yaml; These rules make your catalog sources more self-contained and less brittle to refactoring and reorganization. Consider what might otherwise happen if foo.flow.yamlwere imported in another project without bar.flow.yaml. "},{"title":"Materializations","type":0,"sectionRef":"#","url":"concepts/materialization/","content":"","keywords":""},{"title":"Discovery​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#discovery","content":"Materializations use real-time connectors to connect to many endpoint types. When you use a materialization connector in the Flow web app, flow helps you configure it through the discovery workflow. To begin discovery, you tell Flow the connector you'd like to use, basic information about the endpoint, and the collection(s) you'd like to materialize there. Flow maps the collection(s) to one or more resources — tables, data streams, or the equivalent — through one or more bindings. You may then modify the generated configuration as needed before publishing the materialization. "},{"title":"Specification​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#specification","content":"Materializations are defined in Flow specification files per the following format: # A set of materializations to include in the catalog. # Optional, type: object materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the materialization connector. image: ghcr.io/estuary/materialize-postgres:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how one or more collections map to materialized endpoint resources. # A single materialization may include many collections and endpoint resources, # each defined as a separate binding. # Required, type: object bindings: - # The source collection to materialize. # This may be defined in a separate, imported specification file. # Required, type: string source: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and materialize a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: # The materialize-postgres connector expects a `table` key # which names a table to materialize into. table: example_table  "},{"title":"How continuous materialization works​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#how-continuous-materialization-works","content":"Flow materializations are continuous materialized views. They maintain a representation of the collection within the endpoint system that is updated in near real-time. It's indexed on thecollection key. As the materialization runs, it ensures that all collection documents and their accumulated reductions are reflected in this managed endpoint resource. When you first publish a materialization, Flow back-fills the endpoint resource with the historical documents of the collection. Once caught up, Flow applies new collection documents using incremental and low-latency updates. As collection documents arrive, Flow: Reads previously materialized documents from the endpoint for the relevant keysReduces new documents into these read documentsWrites updated documents back into the endpoint resource, indexed by their keys For example, consider a collection and its materialization:  collections: acmeCo/colors: key: [/color] schema: type: object required: [color, total] reduce: {strategy: merge} properties: color: {enum: [red, blue, purple]} total: type: integer reduce: {strategy: sum} materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/colors resource: { table: colors }  Suppose documents are periodically added to the collection: {&quot;color&quot;: &quot;red&quot;, &quot;total&quot;: 1} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 2} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 3}  Its materialization into a database table will have a single row for each unique color. As documents arrive in the collection, the row total is updated within the materialized table so that it reflects the overall count:  Flow does not keep separate internal copies of collection or reduction states, as some other systems do. The endpoint resource is the one and only place where state &quot;lives&quot; within a materialization. This makes materializations very efficient and scalable to operate. They are able to maintain very large tables stored in highly scaled storage systems like OLAP data warehouses. "},{"title":"Projected fields​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#projected-fields","content":"Many endpoint systems are document-oriented and can directly work with collections of JSON documents. Others are table-oriented and require an up-front declaration of columns and types to be most useful, such as a SQL CREATE TABLE definition. Flow uses collection projections to relate locations within a hierarchical JSON document to equivalent named fields. A materialization can in turn select a subset of available projected fields where, for example, each field becomes a column in a SQL table created by the connector. It would be tedious to explicitly list projections for every materialization, though you certainly can if desired. Instead, Flow and the materialization connector negotiate a recommended field selection on your behalf, which can be fine-tuned. For example, a SQL database connector will typically require that fields comprising the primary key be included, and will recommend that scalar values be included, but will by default exclude document locations that don't have native SQL representations, such as locations which can have multiple JSON types or are arrays or maps. materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/example/collection resource: { table: example_table } # Select (or exclude) projections of the collection for materialization as fields. # If not provided, the recommend fields of the endpoint connector are used. # Optional, type: object fields: # Whether to include fields that are recommended by the endpoint connector. # If false, then fields can still be added using `include`. # Required, type: boolean recommended: true # Fields to exclude. This is useful for deselecting a subset of recommended fields. # Default: [], type: array exclude: [myField, otherField] # Fields to include. This can supplement recommended fields, or can # designate explicit fields to use if recommended fields are disabled. # # Values of this map are used to customize connector behavior on a per-field basis. # They are passed directly to the connector and are not interpreted by Flow. # Consult your connector's documentation for details of what customizations are available. # This is an advanced feature and is not commonly used. # # default: {}, type: object include: {goodField: {}, greatField: {}}  "},{"title":"Partition selectors​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#partition-selectors","content":"Partition selectors let you materialize only a subset of a collection that haslogical partitions. For example, you might have a large collection that is logically partitioned on each of your customers: collections: acmeCo/anvil/orders: key: [/id] schema: orders.schema.yaml projections: customer: location: /order/customer partition: true  A large customer asks if you can provide an up-to-date accounting of their orders. This can be accomplished with a partition selector: materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/anvil/orders resource: { table: coyote_orders } # Process partitions where &quot;Coyote&quot; is the customer. partitions: include: customer: [Coyote]  Learn more about partition selectors. "},{"title":"Destination-specific performance​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#destination-specific-performance","content":"Flow processes updates in transactions, as quickly as the destination endpoint can handle them. This might be milliseconds in the case of a fast key/value store, or many minutes in the case of an OLAP warehouse. If the endpoint is also transactional, Flow integrates its internal transactions with those of the endpoint for integrated end-to-end “exactly once” semantics. The materialization is sensitive to back pressure from the endpoint. As a database gets busy, Flow adaptively batches and combines documents to consolidate updates: In a given transaction, Flow reduces all incoming documents on the collection key. Multiple documents combine and result in a single endpoint read and write during the transaction.As a target database becomes busier or slower, transactions become larger. Flow does more reduction work within each transaction, and each endpoint read or write accounts for an increasing volume of collection documents. This allows you to safely materialize a collection with a high rate of changes into a small database, so long as the cardinality of the materialization is of reasonable size. "},{"title":"Delta updates​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#delta-updates","content":"As described above, Flow's standard materialization mechanism involves querying the target system for data state before reducing new documents directly into it. For these standard updates to work, the endpoint must be a stateful system, like a relational database. However, other systems — like Webhooks and Pub/Sub — may also be endpoints. None of these typically provide a state representation that Flow can query. They are write-only in nature, so Flow cannot use their endpoint state to help it fully reduce collection documents on their keys. Even some stateful systems are incompatible with Flow's standard updates due to their unique design and architecture. For all of these endpoints, Flow offers a delta-updates mode. When using delta updates, Flow does not attempt to maintain full reductions of each unique collection key. Instead, Flow locally reduces documents within each transaction (this is often called a &quot;combine&quot;), and then materializes onedelta document per key to the endpoint. In other words, when delta updates are used, Flow sends information about data changes by key, and further reduction is left up to the endpoint system. Some systems may reduce documents similar to Flow; others use a different mechanism; still others may not perform reductions at all. A given endpoint may support standard updates, delta updates, or both. This depends on the materialization connector. Expect that a connector will use standard updates only unless otherwise noted in its documentation. "},{"title":"Derivations","type":0,"sectionRef":"#","url":"concepts/derivations/","content":"","keywords":""},{"title":"Creating derivations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#creating-derivations","content":"You can create a derivation in your local development environment using flowctl. Use flowctl draft to begin work with a draft, and manually add a derivation to the Flow specification file. If necessary, generate a typescript file and define lambda functions there. "},{"title":"Specification​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#specification","content":"A derivation is specified as a regular collection with an additional derivation stanza: collections: # The unique name of the derivation. acmeCo/my/derivation: schema: my-schema.yaml key: [/key] # Presence of a `derivation` stanza makes this collection a derivation. # Type: object derivation: # Register definition of the derivation. # If not provided, registers have an unconstrained schema # and initialize to the `null` value. # Optional, type: object register: # JSON Schema of register documents. As with collection schemas, # this is either an inline definition or a relative URL reference. # Required, type: string (relative URL form) or object (inline form) schema: type: integer # Initial value taken by a register which has never been updated before. # Optional, default: null initial: 0 # TypeScript module that implements any lambda functions invoked by this derivation. # Optional, type: object typescript: # TypeScript module implementing this derivation. # Module is either a relative URL of a TypeScript module file (recommended), # or an inline representation of a TypeScript module. # The file specified will be created when you run `flowctl typescript generate` module: acmeModule.ts # NPM package dependencies of the module # Version strings can take any form understood by NPM. # See https://docs.npmjs.com/files/package.json#dependencies npmDependencies: {} # Transformations of the derivation, # specified as a map of named transformations. transform: # Unique name of the transformation, containing only Unicode # Letters and Numbers (no spaces or punctuation). myTransformName: # Source collection read by this transformation. # Required, type: object source: # Name of the collection to be read. # Required. name: acmeCo/my/source/collection # JSON Schema to validate against the source collection. # If not set, the schema of the source collection is used. # Optional, type: string (relative URL form) or object (inline form) schema: {} # Partition selector of the source collection. # Optional. Default is to read all partitions. partitions: {} # Delay applied to sourced documents before being processed # by this transformation. # Default: No delay, pattern: ^\\\\d+(s|m|h)$ readDelay: &quot;48h&quot; # Shuffle determines the key by which source documents are # shuffled (mapped) to a register. # Optional, type: object. # If not provided, documents are shuffled on the source collection key. shuffle: # Key is a composite key which is extracted from documents # of the source. key: [/shuffle/key/one, /shuffle/key/two] # Update lambda of the transformation. # Optional, type: object update: {lambda: typescript} # Publish lambda of the transformation. # Optional, type: object publish: {lambda: typescript} # Priority applied to processing documents of this transformation # relative to other transformations of the derivation. # Default: 0, integer &gt;= 0 priority: 0  "},{"title":"Background​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#background","content":"The following sections will refer to the following common example to illustrate concepts. Suppose you have an application through which users send one another some amount of currency, like in-game tokens or dollars or digital kittens: transfers.flow.yamltransfers.schema.yaml collections: # Collection of 💲 transfers between accounts: # {id: 123, sender: alice, recipient: bob, amount: 32.50} acmeBank/transfers: schema: transfers.schema.yaml key: [/id]  There are many views over this data that you might require, such as summaries of sender or receiver activity, or current account balances within your application. "},{"title":"Transformations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#transformations","content":"A transformation binds a source collection to a derivation. As documents of the source collection arrive, the transformation processes the document to publish new documents,update aregister, or both. Read source documents are shuffled on a shuffle key to co-locate the processing of documents that have equal shuffle keys. The transformation then processes documents by invoking lambdas: user-defined functions that accept documents as arguments and return documents in response. A derivation may have many transformations, and each transformation has a long-lived and stable name. Each transformation independently reads documents from its source collection and tracks its own read progress. More than one transformation can read from the same source collection, and transformations may also source from their own derivation, enabling cyclic data-flows and graph algorithms. Transformations may be added to or removed from a derivation at any time. This makes it possible to, for example, add a new collection into an existing multi-way join, or gracefully migrate to a new source collection without incurring downtime. However, renaming a running transformation is not possible. If attempted, the old transformation is dropped and a new transformation under the new name is created, which begins reading its source collection all over again. graph LR; d[Derivation]; t[Transformation]; r[Registers]; p[Publish λ]; u[Update λ]; c[Sourced Collection]; d-- has many --&gt;t; t-- reads from --&gt;c; t-- invokes --&gt;u; t-- invokes --&gt;p; u-- updates --&gt;r; r-- reads --&gt;p; d-- indexes --&gt;r; "},{"title":"Sources​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#sources","content":"The source of a transformation is a collection. As documents are published into the source collection, they are continuously read and processed by the transformation. A partition selector may be provided to process only a subset of the source collection's logical partitions. Selectors are efficient: only partitions that match the selector are read, and Flow can cheaply skip over partitions that don't. Derivations re-validate their source documents against the source collection's schema as they are read. This is because collection schemas may evolve over time, and could have inadvertently become incompatible with historical documents of the source collection. Upon a schema error, the derivation will pause and give you an opportunity to correct the problem. You may also provide an alternative source schema. Source schemas aide in processing third-party sources of data that you don't control, which can have unexpected schema changes without notice. You may want to capture this data with a minimal and very permissive schema. Then, a derivation can apply a significantly stricter source schema, which verifies your current expectations of what the data should be. If those expectations turn out to be wrong, little harm is done: your derivation is paused but the capture continues to run. You must simply update your transformations to account for the upstream changes and then continue without any data loss. "},{"title":"Shuffles​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#shuffles","content":"As each source document is read, it's shuffled — or equivalently, mapped — on an extracted key. If you're familiar with data shuffles in tools like MapReduce, Apache Spark, or Flink, the concept is very similar. Flow catalog tasks scale to run across many machines at the same time, where each machine processes a subset of source documents. Shuffles let Flow know how to group documents so that they're co-located, which can increase processing efficiency and reduce data volumes. They are also used to map source documents to registers. graph LR; subgraph s1 [Source Partitions] p1&gt;acmeBank/transfers/part-1]; p2&gt;acmeBank/transfers/part-2]; end subgraph s2 [Derivation Task Shards] t1([task/shard-1]); t2([task/shard-2]); end p1-- sender: alice --&gt;t1; p1-- sender: bob --&gt;t2; p2-- sender: alice --&gt;t1; p2-- sender: bob --&gt;t2; If you don't provide a shuffle key, Flow will shuffle on the source collection key, which is typically what you want. If a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string or integer) as well as the number of components in a composite key. For example, one transformation couldn't shuffle transfers on [/id]while another shuffles on [/sender], because sender is a string andid an integer. Similarly mixing a shuffle of [/sender] alongside [/sender, /recipient]is prohibited because the keys have different numbers of components. However, one transformation can shuffle on [/sender]while another shuffles on [/recipient], as in the examples below. "},{"title":"Registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#registers","content":"Registers are the internal memory of a derivation. They are a building block that enable derivations to tackle advanced stateful streaming computations like multi-way joins, windowing, and transaction processing. As we've already seen, not all derivations require registers, but they are essential for a variety of important use cases. Each register is a document with a user-definedschema. Registers are keyed, and every derivation maintains an index of keys and their corresponding register documents. Every source document is mapped to a specific register document through its extracted shuffle key. For example, when shuffling acmeBank/transfers on [/sender] or [/recipient], each account (&quot;alice&quot;, &quot;bob&quot;, or &quot;carol&quot;) is allocated its own register. You might use that register to track a current account balance given the received inflows and sent outflows of each account. If you instead shuffle on [/sender, /recipient], each pair of accounts (&quot;alice -&gt; bob&quot;, &quot;alice -&gt; carol&quot;, &quot;bob -&gt; carol&quot;) is allocated a register. Transformations of a derivation may have different shuffle keys, but the number of key components and their JSON types must agree. Two transformations could map on [/sender] and [/recipient], but not [/sender] and [/recipient, /sender]. Registers are best suited for relatively small, fast-changing documents that are shared within and across the transformations of a derivation. The number of registers indexed within a derivation may be very large, and if a register has never before been used, it starts with a user-defined initial value. From there, registers may be modified through an update lambda. info Under the hood, registers are backed by replicated, embedded RocksDB instances, which co-locate with the lambda execution contexts that Flow manages. As contexts are assigned and re-assigned, their register databases travel with them. If any single RocksDB instance becomes too large, Flow is able to perform an online split, which subdivides its contents into two new databases — and paired execution contexts — which are re-assigned to other machines. "},{"title":"Lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#lambdas","content":"Lambdas are user-defined functions that are invoked by derivations. They accept documents as arguments and return transformed documents in response. Lambdas can be used to update registers, publish documents into a derived collection, or compute a non-trivial shuffle key of a document. Beta The ability for lambdas to compute a document's shuffle key is coming soon. Flow supports TypeScript lambdas, which you define in an accompanying TypeScript module and reference in a derivation's typescript stanzas. See the derivation specification and Creating TypeScript modules for more details on how to get started. TypeScript lambdas are &quot;serverless&quot;; Flow manages the execution and scaling of your Lambda on your behalf. Alternatively, Flow also supports remote lambdas, which invoke an HTTP endpoint you provide, such as an AWS Lambda or Google Cloud Run function. In terms of the MapReduce functional programming paradigm, Flow lambdas are mappers, which map documents into new user-defined shapes. Reductions are implemented by Flow using the reduction annotations of your collection schemas. "},{"title":"Publish lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#publish-lambdas","content":"A publish lambda publishes documents into the derived collection. To illustrate first with an example, suppose you must know the last transfer from each sender that was over $100: last-large-send.flow.yamllast-large-send.tslast-large-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/last-large-send: schema: transfers.schema.yaml key: [/sender] derivation: typescript: module: last-large-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  This transformation defines a TypeScript publish lambda, which is implemented in an accompanying TypeScript module. The lambda is invoked as each source transfer document arrives. It is given the source document, and also includes the a _register and _previous register, which are not used here. The lambda outputs zero or more documents, each of which must conform to the derivation's schema. As this derivation's collection is keyed on /sender, the last published document (the last large transfer) of each sender is retained. If it were instead keyed on /id, then all transfers with large amounts would be retained. In SQL terms, the collection key acts as a GROUP BY.  Derivation collection schemas may havereduction annotations, and publish lambdas can be combined with reductions in interesting ways. You may be familiar with map and reduce functions built into languages likePython,JavaScript; and many others, or have used tools like MapReduce or Spark. In functional terms, lambdas you write within Flow are &quot;mappers,&quot; and reductions are always done by the Flow runtime using your schema annotations. Suppose you need to know the runningaccount balancesof your users given all of their transfers thus far. Tackle this by reducing the final account balance for each user from all of the credit and debit amounts of their transfers: balances.flow.yamlbalances.tsbalances-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/balances: schema: type: object required: [user] reduce: { strategy: merge } properties: user: { type: string } balance: type: number reduce: { strategy: sum } key: [/user] derivation: typescript: module: balances.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  "},{"title":"Update lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#update-lambdas","content":"An update lambda transforms a source document into an update of the source document's register. To again illustrate through an example, suppose your compliance department wants you to flag the first transfer a sender sends to a new recipient. You achieve this by shuffling on pairs of[/sender, /recipient] and using a register to track whether this account pair has been seen before: first-send.flow.yamlfirst-send.tsfirst-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/first-send: schema: transfers.schema.yaml key: [/id] derivation: # We'll store a `true/false` boolean in our register documents, # which is initially `false` and becomes `true` after the first transfer. register: schema: { type: boolean } initial: false typescript: module: first-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers # Shuffle so that each account pair # is allocated its own register. shuffle: key: [/sender, /recipient] update: lambda: typescript publish: lambda: typescript  This transformation uses both a publish and an update lambda, implemented in an accompanying TypeScript module. The update lambda is invoked first for each source document, and it returns zero or more documents, which each must conform to the derivation's register schema (in this case, a simple boolean). The publish lambda is invoked next, and is given the sourcedocument as well as the before (previous) and after (_register) values of the updated register. In this case, we don't need the after value: our update lambda implementation implies that it's always true. The before value, however, tells us whether this was the very first update of this register, and by implication was the first transfer for this pair of accounts. sequenceDiagram autonumber Derivation-&gt;&gt;Update λ: update({sender: alice, recipient: bob})? Update λ--&gt;&gt;Derivation: return &quot;true&quot; Derivation-&gt;&gt;Registers: lookup(key = [alice, bob])? Registers--&gt;&gt;Derivation: not found, initialize as &quot;false&quot; Derivation--&gt;&gt;Derivation: Register: &quot;false&quot; =&gt; &quot;true&quot; Derivation-)Registers: store(key = [alice, bob], value = &quot;true&quot;) Derivation-&gt;&gt;Publish λ: publish({sender: alice, recipient: bob}, register = &quot;true&quot;, previous = &quot;false&quot;)? Publish λ--&gt;&gt;Derivation: return {sender: alice, recipient: bob} FAQ Why not have one lambda that can return a register update and derived documents? Performance.Update and publish are designed to be parallelized and pipelined over many source documents simultaneously, while still giving the appearance and correctness of lambdas are invoked in strict serial order. Notice that (1) above doesn't depend on actually knowing the register value, which doesn't happen until (4). Many calls like (1) can also happen in parallel, so long as their applications to the register value (5) happen in the correct order. In comparison, a single-lambda design would require Flow to await each invocation before it can begin the next.  Register schemas may also havereduction annotations, and documents returned by update lambdas are reduced into the current register value. The compliance department reached out again, and this time they need you to identify transfers where the sender's account had insufficient funds. You manage this by tracking the running credits and debits of each account in a register. Then, you enrich each transfer with the account's current balance and whether the account was overdrawn: flagged-transfers.flow.yamlflagged-transfers.tsflagged-transfers-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/flagged-transfers: schema: # Extend transfer schema with `balance` and `overdrawn` fields. $ref: transfers.schema.yaml required: [balance, overdrawn] properties: balance: { type: number } overdrawn: { type: boolean } key: [/id] projections: # Logically partition on transfers which are flagged as overdrawn. overdrawn: location: /overdrawn partition: true derivation: # Registers track the current balance of each account. register: schema: type: number reduce: { strategy: sum } initial: 0 typescript: module: flagged-transfers.ts transform: fromTransferSender: source: { name: examples/acmeBank/transfers } shuffle: { key: [/sender] } # Debit the sender's register balance. update: { lambda: typescript } # Publish transfer enriched with current sender balance. publish: { lambda: typescript } fromTransferRecipient: source: { name: examples/acmeBank/transfers } shuffle: { key: [/recipient] } # Credit the recipient's register balance. update: { lambda: typescript }  Source transfers are read twice. The first read shuffles on /recipientto track account credits, and the second shuffles on /senderto track account debits and to publish enriched transfer events. Update lambdas return the amount of credit or debit, and these amounts are summed into a derivation register keyed on the account. sequenceDiagram autonumber Derivation-&gt;&gt;Registers: lookup(key = alice)? Registers--&gt;&gt;Derivation: not found, initialize as 0 Derivation-&gt;&gt;Update λ: update({recipient: alice, amount: 50, ...})? Update λ--&gt;&gt;Derivation: return +50 Derivation-&gt;&gt;Update λ: update({sender: alice, amount: 75, ...})? Update λ--&gt;&gt;Derivation: return -75 Derivation--&gt;&gt;Derivation: Register: 0 + 50 =&gt; 50 Derivation--&gt;&gt;Derivation: Register: 50 - 75 =&gt; -25 Derivation-&gt;&gt;Publish λ: publish({sender: alice, amount: 75, ...}, register = -25, previous = 50)? Publish λ--&gt;&gt;Derivation: return {sender: alice, amount: 75, balance: -25, overdrawn: true} "},{"title":"Creating TypeScript modules​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#creating-typescript-modules","content":"To create a new TypeScript module for the lambdas of your derivation, you can use flowctl typescript generate to generate it. In the derivation specification, choose the name for the new module and run flowctl typescript generate. Flow creates a module with the name you specified, stubs of the required interfaces, and TypeScript types that match your schemas. Update the module with your lambda function bodies, and proceed to test and deploy your catalog. Using the example below, flowctl typescript generate --source=acmeBank.flow.yaml will generate the stubbed-out acmeBank.ts. acmeBank.flow.yamlacmeBank.ts (generated stub) collections: acmeBank/balances: schema: balances.schema.yaml key: [/account] derivation: typescript: module: acmeBank.ts transform: fromTransfers: source: { name: acmeBank/transfers } publish: { lambda: typescript }  Learn more about TypeScript generation "},{"title":"NPM dependencies​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#npm-dependencies","content":"Your TypeScript modules may depend on otherNPM packages, which can be be imported through the npmDependenciesstanza of the derivation spec. For example, moment is a common library for working with times: derivation.flow.yamlfirst-send.ts derivation: typescript: module: first-send.ts npmDependencies: moment: &quot;^2.24&quot; transform: { ... }  Use any version string understood by package.json, which can include local packages, GitHub repository commits, and more. See package.json documentation. During the catalog build process, Flow gathers NPM dependencies across all Flow specification files and patches them into the catalog's managed package.json. Flow organizes its generated TypeScript project structure for a seamless editing experience out of the box with VS Code and other common editors. "},{"title":"Remote lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#remote-lambdas","content":"A remote Lambda is one that you implement and host yourself as a web-accessible endpoint, typically via a service like AWS Lambda or Google Cloud Run. Flow will invoke your remote Lambda as needed, POST-ing JSON documents to process and expecting JSON documents in the response. "},{"title":"Processing order​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#processing-order","content":"Derivations may have multiple transformations that simultaneously read from different source collections, or even multiple transformations that read from the same source collection. Roughly speaking, the derivation will globally process transformations and their source documents in the time-based order in which the source documents were originally written to their source collections. This means that a derivation started a month ago and a new copy of the derivation started today, will process documents in the same order and arrive at the same result. Derivations are repeatable. More precisely, processing order is stable for each individual shuffle key, though different shuffle keys may process in different orders if more than one task shard is used. Processing order can be attenuated through a read delayor differing transformation priority. "},{"title":"Read delay​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-delay","content":"A transformation can define a read delay, which will hold back the processing of its source documents until the time delay condition is met. For example, a read delay of 15 minutes would mean that a source document cannot be processed until it was published at least 15 minutes ago. If the derivation is working through a historical backlog of source documents, than a delayed transformation will respect its ordering delay relative to the publishing times of other historical documents also being read. Event-driven workflows are a great fit for reacting to events as they occur, but aren’t terribly good at taking action when something hasn’t happened: A user adds a product to their cart, but then doesn’t complete a purchase.A temperature sensor stops producing its expected, periodic measurements. A common pattern for tackling these workflows in Flow is to read a source collection without a delay and update a register. Then, read a collection with a read delay and determine whether the desired action has happened or not. For example, source from a collection of sensor readings and index the last timestamp of each sensor in a register. Then, source the same collection again with a read delay: if the register timestamp isn't more recent than the delayed source reading, the sensor failed to produce a measurement. Flow read delays are very efficient and scale better than managing very large numbers of fine-grain timers. Learn more from the Citi Bike idle bikes example "},{"title":"Read priority​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-priority","content":"Sometimes it's necessary for all documents of a source collection to be processed by a transformation before any documents of some other source collection are processed, regardless of their relative publishing time. For example, a collection may have corrections that should be applied before the historical data of another collection is re-processed. Transformation priorities allow you to express the relative processing priority of a derivation's various transformations. When priorities are not equal, all available source documents of a higher-priority transformation are processed before any source documents of a lower-priority transformation. "},{"title":"Where to accumulate?​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#where-to-accumulate","content":"When you build a derived collection, you must choose where accumulation will happen: whether Flow will reduce into documents held within your materialization endpoint, or within the derivation's registers. These two approaches can produce equivalent results, but they do so in very different ways. "},{"title":"Accumulate in your database​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-your-database","content":"To accumulate in your materialization endpoint, such as a database, you define a derivation with a reducible schema and use only publish lambdas and no registers. The Flow runtime uses your reduction annotations to combine published documents, which are written to the collection. It then fully reduces collection documents into the values stored in the database. This keeps the materialized table up to date. A key insight is that the database is the only stateful system in this scenario, and Flow uses reductions in two steps: To combine many published documents into intermediate delta documents, which are the documents written to collection storage.To reduce delta states into the final database-stored document. For example, consider a collection that’s summing a value: Time\tDB\tLambdas\tDerived DocumentT0\t0\tpublish(2, 1, 2)\t5 T1\t5\tpublish(-2, 1)\t-1 T2\t4\tpublish(3, -2, 1)\t2 T3\t6\tpublish()\t This works especially well when materializing into a transactional database. Flow couples its processing transactions with corresponding database transactions, ensuring end-to-end “exactly once” semantics. When materializing into a non-transactional store, Flow is only able to provide weaker “at least once” semantics; it’s possible that a document may be combined into a database value more than once. Whether that’s a concern depends a bit on the task at hand. Some reductions can be applied repeatedly without changing the result (they're &quot;idempotent&quot;), while in other use cases approximations are acceptable. For the summing example above, &quot;at-least-once&quot; semantics could give an incorrect result. "},{"title":"Accumulate in registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-registers","content":"To accumulate in registers, you use a derivation that defines a reducible register schema that's updated through update lambdas. The Flow runtime allocates, manages, and scales durable storage for registers; you don’t have to. Then you use publish lambdas to publish a snapshot of your register value into your collection. Returning to our summing example: Time\tRegister\tLambdas\tDerived DocumentT0\t0\tupdate(2, 1, 2), publish(register)\t5 T1\t5\tupdate(-2, 1), publish(register)\t4 T2\t4\tupdate(3, -2, 1), publish(register)\t6 T3\t6\tupdate()\t Register derivations are a great solution for materializations into non-transactional stores because the documents they produce can be applied multiple times without breaking correctness. They’re also well-suited for materializations into endpoints that aren't stateful, such as pub/sub systems or Webhooks, because they can produce fully reduced values as stand-alone updates. Learn more in the derivation pattern examples of Flow's repository "},{"title":"Schemas","type":0,"sectionRef":"#","url":"concepts/schemas/","content":"","keywords":""},{"title":"JSON Schema​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#json-schema","content":"JSON Schemais an expressive open standard for defining the schema and structure of documents. Flow uses it for all schemas defined in Flow specifications. JSON Schema goes well beyond basic type information and can modeltagged unions, recursion, and other complex, real-world composite types. Schemas can also define rich data validations like minimum and maximum values, regular expressions, dates, timestamps, email addresses, and other formats. Together, these features let schemas represent structure as well asexpectations and constraints that are evaluated and must hold true for every collection document before it’s added to the collection. They’re a powerful tool for ensuring end-to-end data quality: for catching data errors and mistakes early, before they can impact your production data products. "},{"title":"Generation​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#generation","content":"When capturing data from an external system, Flow can usually generate suitable JSON schemas on your behalf. Learn more about using connectors "},{"title":"Translations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#translations","content":"You must only provide Flow a model of a given dataset one time, as a JSON schema. Having done that, Flow leverages static inference over your schemas to perform many build-time validations of your catalog entities, helping you catch potential problems early. Schema inference is also used to provide translations into other schema flavors: Most projections of a collection are automatically inferred from its schema. Materializations use your projections to create appropriate representations in your endpoint system. A SQL connector will create table definitions with appropriate columns, types, and constraints.Flow generates TypeScript definitions from schemas to provide compile-time type checks of user lambda functions. These checks are immensely helpful for surfacing mismatched expectations around, for example, whether a field could ever be null or is misspelt — which, if not caught, might otherwise fail at runtime. "},{"title":"Annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#annotations","content":"The JSON Schema standard introduces the concept ofannotations, which are keywords that attach metadata to a location within a validated JSON document. For example, title and description can be used to annotate a schema with its meaning: properties: myField: title: My Field description: A description of myField  Flow extends JSON Schema with additional annotation keywords, which provide Flow with further instruction for how documents should be processed. What’s especially powerful about annotations is that they respond toconditionals within the schema. Consider a schema validating a positive or negative number: type: number oneOf: - exclusiveMinimum: 0 description: A positive number. - exclusiveMaximum: 0 description: A negative number. - const: 0 description: Zero.  Here, the activated description of this schema location depends on whether the integer is positive, negative, or zero. "},{"title":"Writing schemas​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#writing-schemas","content":"Your schema can be quite permissive or as strict as you wish. There are a few things to know, however. The top-level type must be object. Flow adds a bit of metadata to each of your documents under the _meta property, which can only be done with a top-level object. Any fields that are part of the collection's key must provably exist in any document that validates against the schema. Put another way, every document within a collection must include all of the fields of the collection's key, and the schema must guarantee that. For example, the following collection schema would be invalid because the id field, which is used as its key, is not required, so it might not actually exist in all documents: collections: acmeCo/whoops: schema: type: object required: [value] properties: id: {type: integer} value: {type: string} key: [/id]  To fix the above schema, change required to [id, value]. Learn more of how schemas can be expressed within collections. "},{"title":"Organization​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#organization","content":"JSON schema has a $ref keyword which is used to reference a schema stored elsewhere. Flow resolves $ref as a relative URL of the current file, and also supportsJSON fragment pointersfor referencing a specific schema within a larger schema document, such as ../my/widget.schema.yaml#/path/to/schema. It's recommended to use references in order to organize your schemas for reuse. $ref can also be used in combination with other schema keywords to further refine a base schema. Here's an example that uses references to organize and further tighten the constraints of a reused base schema: flow.yamlschemas.yaml collections: acmeCo/coordinates: key: [/id] schema: schemas.yaml#/definitions/coordinate acmeCo/integer-coordinates: key: [/id] schema: schemas.yaml#/definitions/integer-coordinate acmeCo/positive-coordinates: key: [/id] schema: # Compose a restriction that `x` &amp; `y` must be positive. $ref: schemas.yaml#/definitions/coordinate properties: x: {exclusiveMinimum: 0} y: {exclusiveMinimum: 0}  tip You can write your JSON schemas as either YAML or JSON across any number of files, all referenced from Flow catalog files or other schemas. Schema references are always resolved as URLs relative to the current file, but you can also use absolute URLs to a third-party schema likeschemastore.org. "},{"title":"Reductions​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reductions","content":"Flow collections have keys, and multiple documents may be added to collections that share a common key. When this happens, Flow will opportunistically merge all such documents into a single representative document for that key through a process known as reduction. Flow's default is simply to retain the most recent document of a given key, which is often the behavior that you're after. Schema reduce annotations allow for far more powerful behaviors. The Flow runtime performs reductions frequently and continuously to reduce the overall movement and cost of data transfer and storage. A torrent of input collection documents can often become a trickle of reduced updates that must be stored or materialized into your endpoints. info Flow never delays processing in order to batch or combine more documents, as some systems do (commonly known as micro-batches, or time-based polling). Every document is processed as quickly as possible, from end to end. Instead Flow uses optimistic transaction pipelining to do as much useful work as possible, while it awaits the commit of a previous transaction. This natural back-pressure affords plenty of opportunity for data reductions while minimizing latency. "},{"title":"reduce annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reduce-annotations","content":"Reduction behaviors are defined by reduceJSON schema annotationswithin your document schemas. These annotations provide Flow with the specific reduction strategies to use at your various document locations. If you're familiar with the map and reduce primitives present in Python, Javascript, and many other languages, this should feel familiar. When multiple documents map into a collection with a common key, Flow reduces them on your behalf by using your reduce annotations. Here's an example that sums an integer: type: integer reduce: { strategy: sum } # 1, 2, -1 =&gt; 2  Or deeply merges a map: type: object reduce: { strategy: merge } # {&quot;a&quot;: &quot;b&quot;}, {&quot;c&quot;: &quot;d&quot;} =&gt; {&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: &quot;d&quot;}  Learn more in thereductions strategiesreference documentation. Reductions and collection keys​ Reduction annotations change the common patterns for how you think about collection keys. Suppose you are building a reporting fact table over events of your business. Today you would commonly consider a unique event ID to be its natural key. You would load all events into your warehouse and perform query-time aggregation. When that becomes too slow, you periodically refresh materialized views for fast-but-stale queries. With Flow, you instead use a collection key of your fact table dimensions, and use reduce annotations to define your metric aggregations. A materialization of the collection then maintains a database table which is keyed on your dimensions, so that queries are both fast and up to date. Composition with conditionals​ Like any other JSON Schema annotation,reduce annotations respond to schema conditionals. Here we compose append and lastWriteWins strategies to reduce an appended array which can also be cleared: type: array oneOf: # If the array is non-empty, reduce by appending its items. - minItems: 1 reduce: { strategy: append } # Otherwise, if the array is empty, reset the reduced array to be empty. - maxItems: 0 reduce: { strategy: lastWriteWins } # [1, 2], [3, 4, 5] =&gt; [1, 2, 3, 4, 5] # [1, 2], [], [3, 4, 5] =&gt; [3, 4, 5] # [1, 2], [3, 4, 5], [] =&gt; []  Combining schema conditionals with annotations can be used to buildrich behaviors. "},{"title":"Storage mappings","type":0,"sectionRef":"#","url":"concepts/storage-mappings/","content":"","keywords":""},{"title":"Recovery logs​","type":1,"pageTitle":"Storage mappings","url":"concepts/storage-mappings/#recovery-logs","content":"Flow tasks — captures, derivations, and materializations — use recovery logs to durably store their processing context. Recovery logs are an opaque binary log, but may contain user data and are stored within the user’s buckets. They must have a defined storage mapping. The recovery logs of a task are always prefixed by recovery/, and a task named acmeCo/produce-TNT would require a storage mapping like: storageMappings: recovery/acmeCo/: stores: - provider: S3 bucket: acmeco-recovery  You may wish to use a separate bucket for recovery logs, distinct from the bucket where collection data is stored. Buckets holding collection data are free to use a bucket lifecycle policy to manage data retention; for example, to remove data after six months. This is not true of buckets holding recovery logs. Flow prunes data from recovery logs once it is no longer required. warning Deleting data from recovery logs while it is still in use can cause Flow processing tasks to fail permanently. "},{"title":"Registration and setup","type":0,"sectionRef":"#","url":"getting-started/installation/","content":"","keywords":""},{"title":"Get started with the Flow web application​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#get-started-with-the-flow-web-application","content":"You can sign up to get started as a Flow trial user by visiting the web application here. Once you've signed up with your personal information, an Estuary team member will be in touch to activate your account and discuss your business use-case, if applicable. "},{"title":"Get started with the Flow CLI​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#get-started-with-the-flow-cli","content":"After your account has been activated through the web app, you can begin to work with your data flows from the command line. This is not required, but it enables more advanced workflows or might simply be your preference. Flow has a single binary, flowctl. flowctl binaries for MacOS and Linux are available. To install, copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flowctl/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flowctl/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl  Alternatively, you can find the source files on GitHub here. Learn more about using flowctl. "},{"title":"Self-hosting Flow​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#self-hosting-flow","content":"The Flow runtime is available under the Business Source License. It's possible to self-host Flow using a cloud provider of your choice. Beta Setup for self-hosting is not covered in this documentation, and full support is not guaranteed at this time. We recommend using the hosted version of Flow for the best experience. If you'd still like to self-host, refer to the GitHub repository or the Estuary Slack. "},{"title":"What's next?​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#whats-next","content":"Start using Flow with these recommended resources. Create your first data flow: Follow this guide to create your first data flow in the Flow web app, while learning essential flow concepts. High level concepts: Start here to learn more about important Flow terms. "},{"title":"Tests","type":0,"sectionRef":"#","url":"concepts/tests/","content":"","keywords":""},{"title":"Ingest​","type":1,"pageTitle":"Tests","url":"concepts/tests/#ingest","content":"ingest steps add documents to a named collection. All documents must validate against the collection'sschema, or a catalog build error will be reported. All documents from a single ingest step are added in one transaction. This means that multiple documents with a common key will be combined priorto their being appended to the collection. Suppose acmeCo/people had key [/id]: tests: acmeCo/tests/greetings: - ingest: description: Zeldas are combined to one added document. collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda One&quot; } - { userId: 1, name: &quot;Zelda Two&quot; } - verify: description: Only one Zelda is greeted. collection: acmeCo/greetings documents: - { userId: 1, greeting: &quot;Hello Zelda Two&quot; }  "},{"title":"Verify​","type":1,"pageTitle":"Tests","url":"concepts/tests/#verify","content":"verify steps assert that the current contents of a collection match the provided document fixtures. Verified documents are fully reduced, with one document for each unique key, ordered under the key's natural order. You can verify the contents of both derivations and captured collections. Documents given in verify steps do not need to be comprehensive. It is not an error if the actual document has additional locations not present in the document to verify, so long as all matched document locations are equal. Verified documents also do not need to validate against the collection's schema. They do, however, need to include all fields that are part of the collection's key. tests: acmeCo/tests/greetings: - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda&quot; } - { userId: 2, name: &quot;Link&quot; } - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda Again&quot; } - { userId: 3, name: &quot;Pikachu&quot; } - verify: collection: acmeCo/greetings documents: # greetings are keyed on /userId, and the second greeting is kept. - { userId: 1, greeting: &quot;Hello Zelda Again&quot; } # `greeting` is &quot;Hello Link&quot;, but is not asserted here. - { userId: 2 } - { userId: 3, greeting: &quot;Hello Pikachu&quot; }  "},{"title":"Partition selectors​","type":1,"pageTitle":"Tests","url":"concepts/tests/#partition-selectors","content":"Verify steps may include a partition selector to verify only documents of a specific partition: tests: acmeCo/tests/greetings: - verify: collection: acmeCo/greetings description: Verify only documents which greet Nintendo characters. documents: - { userId: 1, greeting: &quot;Hello Zelda&quot; } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } partitions: include: platform: [Nintendo]  Learn more about partition selectors. "},{"title":"Tips​","type":1,"pageTitle":"Tests","url":"concepts/tests/#tips","content":"The following tips can aid in testing large or complex derivations. "},{"title":"Testing reductions​","type":1,"pageTitle":"Tests","url":"concepts/tests/#testing-reductions","content":"Reduction annotations are expressive and powerful, and their use should thus be tested thoroughly. An easy way to test reduction annotations on captured collections is to write a two-step test that ingests multiple documents with the same key and then verifies the result. For example, the following test might be used to verify the behavior of a simple sum reduction: tests: acmeCo/tests/sum-reductions: - ingest: description: Ingest documents to be summed. collection: acmeCo/collection documents: - {id: 1, value: 5} - {id: 1, value: 4} - {id: 1, value: -3} - verify: description: Verify value was correctly summed. collection: acmeCo/collection documents: - {id: 1, value: 6}  "},{"title":"Reusing common fixtures​","type":1,"pageTitle":"Tests","url":"concepts/tests/#reusing-common-fixtures","content":"When you write a lot of tests, it can be tedious to repeat documents that are used multiple times. YAML supports anchors and references, which you can implement to re-use common documents throughout your tests. One nice pattern is to define anchors for common ingest steps in the first test, which can be re-used by subsequent tests. For example: tests: acmeCo/tests/one: - ingest: &amp;mySetup collection: acmeCo/collection documents: - {id: 1, ...} - {id: 2, ...} ... - verify: ... acmeCo/tests/two: - ingest: *mySetup - verify: ...  This allows all the subsequent tests to re-use the documents from the first ingest step without having to duplicate them. "},{"title":"Configure connections with SSH tunneling","type":0,"sectionRef":"#","url":"guides/connect-network/","content":"","keywords":""},{"title":"General setup​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#general-setup","content":"Activate an SSH implementation on a server, if you don't have one already. Consult the documentation for your server's operating system and/or cloud service provider, as the steps will vary. Configure the server to your organization's standards, or reference the SSH documentation for basic configuration options. Referencing the config files and shell output, collect the following information: The SSH user, which will be used to log into the SSH server, for example, sshuser. You may choose to create a new user for this workflow.The SSH endpoint for the SSH server, formatted as ssh://user@hostname[:port]. This may look like the any of following: ssh://sshuser@ec2-198-21-98-1.compute-1.amazonaws.comssh://sshuser@198.21.98.1ssh://sshuser@198.21.98.1:22 Hint The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Taken together, these configuration details would allow you to log into the SSH server from your local machine. They'll allow the connector to do the same. Configure your internal network to allow the SSH server to access your capture or materialization endpoint. Configure your network to expose the SSH server endpoint to external traffic. The method you use depends on your organization's IT policies. Currently, Estuary doesn't provide a list of static IPs for whitelisting purposes, but if you require one, contact Estuary support. "},{"title":"Setup for AWS​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-aws","content":"To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com. "},{"title":"Setup for Google Cloud​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-google-cloud","content":"To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM). Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address. "},{"title":"Setup for Azure​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-azure","content":"To allow SSH tunneling to a database instance hosted on Azure, you'll need to create a virtual machine (VM) in the same virtual network as your endpoint database. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Create and connect to a VM in a virtual network, and add the endpoint database to the network. Create a new virtual network and subnet. Create a Linux or Windows VM within the virtual network, directing the SSH public key source to the public key you generated previously. Note the VM's public IP; you'll need this later. Create a service endpoint for your database in the same virtual network as your VM. Instructions for Azure Database For PostgreSQL can be found here; note that instructions for other database engines may be different. "},{"title":"Configuration​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#configuration","content":"After you've completed the prerequisites, you should have the following parameters: SSH Endpoint / sshEndpoint: the remote SSH server's hostname, or public IP address, formatted as ssh://user@hostname[:port] The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made. Private Key / privateKey: the contents of the SSH private key file Use these to add SSH tunneling to your capture or materialization definition, either by filling in the corresponding fields in the web app, or by working with the YAML directly. Reference the Connectors page for a YAML sample. "},{"title":"Create a simple data flow","type":0,"sectionRef":"#","url":"guides/create-dataflow/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#prerequisites","content":"This guide is intended for new Flow users and briefly introduces Flow's key concepts. Though it's not required, you may find it helpful to read the high level concepts documentation for more detail before you begin. "},{"title":"Introduction​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#introduction","content":"In Estuary Flow, you create data flows to connect data source and destination systems. The set of specifications that defines a data flow is known as a catalog, and is made of several important entities. The simplest Flow catalog comprises three types of entities: A data capture, which ingests data from an external sourceOne or more collections, which store that data in a cloud-backed data lakeA materialization, to push the data to an external destination Almost always, the capture and materialization each rely on a connector. A connector is a plug-in component that interfaces between Flow and whatever data system you need to connect to. Here, we'll walk through how to leverage various connectors, configure them, and deploy your catalog to create an active data flow. "},{"title":"Create a capture​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#create-a-capture","content":"You'll first create a capture to connect to your data source system. This process will create one or more collections in Flow, which you can then materialize to another system. Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Captures tab and choose New capture. On the Create Capture page, choose a name for your capture. Your capture name must begin with a prefix to which you have access. Click inside the Name field to generate a drop-down menu of available prefixes, and select your prefix. Append a unique capture name after the / to create the full name, for example acmeCo/myFirstCapture. Use the Connector drop down to choose your desired data source. A form appears with the properties required for that connector. More details are on each connector are provided in the connectors reference. Fill out the required properties and click Discover Endpoint. Flow uses the provided information to initiate a connection to the source system. It identifies one or more data resources — these may be tables, data streams, or something else, depending on the connector. Each resource is mapped to a collection through a binding. If there's an error, you'll be prompted to fix and test your configuration. Look over the generated capture definition and the schema of the resulting Flow collection(s). Flow generates catalog specifications as YAML files. You can modify it by filling in new values in the form and clicking Discover Endpoint, or by editing the YAML files directly in the web application. (Those who prefer a command-line interface can manage and edit YAML in their preferred development environment). It's not always necessary to review and edit the YAML — Flow will prevent the publication of invalid catalogs. Once you're satisfied with the configuration, click Save and publish. You'll see a notification when the capture publishes successfully. Click Materialize collections to continue. "},{"title":"Create a materialization​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#create-a-materialization","content":"Now that you've captured data into one or more collections, you can materialize it to a destination. The New Materializations page is pre-populated with the capture and collection you just created. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/myFirstMaterialization. Use the Connector drop down to choose your desired data destination. The rest of the page populates with the properties required for that connector. More details are on each connector are provided in the connectors reference. Fill out the required properties and click Discover Endpoint. Flow initiates a connection with the destination system, and creates a binding to map each collection in your catalog to a resource in the destination. Again, these may be tables, data streams, or something else. When you publish the complete catalog, Flow will create these new resources in the destination. Look over the generated materialization definition and edit it, if you'd like. Click Save and publish. You'll see a notification when the full data flow publishes successfully. "},{"title":"What's next?​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#whats-next","content":"Now that you've deployed your first data flow, you can explore more possibilities. Read the high level concepts to better understand how Flow works and what's possible. Create more complex data flows by mixing and matching collections in your captures and materializations. For example: Materialize the same collection to multiple destinations. If a capture produces multiple collections, materialize each one to a different destination. Materialize collections that came from different sources to the same destination. Advanced users can modify collection schemas, apply data reductions, or transform data with a derivation(derivations are currently available using the CLI, but support in the web application is coming soon.) "},{"title":"Who should use Flow?","type":0,"sectionRef":"#","url":"overview/who-should-use-flow/","content":"","keywords":""},{"title":"Benefits​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#benefits","content":"These characteristics set Flow apart from other data integration workflows and address the pain points listed above. "},{"title":"Fully integrated pipelines​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#fully-integrated-pipelines","content":"With Flow, you can build, test, and evolve pipelines that continuously capture, transform, and materialize data across all of your systems. With one tool, you can power workflows that have historically required you to first piece together services, then integrate and operate them in-house to meet your needs. To achieve comparable capabilities to Flow you would need: A low-latency streaming system, such as AWS KinesisData lake build-out, such as Kinesis Firehose to S3Custom ETL application development, such as Spark, Flink, or AWS λSupplemental data stores for intermediate transformation statesETL job management and execution, such as a self-hosting or Google Cloud DataflowCustom reconciliation of historical vs streaming datasets, including onerous backfills of new streaming applications from historical data Flow dramatically simplifies this inherent complexity. It saves you time and costs, catches mistakes before they hit production, and keeps your data fresh across all the places you use it. With both a UI-forward web application and a powerful CLI , more types of professionals can contribute to what would otherwise require a highly specialized set of technical skills. "},{"title":"Efficient architecture​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#efficient-architecture","content":"Flow mixes a variety of architectural techniques to deliver great throughput, avoid latency, and minimize operating costs. These include: Leveraging reductions to reduce the amount of data that must be ingested, stored, and processed, often dramaticallyExecuting transformations predominantly in-memoryOptimistic pipelining and vectorization of internal remote procedure calls (RPCs) and operationsA cloud-native design that optimizes for public cloud pricing models Flow also makes it easy to materialize focused data views directly into your warehouse, so you don't need to repeatedly query the much larger source datasets. This can dramatically lower warehouse costs. "},{"title":"Powerful transformations​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#powerful-transformations","content":"With Flow, you can build pipelines that join a current event with an event that happened days, weeks, even years in the past. Flow can model arbitrary stream-to-stream joins without the windowing constraints imposed by other systems, which limit how far back in time you can join. Flow transforms data in durable micro-transactions, meaning that an outcome, once committed, won't be silently re-ordered or changed due to a crash or machine failure. This makes Flow uniquely suited for operational workflows, like assigning a dynamic amount of available inventory to a stream of requests — decisions that, once made, should not be forgotten. You can also evolve transformations as business requirements change, enriching them with new datasets or behaviors without needing to re-compute from scratch. "},{"title":"Data integrity​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#data-integrity","content":"Flow is architected to ensure that your data is accurate and that changes don't break pipelines. It supports strong schematization, durable transactions with exactly-once semantics, and easy end-to-end testing. Required JSON schemas ensure that only clean, consistent data is ingested into Flow or written to external systems. If a document violates its schema, Flow pauses the pipeline, giving you a chance to fix the error.Schemas can encode constraints, like that a latitude value must be between +90 and -90 degrees, or that a field must be a valid email address.Flow can projects JSON schema into other flavors, like TypeScript types or SQL tables. Strong type checking catches bugs before they're applied to production.Flow's declarative tests verify the integrated, end-to-end behavior of data flows. "},{"title":"Dynamic scaling​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#dynamic-scaling","content":"The Flow runtime scales from a single process up to a large Kubernetes cluster for high-volume production deployments. Processing tasks are quickly reassigned upon any machine failure for high availability. Each process can also be scaled independently, at any time, and without downtime. This is unique to Flow. Comparable systems require that an arbitrary data partitioning be decided upfront, a crucial performance knob that's awkward and expensive to change. Instead, Flow can repeatedly split a running task into two new tasks, each half the size, without stopping it or impacting its downstream uses. "},{"title":"Comparisons","type":0,"sectionRef":"#","url":"overview/comparisons/","content":"","keywords":""},{"title":"Apache Beam and Google Cloud Dataflow​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#apache-beam-and-google-cloud-dataflow","content":"Flow’s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there’s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences. Like Beam, Flow’s primary primitive is a collection. You build a processing graph (called a pipeline in Beam and a catalog in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow’s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas. Unlike Beam, Flow doesn’t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types. Also, while Beam allows you the option to define combine operators, Flow’s runtime always applies combine operators. These are built using the declared semantics of the document’s schema, which makes it much more efficient and cost-effective to work with streaming data. Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow’s modeling of state – via its per-key register concept – is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer. "},{"title":"Kafka​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#kafka","content":"Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem. Flow is built on Gazette, a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette’s consumer framework, which has similarities to Kafka consumers. Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics. Journals in Gazette and Flow are roughly analogous to Kafka partitions. Each journal is a single append-only log. Gazette has no native notion of a topic, but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called fragments, which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently. Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called collections. Flow collections are somewhat similar to Kafka streams, but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning. Flow tasks are most similar to Kafka stream processors, but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls shards, to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling. "},{"title":"Spark​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#spark","content":"Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems. You can think of a Flow collection as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can materialize to push the data into some external system like cloud storage or a database. You can also create a collection that is derived by applying stateful transformations to one or more source collections. Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data. Spark's processing primitives, applications, jobs, and tasks, don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the catalog. A Flow catalog is a composition of Flow tasks, which are quite different from tasks in Spark. In Flow, a task is a logical unit of work that does one of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow shard. In Flow, a task is a logical unit of work, and shards represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling. Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations. "},{"title":"Hadoop, HDFS, and Hive​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#hadoop-hdfs-and-hive","content":"There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow. To gain an understanding of Flow's processing model for derivations, see this blog post about MapReduce in Flow. HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive. For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing collections, rather than creating a new one each time a job is run. In fact, Flow has no notion of a job like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. This blog post explores those differences in greater depth. To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries. In Flow, you instead define a capture of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow derivations, which again apply the transformations incrementally and in real time. While you could actually use tools like Hive to directly query data from Flow collections — the layout of collection data in cloud storage is intentionally compatible with this — you could also materialize a view of your transformation results to any database, which is also kept up to date in real time. "},{"title":"Fivetran, Airbyte, and other ELT solutions​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#fivetran-airbyte-and-other-elt-solutions","content":"Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the world in terms of independent scheduled jobs that copy data from source to destination, Flow catalogs model a directed graph ofcaptures (reads from sources),derivations (transforms), andmaterializations (writes to destinations). Collectively, these are called tasks. Tasks in Flow are only indirectly linked. Captures read data from a source and output to collections. Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing*. Like Airbyte, Flow uses connectors for interacting with external systems in captures and materializations. For captures, Flow integrates the Airbyte specification, so all Airbyte source connectors can be used with Flow. For materializations, Flow uses its own protocol which is not compatible with the Airbyte spec. In either case, the usage of connectors is pretty similar. In terms of technical capabilities, Flow can do everything that these tools can and more. Both Fivetran and Airbyte both currently have graphical interfaces that make them much easier for non-technical users to configure. Flow, too, is focused on empowering non-technical users through its web application. At the same time, it Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow. * Some materialization endpoints can only make at-least-once guarantees. "},{"title":"dbt​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#dbt","content":"dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively. In addition to – and perhaps more important than – its transform capability, dbt brought an entirely new workflow for working with data: one that prioritizes version control, testing, local development, documentation, composition, and re-use. Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse. Flow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else. These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse. Within the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to productionize important insights as materialized views or by pushing to another destination. Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in a separate, dedicated tool like dbt. While Flow and dbt don’t interact directly, both offer easy integration through your data warehouse. "},{"title":"Materialize, Rockset, ksqlDB, and other real-time databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#materialize-rockset-ksqldb-and-other-real-time-databases","content":"Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers, and can keep SQL views up to date in real time. These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not. However, this doesn't mean that Flow should replace these systems in your stack. In fact, it can be optimal to use Flow to feed data into them. Flow adds real-time data capture and materialization options that many real-time databases don't support. Once data has arrived in the database, you have access to real-time SQL analysis and other analytical tools not native to Flow. For further explanation, read the section below on OLAP databases. "},{"title":"Snowflake, BigQuery, and other OLAP databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#snowflake-bigquery-and-other-olap-databases","content":"Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by materializing views into it. Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow catalogs define collections. These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key. Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run. Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date. A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow’s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant. "},{"title":"Authorizing users and authenticating with Flow","type":0,"sectionRef":"#","url":"reference/authentication/","content":"","keywords":""},{"title":"Subjects, objects, and inherited capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#subjects-objects-and-inherited-capabilities","content":"The entity to which you grant a capability is called the subject, and the entity over which access is granted is called the object. The subject can be either a user or a prefix, and the object is always a prefix. This allows subjects to inherit nested capabilities, so long as they are granted admin. For example, user X of Acme Co has admin access to the acmeCo/ prefix, and user Y has write access. A third party has granted acmeCo/ read access to shared data at outside-org/acmeCo-share/. User X automatically inherits read access to outside-org/acmeCo-share/, but user Y does not. "},{"title":"Default authorization settings​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#default-authorization-settings","content":"When you first sign up to use Flow, your organization is provisioned a prefix, and your username is granted admin access to the prefix. Your prefix is granted write access to itself and read access to its logs, which are stored under a unique sub-prefix of the global ops/ prefix. Using the same example, say user X signs up on behalf of their company, AcmeCo. User X is automatically granted admin access to the acmeCo/ prefix.acmeCo/, in turn, has write access to acmeCo/ and read access to ops/acmeCo/. As more users and prefixes are added, admins can provision capabilities using the CLI. "},{"title":"Authenticating Flow in the web app​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-in-the-web-app","content":"You must sign in to begin a new session using the Flow web application. For the duration of the session, you'll be able to perform actions depending on the capabilities granted to the user profile. You can view the capabilities currently provisioned in your organization on the Admin tab. "},{"title":"Authenticating Flow using the CLI​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-using-the-cli","content":"You can use the flowctl CLI to work with your organization's catalogs and drafts in your local development environment. To authenticate a local development session using the CLI, do the following: Sign into the Flow web application. Click the Admin tab, scroll to the Access Token box, and copy the token. In the terminal of your local development environment, run: flowctl auth token --token=&lt;copied-token&gt;  The token will expire after a predetermined duration. Generate a new token using the web application and re-authenticate. "},{"title":"Provisioning capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#provisioning-capabilities","content":"As an admin, you can provision capabilities using the CLI with the subcommands of flowctl auth roles. For example: flowctl auth roles list returns a list of all currently provisioned capabilities flowctl auth roles grant --object-role=acmeCo/ --capability=admin --subject-user-id=userZ grants user Z admin access to acmeCo flowctl auth roles revoke --object-role=outside-org/acmeCo-share/ --capability=read --subject-role=acmeCo/ would be used by an admin of outside-orgto revoke acmeCo/'s read access to outside-org/acmeCo-share/. You can find detailed help for all subcommands using the --help or -h flag. "},{"title":"Connectors","type":0,"sectionRef":"#","url":"reference/Connectors/","content":"Connectors A current list and configuration details for Estuary's connectors can be found on the following pages: Capture connectorsMaterialization connectors You can learn more about how connectors work and how to use them in their conceptual documentation.","keywords":""},{"title":"Configuring task shards","type":0,"sectionRef":"#","url":"reference/Configuring-task-shards/","content":"","keywords":""},{"title":"Properties​","type":1,"pageTitle":"Configuring task shards","url":"reference/Configuring-task-shards/#properties","content":"Property\tTitle\tDescription\tType/disable\tDisable\tDisable processing of the task's shards.\tBoolean /logLevel\tLog level\tLog levels may currently be \\&quot;error\\&quot;, \\&quot;warn\\&quot;, \\&quot;info\\&quot;, \\&quot;debug\\&quot;, or \\&quot;trace\\&quot;. If not set, the effective log level is \\&quot;info\\&quot;.\tString /maxTxnDuration\tMaximum transaction duration\tThis duration upper-bounds the amount of time during which a transaction may process documents before it must initiate a commit. Note that it may take some additional time for the commit to complete after it is initiated. The shard may run for less time if there aren't additional ready documents for it to process. If not set, the maximum duration defaults to one second.\tString /minTxnDuration\tMinimum transaction duration\tThis duration lower-bounds the amount of time during which a transaction must process documents before it must flush and commit. It may run for more time if additional documents are available. The default value is zero seconds.\tString For more information about these controls and when you might need to use them, see: TransactionsLog level "},{"title":"Sample​","type":1,"pageTitle":"Configuring task shards","url":"reference/Configuring-task-shards/#sample","content":"materializations: acmeCo/snowflake-materialization: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret cloud_provider: aws region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev bindings: - resource: table: anvils source: acmeCo/anvils shards: logLevel: debug minTxnDuration: 30s maxTxnDuration: 4m  "},{"title":"Capture connectors","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/","content":"","keywords":""},{"title":"Available capture connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#available-capture-connectors","content":"Amazon Kinesis ConfigurationPackage — ghcr.io/estuary/source-kinesis:dev Amazon S3 ConfigurationPackage — ghcr.io/estuary/source-s3:dev Amplitude ConfigurationPackage - ghcr.io/estuary/source-amplitude:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/source-kafka:dev Exchange Rates API ConfigurationPackage - ghcr.io/estuary/source-exchange-rates:dev Facebook Marketing ConfigurationPackage - ghcr.io/estuary/source-facebook-marketing:dev GitHub ConfigurationPackage - ghcr.io/estuary/source-github:dev Google Ads ConfigurationPackage - ghcr.io/estuary/source-google-ads:dev Google Analytics ConfigurationPackage - ghcr.io/estuary/source-google-analytics-v4:dev Google Cloud Storage ConfigurationPackage — ghcr.io/estuary/source-gcs:dev Google Firestore ConfigurationPackage - ghcr.io/estuary/source-firestore:dev Google Sheets ConfigurationPackage - ghcr.io/estuary/source-google-sheets:dev HTTP file ConfigurationPackage - ghcr.io/estuary/source-http-file:dev Hubspot ConfigurationPackage - ghcr.io/estuary/source-hubspot:dev Intercom ConfigurationPackage - ghcr.io/estuary/source-intercom:dev LinkedIn Ads ConfigurationPackage - ghcr.io/estuary/source-linkedin-ads:dev Mailchimp ConfigurationPackage - ghcr.io/estuary/source-mailchimp:dev MySQL ConfigurationPackage - ghcr.io/estuary/source-mysql:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/source-postgres:dev Stripe ConfigurationPackage - ghcr.io/estuary/source-stripe:dev Zendesk Support ConfigurationPackage - ghcr.io/estuary/source-zendesk-support:dev "},{"title":"Amazon Kinesis","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-kinesis/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#prerequisites","content":"To use this connector, you'll need: One or more Amazon Kinesis streams. For a given capture, all streams must: Contain JSON data onlyBe in the same AWS region An IAM user with the following permissions: ListShards on all resourcesGetRecords on all streams usedGetShardIterator on all streams usedDescribeStream on all streams usedDescribeStreamSummary on all streams used These permissions should be specified with the kinesis: prefix in an IAM policy document. For more details and examples, see Controlling Access to Amazon Kinesis Data in the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon Kinesis source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-secret-access-key&quot; /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a kinesis-compatible API that isn't provided by AWS.\tstring /region\tAWS region\tThe name of the AWS region where the Kinesis stream is located.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tStream name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kinesis:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: ${STREAM_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kinesis stream. Learn more about capture definitions.. "},{"title":"Amazon S3","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-s3/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#prerequisites","content":"To use this connector, either your S3 bucket must be public, or you must have access via a root or IAM user. For public buckets, verify that the access policy allows anonymous reads.For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the S3 source connector. tip You might organize your S3 bucket using prefixes to emulate a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /awsAccessKeyId\tAWS Access Key ID\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /awsSecretAccessKey\tAWS Secret Access Key\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /bucket\tBucket\tName of the S3 bucket\tstring\tRequired /endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to. Use if you're capturing from a S3-compatible API that isn't provided by AWS\tstring /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-s3:dev config: bucket: &quot;my-bucket&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different S3 prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#advanced-parsing-cloud-storage-data","content":"Cloud storage platforms like S3 can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically. Avro CSV JSON W3C Extended Log info At this time, Flow only supports S3 captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type. CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Amplitude","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amplitude/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#supported-data-resources","content":"The following data resources are supported through the Amplitude APIs: Active User CountsAnnotationsAverage Session LengthCohortsEvents By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#prerequisites","content":"An Amplitude project with an API Key and Secret Key "},{"title":"Configuration​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amplitude source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tAmplitude API Key.\tstring\tRequired /secret_key\tSecret Key\tAmplitude Secret Key.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Amplitude project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amplitude:dev config: api_key: &lt;secret&gt; secret_key: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: cohorts syncMode: full_refresh target: ${PREFIX}/cohorts - resource: stream: annotations syncMode: full_refresh target: ${PREFIX}/annotations - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: active_users syncMode: incremental target: ${PREFIX}/activeusers - resource: stream: average_session_length syncMode: incremental target: ${PREFIX}/averagesessionlength  "},{"title":"Exchange Rates API","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/exchange-rates/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#prerequisites","content":"An API key generated through an Exchange Rate API account. After you sign up, your API key can be found on your account page. You may use the free account, but note that you'll be limited to the default base currency, EUR. "},{"title":"Configuration​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Exchange Rates source connector. "},{"title":"Properties​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/access_key\tAccess key\tYour API access key. The key is case sensitive.\tstring\tRequired /base\tBase currency\tISO reference currency. See the documentation. Free plan doesn't support Source Currency Switching, default base currency is EUR\tstring\tEUR /ignore_weekends\tIgnore weekends\tIgnore weekends? (Exchanges don't run on weekends)\tboolean\ttrue /start_date\tStart date\tThe date in the format YYYY-MM-DD. Data will begin from this date.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData stream from which Flow captures data. Always set to exchange_rates.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-exchange-rates:dev config: base: EUR access_key: &lt;secret&gt; start_date: 2022-01-01 ignore_weekends: true bindings: - resource: stream: exchange_rates syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  This capture definition should only have one binding, as exchange_rates is the only available data stream. Learn more about capture definitions. "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/apache-kafka/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#prerequisites","content":"A Kafka cluster with: bootstrap.servers configured so that clients may connect via the desired host and portAn authentication mechanism of choice set up (highly recommended for production environments)Connection security enabled with TLS (highly recommended for production environments) "},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#authentication-and-connection-security","content":"Neither authentication nor connection security are enabled by default in your Kafka cluster, but both are important considerations. Similarly, Flow's Kafka connectors do not strictly require authentication or connection security mechanisms. You may choose to omit them for local development and testing; however, both are strongly encouraged for production environments. A wide variety of authentication methods is available in Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. Behavior using other authentication methods is not guaranteed. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol. If you don't already have authentication enabled on your cluster, Estuary recommends either of listed SASL/SCRAM methods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications. For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentation for details. Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Apache Kafka source connector. "},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to. The Kafka client will be informed of the rest of the cluster nodes by connecting to one of these nodes.\tarray\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /authentication\tAuthentication\tConnection details used to authenticate a client connection to Kafka via SASL.\tnull, object /authentication/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /authentication/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /authentication/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tKafka topic name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: [localhost:9093] tls: system_certificates authentication: mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman bindings: - resource: stream: ${TOPIC_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kafka topic. Learn more about capture definitions.. "},{"title":"Facebook Marketing","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/facebook-marketing/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#supported-data-resources","content":"The following data resources are supported: AdsAd activitiesAd creativesAd insightsBusiness ad accountsCampaignsImagesVideos By default, each resource associated with your Facebook Business account is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#prerequisites","content":"A Facebook Business account, and its Ad Account ID.A Facebook app with: The Marketing API enabled.A Marketing API access token generated.Access upgrade from Standard Access (the default) to Advanced Access. This allows a sufficient rate limit to support the connector. Follow the steps below to meet these requirements. "},{"title":"Setup​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#setup","content":"Find your Facebook Business Ad Account ID. You'll use this for the Business ID property. In Meta for Developers, create a new app of the type Business. On your new app's dashboard, click the button to set up the Marketing API. On the Marketing API Tools tab, generate a Marketing API access token with all available permissions (ads_management, ads_read, read_insights, and business_management). Request Advanced Access for your app. Specifically request the Advanced Access to the following: The feature Ads Management Standard Access The permission ads_read The permission ads_management Once your request is approved, you'll have a high enough rate limit to proceed with running the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Facebook Marketing source connector. "},{"title":"Properties​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#properties","content":"Endpoint​ By default, this connector captures all data associated with your Business Ad Account. You can refine the data you capture from Facebook Marketing using the optional Custom Insights configuration. You're able to specify certain fields to capture and apply data breakdowns.Breakdowns are a feature of the Facebook Marketing Insights API that allows you to group API output by common metrics.Action breakdownsare a subset of breakdowns that must be specified separately. Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe value of the access token generated.\tstring\tRequired /business_id\tBusiness ID\tYour Facebook Business Ad Account ID. The connector will fetch all the Ad Accounts that this business has access to.\tstring\tRequired /custom_insights\tCustom Insights\tA list which contains insights entries. Each entry must have a name and can contains fields, breakdowns or action_breakdowns\tarray /custom_insights/-/action_breakdowns\tAction Breakdowns\tA list of chosen action_breakdowns to apply\tarray\t[] /custom_insights/-/action_breakdowns/-\tValidActionBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/breakdowns\tBreakdowns\tA list of chosen breakdowns to apply\tarray\t[] /custom_insights/-/breakdowns/-\tValidBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/end_date\tEnd Date\tThe date until which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z. All data generated between the start date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /custom_insights/-/fields\tFields\tA list of chosen fields to capture\tarray\t[] /custom_insights/-/fields/-\tValidEnums\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/name\tName\tThe name of the insight\tstring /custom_insights/-/start_date\tStart Date\tThe date from which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z.\tstring /custom_insights/-/time_increment\tTime Increment\tTime window in days by which to aggregate statistics. The sync will be chunked into N day intervals, where N is the number of days you specified. For example, if you set this value to 7, then all statistics will be reported as 7-day aggregates by starting from the start_date. If the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).\tinteger\t1 /end_date\tEnd Date\tThe date until which you'd like to capture data, in the format YYYY-MM-DDT00:00:00Z. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /fetch_thumbnail_images\tFetch Thumbnail Images\tIn each Ad Creative, fetch the thumbnail_url and store the result in thumbnail_data_url\tboolean\tfalse /include_deleted\tInclude Deleted\tInclude data from deleted Campaigns, Ads, and AdSets\tboolean\tfalse /page_size\tPage Size of Requests\tPage size used when sending requests to Facebook API to specify number of records per page when response has pagination. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t25 /start_date\tStart Date\tThe date from which you'd like to begin capturing data, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Facebook Marketing account from which collections are captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-facebook-marketing:dev config: access_token: &lt;secret&gt; business_id: 000000000000000 start_date: 2022-03-01T00:00:00Z custom_insights: - name: my-custom-insight fields: [ad_id, account_currency] breakdowns: [device_platform] action_breakdowns: [action_type] start_date: 2022-03-01T00:00:00Z bindings: - resource: stream: ad_account syncMode: incremental target: ${PREFIX}/ad_account - resource: stream: ad_sets syncMode: incremental target: ${PREFIX}/ad_sets - resource: stream: ads_insights syncMode: incremental target: ${PREFIX}/ads_insights - resource: stream: ads_insights_age_and_gender syncMode: incremental target: ${PREFIX}/ads_insights_age_and_gender - resource: stream: ads_insights_country syncMode: incremental target: ${PREFIX}/ads_insights_country - resource: stream: ads_insights_region syncMode: incremental target: ${PREFIX}/ads_insights_region - resource: stream: ads_insights_dma syncMode: incremental target: ${PREFIX}/ads_insights_dma - resource: stream: ads_insights_platform_and_device syncMode: incremental target: ${PREFIX}/ads_insights_platform_and_device - resource: stream: ads_insights_action_type syncMode: incremental target: ${PREFIX}/ads_insights_action_type - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns - resource: stream: activities syncMode: incremental target: ${PREFIX}/activities - resource: stream: ads syncMode: incremental target: ${PREFIX}/ads - resource: stream: ad_creatives syncMode: full_refresh target: ${PREFIX}/ad_creatives  Learn more about capture definitions. "},{"title":"Google Cloud Storage","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/gcs/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#prerequisites","content":"To use this connector, either your GCS bucket must be public, or you must have access via a Google service account. For public buckets, verify that objects in the bucket are publicly readable.For buckets accessed by a Google Service Account: Ensure that the user has been assigned a role with read access.Create a JSON service account key. Google's Application Default Credentials will use this file for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GCS source connector. tip You might use prefixes to organize your GCS bucket in a way that emulates a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /bucket\tBucket\tName of the Google Cloud Storage bucket\tstring\tRequired /googleCredentials\tGoogle Service Account\tService account JSON key to use as Application Default Credentials\tstring /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose key (relative to the prefix) matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gcs:dev config: bucket: my-bucket googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different GCS prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#advanced-parsing-cloud-storage-data","content":"Cloud storage platforms like GCS can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically. Avro CSV JSON W3C Extended Log info At this time, Flow only supports GCS captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type. CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Advanced: Configure Google service account impersonation​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#advanced-configure-google-service-account-impersonation","content":"As part of your Google IAM management, you may have configured one service account to impersonate another service account. You may find this useful when you want to easily control access to multiple service accounts with only one set of keys. If necessary, you can configure this authorization model for a GCS capture in Flow using the GitOps workflow. To do so, you'll enable sops encryption and impersonate the target account with JSON credentials. Before you begin, make sure you're familiar with how to encrypt credentials in Flow using sops. Use the following sample as a guide to add the credentials JSON to the capture's endpoint configuration. The sample uses the encrypted suffix feature of sops to encrypt only the sensitive credentials, but you may choose to encrypt the entire configuration. config: bucket: &lt;bucket-name&gt; googleCredentials_sops: # URL containing the account to impersonate and the associated project service_account_impersonation_url: https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/&lt;target-account&gt;@&lt;project&gt;.iam.gserviceaccount.com:generateAccessToken # Credentials for the account that has been configured to impersonate the target. source_credentials: # In addition to the listed fields, copy and paste the rest of your JSON key file as your normally would # for the `googleCredentials` field client_email: &lt;origin-account&gt;@&lt;anotherproject&gt;.iam.gserviceaccount.com token_uri: https://oauth2.googleapis.com/token type: service_account type: impersonated_service_account  "},{"title":"GitHub","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/github/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#supported-data-resources","content":"When you configure the connector, you specify a list of GitHub organizations and/or repositories from which to capture data. From your selection, the following data resources are captured: Full refresh (batch) resources\tIncremental (real-time supported) resourcesAssignees\tComments Branches\tCommit comment reactions Collaborators\tCommit comments Issue labels\tCommits Pull request commits\tDeployments Tags\tEvents Team members\tIssue comment reactions Team memberships\tIssue events Teams\tIssue milestones Users\tIssue reactions Issues Project cards Project columns Projects Pull request comment reactions Pull request stats Pull requests Releases Repositories Review comments Reviews Stargazers Workflow runs Workflows Each resource is mapped to a Flow collection through a separate binding. info The /start_date field is not applicable to the following resources: AssigneesBranchesCollaboratorsIssue labelsOrganizationsPull request commitsPull request statsRepositoriesTagsTeamsUsers "},{"title":"Prerequisites​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#prerequisites","content":"There are two ways to authenticate with GitHub when capturing data into Flow: using OAuth2, and manually, by generating a personal access token. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with GitHub in the Flow web app​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#using-oauth2-to-authenticate-with-github-in-the-flow-web-app","content":"A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#configuring-the-connector-specification-manually","content":"A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. A GitHub personal access token. You may use multiple tokens to balance the load on your API quota. "},{"title":"Configuration​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GitHub source connector. "},{"title":"Properties​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/branch\tBranch (Optional)\tSpace-delimited list of GitHub repository branches to pull commits for, e.g. `estuary/flow/your-branch`. If no branches are specified for a repository, the default branch will be pulled.\tstring /credentials\tAuthentication\tChoose how to authenticate to GitHub\tobject\tRequired /credentials/option_title\tAuthentication method\tSet to PAT Credentials for manual authentication\tstring /credentials/personal_access_token\tAccess token\tPersonal access token, used for manual authentication. You may include multiple access tokens as a comma separated list. /page_size_for_large_streams\tPage size for large streams (Optional)\tThe Github connector captures from several resources with a large amount of data. The page size of such resources depends on the size of your repository. We recommended that you specify values between 10 and 30.\tinteger\t10 /repository\tGitHub Repositories\tSpace-delimited list of GitHub organizations/repositories, e.g. `estuary/flow` for a single repository, `estuary/*` to get all repositories from an organization and `estuary/flow estuary/another-repo` for multiple repositories.\tstring\tRequired /start_date\tStart date\tThe date from which you'd like to replicate data from GitHub in the format YYYY-MM-DDT00:00:00Z. For the resources that support this configuration, only data generated on or after the start date will be replicated. This field doesn't apply to all resources.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGitHub resource from which collection is captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-github:dev config: credentials: option_title: PAT Credentials personal_access_token: {secret} page_size_for_large_streams: 10 repository: estuary/flow start_date: 2022-01-01T00:00:00Z bindings: - resource: stream: assignees syncMode: full_refresh target: ${PREFIX}/assignees {...}  "},{"title":"Google Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#supported-data-resources","content":"The following data resources are supported. Resources ending in _report represent legacy resources from the Google Adwords API. ad_group_adsad_group_ad_labelad_groupsad_group_labelcampaignscampaign_labelsclick_viewcustomergeographic_viewkeyword_viewuser_location_viewaccount_performance_reportad_performance_reportdisplay_keyword_performance_reportdisplay_topics_performance_reportshopping_performance_report By default, each resource is mapped to a Flow collection through a separate binding. You may also generate custom resources using GAQL queries. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#prerequisites","content":"There are two ways to authenticate with Google when capturing data into Flow: using OAuth2, and manually, using tokens and secret credentials. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the manual method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"One or more Google Ads accounts. Note each account's customer ID A Google Account that has access to the Google Ads account(s). This account may be a manager account. If so, ensure that it is linked to each Google Ads account and make note of its customer ID. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#configuring-the-connector-specification-manually","content":"One or more Google Ads accounts. Note each account's customer ID A Google Ads manager account that has been linked to each Google Ads account A Google Ads developer token. Your Google Ads manager account must be configured prior to applying for a developer token. caution Developer token applications are independently reviewed by Google and may take one or more days to be approved. Be sure to carefully review Google's requirements before submitting an application. A refresh token, which fetches a new developer tokens for you as the previous token expires. A generated Client ID and Client Secret, used for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/conversion_window_days\tConversion Window (Optional)\tA conversion window is the period of time after an ad interaction (such as an ad click or video view) during which a conversion, such as a purchase, is recorded in Google Ads. For more information, see Google's docs.\tinteger\t14 /credentials\tGoogle Credentials object\tRequired /credentials/client_id\tClient ID\tThe Client ID of your Google Ads developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Client Secret of your Google Ads developer application.\tstring\tRequired /credentials/developer_token\tDeveloper Token\tDeveloper token granted by Google to use their APIs.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe token for obtaining a new access token.\tstring\tRequired /custom_queries\tCustom GAQL Queries (Optional) array /custom_queries/-/query\tCustom Query\tA custom defined GAQL query for building the report. Should not contain segments.date expression. See Google's query builder for more information.\tstring /custom_queries/-/table_name\tDestination Table Name\tThe table name in your destination database for chosen query.\tstring /customer_id\tCustomer ID(s)\tComma separated list of (client) customer IDs. Each customer ID must be specified as a 10-digit number without dashes. More instruction on how to find this value in our docs. Metrics streams like AdGroupAdReport cannot be requested for a manager account.\tstring\tRequired /end_date\tEnd Date (Optional)\tUTC date in the format 2017-01-25. Any data after this date will not be replicated.\tstring /login_customer_id\tLogin Customer ID for Managed Accounts (Optional)\tIf your access to the customer account is through a manager account, this field is required and must be set to the customer ID of the manager account (10-digit number without dashes).\tstring /start_date\tStart Date\tUTC date in the format 2017-01-25. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGoogle Ad resource from which a collections is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-ads:dev config: conversion_window_days: 7 credentials: client_id: {secret_client_ID} client_secret: {secret_secret} developer_token: {access_token} refresh_token: {refresh_token} customer_id: 0123456789, 1234567890 login_customer_id: 0987654321 end_date: 2022-01-01 start_date: 2020-01-01 custom_queries: - query: SELECT campaign.id, campaign.name, campaign.status FROM campaign ORDER BY campaign.id table_name: campaigns_custom bindings: - resource: stream: campaign syncMode: incremental target: ${PREFIX}/campaign {...}  "},{"title":"Custom queries​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#custom-queries","content":"You can create custom resources using Google Analytics Query Language (GAQL) queries. Each generated resource will be mapped to a Flow collection. For help generating a valid query, see Google's query builder documentation. If a query fails to validate against a given Google Ads account, it will be skipped. "},{"title":"Google Analytics","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-analytics/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#supported-data-resources","content":"The following data resources are captured to Flow collections by default: Website overviewTraffic sourcesPagesLocationsMonthly active usersFour weekly active usersTwo weekly active usersWeekly active usersDaily active usersDevices Each resource is mapped to a Flow collection through a separate binding. You can also configure custom reports. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Google Analytics view: using OAuth2, and manually, by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Your Google account username and password. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#configuring-the-connector-specification-manually","content":"The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Google Analytics and Google Analytics Reporting APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source Google Analytics view. Follow the steps below to meet these prerequisites: Enable the Google Analytics and Google Analytics Reporting APIs for the Google project with which your Analytics view is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON keyDuring setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Add the service account to the Google Analytics view. Grant the account Viewer permissions (formerly known as Read &amp; Analyze permissions). "},{"title":"Configuration​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Analytics source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/credentials_json\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from GA.\tstring /start_date\tStart Date\tThe date in the format YYYY-MM-DD. Any data before this date will not be replicated.\tstring\tRequired /view_id\tView ID\tThe ID for the Google Analytics View you want to fetch data from. This can be found from the Google Analytics Account Explorer: https://ga-dev-tools.appspot.com/account-explorer/\tstring\tRequired /window_in_days\tWindow in days (Optional)\tThe amount of days each stream slice would consist of beginning from start_date. Bigger the value - faster the fetch. (Min=1, as for a Day; Max=364, as for a Year).\tinteger\t1 Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData resource from the Google Analytics view.\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Custom reports​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#custom-reports","content":"You can include data beyond the default data resources with Custom Reports. These replicate the functionality of Custom Reports in the Google Analytics Web console. To do so, fill out the Custom Reports property witha JSON array as a string with the following schema: [{&quot;name&quot;: string, &quot;dimensions&quot;: [string], &quot;metrics&quot;: [string]}]  You may specify default Google Analytics dimensions and metrics from the table below, or custom dimensions and metrics you've previously defined. Each custom report may contain up to 7 unique dimensions and 10 unique metrics. You must include the ga:date dimension for proper data flow. Supported GA dimensions\tSupported GA metricsga:browser\tga:14dayUsers ga:city\tga:1dayUsers ga:continent\tga:28dayUsers ga:country\tga:30dayUsers ga:date\tga:7dayUsers ga:deviceCategory\tga:avgSessionDuration ga:hostname\tga:avgTimeOnPage ga:medium\tga:bounceRate ga:metro\tga:entranceRate ga:operatingSystem\tga:entrances ga:pagePath\tga:exits ga:region\tga:newUsers ga:socialNetwork\tga:pageviews ga:source\tga:pageviewsPerSession ga:subContinent\tga:sessions ga:sessionsPerUser ga:uniquePageviews ga:users "},{"title":"Sample​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-analytics-v4:dev config: view_id: 000000000 start_date: 2022-03-01 credentials: auth_type: service credentials_json: &lt;secret&gt; window_in_days: 1 bindings: - resource: stream: daily_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: devices syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: four_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: locations syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: monthly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: pages syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: traffic_sources syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: two_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: website_overview syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Google Firestore","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-firestore/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#prerequisites","content":"A Google service account with: Read access to your Firestore data, via roles/datastore.viewer. You can assign this role when you create the service account, or add it to an existing service account. A generated JSON service account key for the account. "},{"title":"Configuration​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Firestore source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/googleCredentials\tCredentials\tGoogle Cloud Service Account JSON credentials.\tstring\tRequired /scan_interval\tScan Interval\tHow frequently to scan all collections to ensure consistency. See supported values. To turn off scans use the value 'never'.\tstring\tRequired, &quot;12h&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/path\tPath to Collection\tFirestore collection from which a Flow collection is captured. Supports parent/*/nested to capture all nested collections of parent's children\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-firestore:dev config: googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; scan_interval: &quot;24h&quot; bindings: - resource: path: orgs/*/runs/*/runResults/*/queryResults target: ${PREFIX}/orgs_runs_runResults_queryResults - resource: path: orgs/*/runs/*/runResults target: ${PREFIX}/orgs_runs_runResults - resource: path: orgs/*/runs target: ${PREFIX}/orgs_runs - resource: path: orgs target: ${PREFIX}/orgs  "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Sheet: using OAuth2, and manually,by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"A link to a Google spreadsheet. Simply copy the link from your browser. Your Google account username and password. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#configuring-the-connector-specification-manually","content":"A link to a Google spreadsheet. Simply copy the link from your browser. Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Share your Google spreadsheet with the service account. You may either share the sheet so that anyone with the link can view it, or share explicitly with the service account's email address. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Sheets source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tGoogle API Credentials for connecting to Google Sheets and Google Drive APIs\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/service_account_info\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /spreadsheet_id\tSpreadsheet Link\tThe link to your spreadsheet.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tSheet\tEach sheet in your Google Sheets document.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to full_refresh.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-sheets:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; spreadsheet_id: https://docs.google.com/spreadsheets/... bindings: - resource: stream: Sheet1 syncMode: full_refresh target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"HTTP file","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/http-file/","content":"","keywords":""},{"title":"Supported data types​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#supported-data-types","content":"This connector automatically captures the data hosted at the specified URL into a single Flow collection. The following file types are supported: AvroCSVJSONW3C Extended Log The following compression methods are supported: ZIPGZIPZSTD By default, Flow automatically detects the file type and compression method. If necessary, you can specify the correct file type, compression, and other properties (CSV only) using the optional parser configuration. "},{"title":"Prerequisites​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#prerequisites","content":"To use this connector, you'll need the URL to an HTTP endpoint that hosts data of one of the supported types. The HTTP endpoint must support HEAD HTTP requests, and the response to this request must include a Last-Modified header. tip You can send a test HEAD request using Curl with the -I parameter, for example:curl -I https://my-site.com/my_hosted_dataset.json.zipUse this online tool to easily do so in your browser. Some HTTP endpoints require credentials for access. If this is the case, have your username and password ready. "},{"title":"Configuration​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the HTTP file source connector. "},{"title":"Properties​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tUser credentials, if required to access the data at the HTTP URL.\tobject /credentials/password\tPassword\tPassword, if required to access the HTTP endpoint.\tstring /credentials/user\tUser\tUsername, if required to access the HTTP endpoint.\tstring /headers\tHeaders object /headers/items\tAdditional HTTP Headers\tAdditional HTTP headers when requesting the file. These are uncommon.\tarray /headers/items/-/key\tHeader Key string /headers/items/-/value\tHeader Value string /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /url\tHTTP File URL\tA valid HTTP url for downloading the source file.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tName of the dataset\tstring\tRequired /syncMode\tSync mode\tConnection method. Set to incremental for real-time updates.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-http-file:dev config: url: https://my-site.com/my_hosted_dataset.json.zip parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my_hosted_dataset.json.zip syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"Advanced: Parsing HTTP-hosted data​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#advanced-parsing-http-hosted-data","content":"HTTP endpoints can support a variety of file types. For each file type, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data at the HTTP endpoint, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the data is compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. If no file type is specified, the connector will try to determine the file type automatically Options are: AvroCSVJSONW3C Extended Log CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Advanced: Using HTTP headers​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#advanced-using-http-headers","content":"For real-time streams of unbounded size, you may need to send headers as part of your HTTP request. This is uncommon, and is supported by the optional Headers configuration. "},{"title":"Hubspot","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/hubspot/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#supported-data-resources","content":"By default, each resource associated with your Hubspot account is mapped to a Flow collection through a separate binding. The following data resources are supported for all subscription levels: CampaignsCompaniesContact ListsContactsContacts List MembershipsDeal PipelinesDealsEmail EventsEngagementsEngagements CallsEngagements EmailsEngagements MeetingsEngagements NotesEngagements TasksFormsForm SubmissionsLine ItemsOwnersProductsProperty HistoryQuotesSubscription ChangesTicketsTicket Pipelines The following data resources are supported for pro accounts (set Subscription type to pro in the configuration): Feedback SubmissionsMarketing EmailsWorkflows "},{"title":"Prerequisites​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#prerequisites","content":"There are two ways to authenticate with Hubspot when capturing data: using OAuth2, and manually, with a private app access token. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Hubspot in the Flow web app​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#using-oauth2-to-authenticate-with-hubspot-in-the-flow-web-app","content":"A Hubspot account "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#configuring-the-connector-specification-manually","content":"A Hubspot account The access token for an appropriately configured private app on the Hubspot account. Setup​ To create a private app in Hubspot and generate its access token, do the following. Ensure that your Hubspot user account has super admin privileges. In Hubspot, create a new private app. Name the app &quot;Estuary Flow,&quot; or choose another name that is memorable to you. Grant the new app Read access for all available scopes. Copy the access token for use in the connector configuration. "},{"title":"Configuration​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Hubspot source connector. "},{"title":"Properties​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#properties","content":"Endpoint​ The following properties reflect the access token authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tPrivate Application\tAuthenticate with a private app access token\tobject\tRequired /credentials/access_token\tAccess Token\tHubSpot Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot; /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /subscription_type\tYour HubSpot account subscription type\tSome streams are only available to certain subscription packages, we use this information to select which streams to pull data from.\tstring\t&quot;starter&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tData resource\tName of the data resource.\tstring\tRequired /syncMode\tSync Mode\tConnection method\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-hubspot:dev config: credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; bindings: - resource: stream: companies syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your configuration will have many more bindings representing all supported resourcesin your Hubspot account. Learn more about capture definitions. "},{"title":"LinkedIn Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/linkedin-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#supported-data-resources","content":"The following data resources are supported: AccountsAccount usersCampaign groupsCampaignsCreativesAdDirectSponsoredContents (Video ads)Ad analytics by campaignAd analytics by creative By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#prerequisites","content":"There are two ways to authenticate with LinkedIn when capturing data into Flow: using OAuth2, and manually, by creating a developer application. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the developer application method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with LinkedIn in the Flow web app​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#using-oauth2-to-authenticate-with-linkedin-in-the-flow-web-app","content":"One or more LinkedIn Ad Accounts with active campaigns. A LinkedIn user with access to the Ad Accounts from which you want to capture data. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#configuring-the-connector-specification-manually","content":"To configure without using OAuth, you'll need to create an application using the LinkedIn Marketing API, and generate its access token. Setup​ Create a marketing application on LinkedIn Developers.Apply to the LinkedIn Developer Program.Generate your access token. caution LinkedIn access tokens expire in 60 days. You must manually update your capture configuration to continue to capture data from LinkedIn. "},{"title":"Configuration​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the capture specification. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the LinkedIn Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/account_ids\tAccount IDs (Optional)\tA space-separated list of the account IDs from which to capture data. Leave empty if you want to capture data from all linked accounts.\tarray\t[] /credentials\tAuthentication object /credentials/auth_method\tAuthentication method\tSet to access_token to authenticate manually.\tstring /credentials/access_token\tAccess token\tAccess token generated from your LinkedIn Developers app.\tstring /start_date\tStart date\tUTC date in the format 2020-09-17. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tLinkedIn Ads stream from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-linkedin-ads:dev config: account_ids: - 000000000 - 111111111 credentials: auth_method: access_token access_token: {secret} start_date: 2022-01-01 bindings: - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaign {...}  "},{"title":"Mailchimp","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/mailchimp/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#prerequisites","content":"There are two ways to authenticate with MailChimp when capturing data: using OAuth2, and manually, with an API key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the API key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Mailchimp in the Flow web app​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#using-oauth2-to-authenticate-with-mailchimp-in-the-flow-web-app","content":"A Mailchimp account "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#configuring-the-connector-specification-manually","content":"A Mailchimp account A Mailchimp API key "},{"title":"Configuration​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Mailchimp source connector. "},{"title":"Properties​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#properties","content":"Endpoint​ The following properties reflect the API Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tAuthentication Type and Details\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication type. Set to apikey.\tstring\tRequired /credentials/apikey\tAPI Key\tYour Mailchimp API key\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tResource\tMailchimp lists, campaigns, or email_activity\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mailchimp:dev config: credentials: auth_type: apikey apikey: &lt;secret&gt; bindings: - resource: stream: lists syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: email_activity syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Intercom","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/intercom/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#supported-data-resources","content":"The following data resources are supported through the Intercom API: AdminsCompaniesCompany attributesCompany segmentsContactsContact attributesConversationsConversation partsSegmentsTagsTeams By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#prerequisites","content":"The access token for you Intercom account. "},{"title":"Configuration​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Intercom source connector. "},{"title":"Properties​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess token\tAccess token for making authenticated requests.\tstring\tRequired /start_date\tStart date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Intercom from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-intercom:dev config: access_token: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: admins syncMode: full_refresh target: ${PREFIX}/admins - resource: stream: companies syncMode: incremental target: ${PREFIX}/companies - resource: stream: company_segments syncMode: incremental target: ${PREFIX}/companysegments - resource: stream: conversations syncMode: incremental target: ${PREFIX}/conversations - resource: stream: conversation_parts syncMode: incremental target: ${PREFIX}/conversationparts - resource: stream: contacts syncMode: incremental target: ${PREFIX}/contacts - resource: stream: company_attributes syncMode: full_refresh target: ${PREFIX}/companyattributes - resource: stream: contact_attributes syncMode: full_refresh target: ${PREFIX}/contactattributes - resource: stream: segments syncMode: incremental target: ${PREFIX}/segments - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: teams syncMode: full_refresh target: ${PREFIX}/teams  "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#prerequisites","content":"This connector supports PostgreSQL versions 10.0 and later. You'll need a PostgreSQL database setup with the following: Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#setup","content":"info These setup instructions are PostgreSQL instances you manage yourself. If you use a cloud-based managed service for your database, different setup steps may be required. Instructions for setup on Amazon RDS can be found here. If you use a different managed service and the standard steps don't work as expected, contact Estuary support. The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role. For a more restricted setup, create a new user with just the required permissions as detailed in the following steps: Connect to your instance and create a new user and password: CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;  Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication: CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES;  Set WAL level to logical: ALTER SYSTEM SET wal_level = logical;  Restart PostgreSQL to allow the WAL level change to take effect. "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#backfills-and-performance-considerations","content":"When the a PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced/publicationName\tPublication name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot name\tThe name of the PostgreSQL replication slot to replicate from. A slot can only support one capture at a time.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in &lt;schema&gt;.&lt;table&gt; form.\tstring\t&quot;public.flow_watermarks&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#amazon-rds","content":"You can use this connector for PostgreSQL instances on Amazon RDS using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Enable logical replication on your RDS PostgreSQL instance. Create a parameter group. Create a unique name and description and set the following properties: Family: postgres13Type: DB Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the database. Reboot the database to allow the new parameter group to take effect. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#google-cloud-sql","content":"You can use this connector for PostgreSQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Enable public IP on your database and add34.121.207.128 as an authorized IP address. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the cloudsql.logical_decoding flag to on to enable logical replication on your loud SQL PostgreSQL instance. In your PostgreSQL client, connect to your instance and issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH REPLICATION IN ROLE cloudsqlsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 5432. Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#azure-database-for-postgresql","content":"You can use this connector for instances on Azure Database for PostgreSQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. In your Azure PostgreSQL instance's support parameters, set replication to logical to enable logical replication. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions. CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;  If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture;  If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Set up the watermarks table and publication. ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON information_schema.columns, information_schema.tables, pg_catalog.pg_attribute, pg_catalog.pg_class, pg_catalog.pg_index, pg_catalog.pg_namespace TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR TABLE schema.table1, schema.table2;  Note the following important items for configuration: Find the instance's host under Server Name, and the port under Connection Strings (usually 5432). Together, you'll use the host:port as the address property when you configure the connector.Format user as username@databasename; for example, flow_capture@myazuredb. "},{"title":"TOASTed values​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#toasted-values","content":"PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately. TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the a value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made. The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update. However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically: When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values "},{"title":"Troubleshooting​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#troubleshooting","content":"If you encounter an issue that you suspect is due to TOASTed values, try the following: Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance. "},{"title":"MySQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/MySQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#prerequisites","content":"To use this connector, you'll need a MySQL database setup with the following: binlog_formatsystem variable set to ROW (the default value).Binary log expiration period set to MySQL's default value of 30 days (2592000 seconds) if at all possible. This value may be set lower if necessary, but we strongly discourage going below 7 days as this may increase the likelihood of unrecoverable failures. A watermarks table. The watermarks table is a small &quot;scratch space&quot; to which the connector occasionally writes a small amount of data (a UUID, specifically) to ensure accuracy when backfilling preexisting table contents. The default name is &quot;flow.watermarks&quot;, but this can be overridden in config.json. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to insert, update, and delete on the watermarks table.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. "},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#setup","content":"To meet these requirements, do the following: Create the watermarks table. This table can have any name and be in any database, so long as config.json is modified accordingly. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Configure the binary log to retain data for the default MySQL setting of 30 days, if previously set lower. SET PERSIST binlog_expire_logs_seconds = 2592000;  "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#backfills-and-performance-considerations","content":"When the a MySQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MySQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /advanced/watermarks_table\tWatermarks Table Name\tThe name of the table used for watermark writes. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;flow.watermarks&quot; /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually. "},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"MySQL on managed cloud platforms​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#mysql-on-managed-cloud-platforms","content":"In addition to standard MySQL, this connector supports cloud-based MySQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#amazon-rds","content":"You can use this connector for MySQL instances on Amazon RDS using the following setup instructions. Estuary recommends creating a read replicain RDS for use with Flow; however, it's not required. You're able to apply the connector directly to the primary instance if you'd like. Setup​ Allow connections to the database from the Estuary Flow IP address. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Create a RDS parameter group to enable replication in MySQL. Create a parameter group. Create a unique name and description and set the following properties: Family: mysql 8.0Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 If using the primary instance (not recommended), associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Create a read replica with the new parameter group applied (recommended). Create a read replicaof your MySQL database. Modify the replicaand set the following: DB parameter group: choose the parameter group you created previouslyBackup retention period: 7 days Reboot the replica to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Run the following command to set the binary log retention to 7 days, the maximum value which RDS MySQL permits: CALL mysql.rds_set_configuration('binlog retention hours', 168);  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#google-cloud-sql","content":"You can use this connector for MySQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Enable public IP on your database and add34.121.207.128 as an authorized IP address. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the instance's binlog_expire_logs_seconds flagto 2592000. Using Google Cloud Shell or your preferred client, create the watermarks table. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 3306. Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#azure-database-for-mysql","content":"You can use this connector for MySQL instances on Azure Database for MySQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the binlog_expire_logs_seconds server perameterto 2592000. Using MySQL workbench or your preferred client, create the watermarks table. tip Your username must be specified in the format username@servername. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Note the instance's host under Server name, and the port under Connection Strings (usually 3306). Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#troubleshooting-capture-errors","content":"The source-mysql connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations: "},{"title":"Unsupported Operations​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#unsupported-operations","content":"If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured. In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety). In the case of ALTER TABLE query we intend to support a limited subset of table alterations in the future, however this error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did. "},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#data-manipulation-queries","content":"If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the MySQL binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section. Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it. "},{"title":"Unhandled Queries​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#unhandled-queries","content":"If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand. In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to tear down and recreate the entire capture so that it restarts from a later point in the binlog. "},{"title":"Metadata Errors​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#metadata-errors","content":"If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes. This should never happen, and most likely means that the MySQL binlog itself is corrupt in some way. If this occurs, it can be resolved by removing the offending table(s) from the capture bindings list and then recreating the capture (generally into a new collection, as this process will cause the table to be re-captured in its entirety). "},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#insufficient-binlog-retention","content":"If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MySQL binlog retention period is set to a dangerously low value, and your capture would risk unrecoverable failure if it were paused or the server became unreachable for a nontrivial amount of time, such that the database expired a binlog segment that the capture was still reading from. (If this were to happen, then change events would be permanently lost and that particular capture would never be able to make progress without potentially producing incorrect data. Thus the capture would need to be torn down and recreated so that each table could be re-captured in its entirety, starting with a complete backfill of current contents.) The &quot;binlog retention period is too short&quot; error should normally be fixed by setting binlog_expire_logs_seconds = 2592000 as described in the Prerequisites section (and when running on a managed cloud platform additional steps may be required, refer to the managed cloud setup instructions above). However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety. "},{"title":"Stripe","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/stripe/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#supported-data-resources","content":"The following data resources are supported through the Stripe API: Balance transactionsBank accountsChargesCheckout sessionsCheckout sessions line itemsCouponsCustomer balance transactionsCustomersDisputesEventsInvoice itemsInvoice line itemsInvoicesPayment intentsPayoutsPlansProductsPromotion codesRefundsSubscription itemsSubscriptionsTransfers By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#prerequisites","content":"Account ID of your Stripe account.Secret key for the Stripe API. "},{"title":"Configuration​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Stripe source connector. "},{"title":"Properties​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account_id\tAccount ID\tYour Stripe account ID (starts with 'acct_', find yours here https://dashboard.stripe.com/settings/account\tstring\tRequired /client_secret\tSecret Key\tStripe API key (usually starts with 'sk_live_'; find yours here https://dashboard.stripe.com/apikeys\tstring\tRequired /lookback_window_days\tLookback Window in days (Optional)\tWhen set, the connector will always re-export data from the past N days, where N is the value set here. This is useful if your data is frequently updated after creation.\tinteger\t0 /start_date\tReplication start date\tUTC date and time in the format 2017-01-25T00:00:00Z. Only data generated after this date will be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Stripe from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Choosing your start date and lookback window​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#choosing-your-start-date-and-lookback-window","content":"The connector will continually capture data beginning on the Replication start date you choose. However, some data from the Stripe API is mutable; for example, a draft invoice can be completed at a later date than it was created. To account for this, it's useful to set the Lookback Window. When this is set, at a given point in time, the connector will not only look for new data; it will also capture changes made to data within the window. For example, if you start the connector with the start date of 2022-06-06T00:00:00Z (June 6) and the lookback window of 3, the connector will begin to capture data starting from June 3. As time goes on while the capture remains active, the lookback window rolls forward along with the current timestamp. On June 10, the connector will continue to monitor data starting from June 7 and capture any changes to that data, and so on. "},{"title":"Sample​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-stripe:dev config: account_id: 00000000 client_secret: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: balance_transactions syncMode: incremental target: ${PREFIX}/balancetransactions - resource: stream: bank_accounts syncMode: full_refresh target: ${PREFIX}/bankaccounts - resource: stream: charges syncMode: incremental target: ${PREFIX}/charges - resource: stream: checkout_sessions syncMode: incremental target: ${PREFIX}/checkoutsessions - resource: stream: checkout_sessions_line_items syncMode: incremental target: ${PREFIX}/checkoutsessionslineitems - resource: stream: coupons syncMode: incremental target: ${PREFIX}/coupons - resource: stream: customer_balance_transactions syncMode: full_refresh target: ${PREFIX}/customerbalancetransactions - resource: stream: customers syncMode: incremental target: ${PREFIX}/customers - resource: stream: disputes syncMode: incremental target: ${PREFIX}/disputes - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: invoice_items syncMode: incremental target: ${PREFIX}/invoice_items - resource: stream: invoice_line_items syncMode: full_refresh target: ${PREFIX}/invoicelineitems - resource: stream: invoices syncMode: incremental target: ${PREFIX}/invoices - resource: stream: payment_intents syncMode: incremental target: ${PREFIX}/paymentintents - resource: stream: payouts syncMode: incremental target: ${PREFIX}/payouts - resource: stream: plans syncMode: incremental target: ${PREFIX}/plans - resource: stream: products syncMode: incremental target: ${PREFIX}/products - resource: stream: promotion_codes syncMode: incremental target: ${PREFIX}/promotioncodes - resource: stream: refunds syncMode: incremental target: ${PREFIX}/refunds - resource: stream: subscription_items syncMode: full_refresh target: ${PREFIX}/subscriptionitems - resource: stream: subscriptions syncMode: incremental target: ${PREFIX}/subscriptions - resource: stream: transfers syncMode: incremental target: ${PREFIX}/transfers  "},{"title":"Zendesk Support","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/zendesk-support/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#supported-data-resources","content":"The following data resources are supported through the Zendesk API: BrandsCustom rolesGroup membershipsGroupsMacrosOrganizationsSatisfaction ratingsSchedulesSLA policiesTagsTicket auditsTicket commentsTicket fieldsTicket formsTicket metricsTicket metric eventsTicketsUsers By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#prerequisites","content":"Subdomain of your Zendesk URL. In the URL https://MY_SUBDOMAIN.zendesk.com/, MY_SUBDOMAIN is the subdomain.Email address associated with your Zendesk account.A Zendesk API token. See the Zendesk docs to enable tokens and generate a new token. "},{"title":"Configuration​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Zendesk Support source connector. "},{"title":"Properties​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/credentials/api_token\tAPI Token\tThe value of the API token generated.\tstring /credentials/credentials\tCredentials method\tType of credentials used. Set to api-token\tstring /credentials/email\tEmail\tThe user email for your Zendesk account.\tstring /start_date\tStart Date\tThe date from which you'd like to replicate data for Zendesk Support API, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired /subdomain\tSubdomain\tThis is your Zendesk subdomain that can be found in your account URL. For example, in https://{MY_SUBDOMAIN}.zendesk.com/, where MY_SUBDOMAIN is the value of your subdomain.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource in Zendesk from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-zendesk-support:dev config: credentials: api_token: &lt;secret&gt; credentials: api_token email: user@domain.com start_date: 2022-03-01T00:00:00Z subdomain: my_subdomain bindings: - resource: stream: group_memberships syncMode: incremental target: ${PREFIX}/groupmemberships - resource: stream: groups syncMode: incremental target: ${PREFIX}/groups - resource: stream: macros syncMode: incremental target: ${PREFIX}/macros - resource: stream: organizations syncMode: incremental target: ${PREFIX}/organizations - resource: stream: satisfaction_ratings syncMode: incremental target: ${PREFIX}/satisfactionratings - resource: stream: sla_policies syncMode: full_refresh target: ${PREFIX}/slapoliciies - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: ticket_audits syncMode: incremental target: ${PREFIX}/ticketaudits - resource: stream: ticket_comments syncMode: incremental target: ${PREFIX}/ticketcomments - resource: stream: ticket_fields syncMode: incremental target: ${PREFIX}/ticketfields - resource: stream: ticket_forms syncMode: incremental target: ${PREFIX}/ticketforms - resource: stream: ticket_metrics syncMode: incremental target: ${PREFIX}/ticketmetrics - resource: stream: ticket_metric_events syncMode: incremental target: ${PREFIX}/ticketmetricevents - resource: stream: tickets syncMode: incremental target: ${PREFIX}/tickets - resource: stream: users syncMode: incremental target: ${PREFIX}/users - resource: stream: brands syncMode: full_refresh target: ${PREFIX}/brands - resource: stream: custom_roles syncMode: full_refresh target: ${PREFIX}/customroles - resource: stream: schedules syncMode: full_refresh target: ${PREFIX}/schedules  "},{"title":"Materialization connectors","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/","content":"","keywords":""},{"title":"Available materialization connectors​","type":1,"pageTitle":"Materialization connectors","url":"reference/Connectors/materialization-connectors/#available-materialization-connectors","content":"Apache Parquet in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-parquet:dev Elasticsearch ConfigurationPackage — ghcr.io/estuary/materialize-elasticsearch:dev Firebolt ConfigurationPackage - ghcr.io/estuary/materialize-firebolt:dev Google BigQuery ConfigurationPackage — ghcr.io/estuary/materialize-bigquery:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/materialize-postgres:dev Rockset ConfigurationPackage — ghcr.io/estuary/materialize-rockset:dev Snowflake ConfigurationPackage — ghcr.io/estuary/materialize-snowflake:dev "},{"title":"Google BigQuery","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/BigQuery/","content":"","keywords":""},{"title":"Performance considerations​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#performance-considerations","content":"Like other Estuary connectors, this is a real-time connector that materializes documents using continuous transactions. However, in practice, there are speed limitations. Standard BigQuery tables are limited to 1500 operations per day. This means that the connector is limited 1500 transactions per day. To avoid running up against this limit, you should set the minimum transaction time to a recommended value of 2 minutes, or a minimum value of 1 minute. You do this by configuring the materialization's task shard. This causes an apparent delay in the materialization, but is necessary to prevent error. This also makes transactions more efficient, which reduces costs in BigQuery, especially for large datasets. Instructions to set the minimum transaction time are detailed below. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#prerequisites","content":"To use this connector, you'll need: A new Google Cloud Storage bucket in the same region as the BigQuery destination dataset. A Google Cloud service account with a key file generated and the following roles: roles/bigquery.dataEditor on the destination dataset roles/bigquery.jobUser on the project with which the BigQuery destination dataset is associated roles/storage.objectAdminon the GCS bucket created above See Setup for detailed steps to set up your service account. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#setup","content":"To configure your service account, complete the following steps. Log into the Google Cloud console and create a service account. During account creation: Grant the user access to the project.Grant the user roles roles/bigquery.dataEditor, roles/bigquery.jobUser, and roles/storage.objectAdmin.Click Done. Select the new service account from the list of service accounts. On the Keys tab, click Add key and create a new JSON key. The key is automatically downloaded to your machine. In your terminal, base64-encode the JSON key: base64 /path/to/key.json You'll use the resulting string when you configure the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a BigQuery materialization, which will direct one or more of your Flow collections to your desired tables within a BigQuery dataset. A BigQuery dataset is the top-level container within a project, and comprises multiple tables. You can think of a dataset as somewhat analogous to a schema in a relational database. For a complete introduction to resource organization in Bigquery, see the BigQuery docs. "},{"title":"Properties​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe project ID for the Google Cloud Storage bucket and BigQuery dataset.\tString\tRequired /billing_project_id\tBilling project ID\tThe project ID to which these operations are billed in BigQuery. Typically, you want this to be the same as project_id (the default).\tString\tSame as project_id /dataset\tDataset\tName of the target BigQuery dataset.\tString\tRequired /region\tRegion\tThe GCS region.\tString\tRequired /bucket\tBucket\tName of the GCS bucket.\tString\tRequired /bucket_path\tBucket path\tBase path within the GCS bucket. Also called &quot;Folder&quot; in the GCS console.\tString /credentials_json\tCredentials JSON\tBase64-encoded string of the full service account file.\tByte\tRequired To learn more about project billing, see the BigQuery docs. Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name.\tstring\tRequired /delta_updates\tDelta updates.\tWhether to use standard or delta updates\tboolean\tfalse "},{"title":"Shard configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#shard-configuration","content":"Beta UI controls for this workflow will be added to the Flow web app soon. For now, you must edit the materialization config manually, either in the web app or using the CLI. To avoid exceeding your BigQuery tables' daily operation limits as discussed in Performance considerations, complete the following steps when configuring your materialization: Using the Flow web application or the flowctl CLI, create a draft materialization. Don't publish it yet. Add the shards configuration to the materialization at the same indentation level as endpoint and resource. Set the minTxnDuration property to at least 1m (we recommend 2m). In the web app, you do this in the catalog editor. shards: minTxnDuration: 2m A full sample is included below. Continue to test and publish the materialization. "},{"title":"Sample​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: project_id: our-bigquery-project dataset: materialized-data region: US bucket: our-gcs-bucket bucket_path: bucket-path/ credentials_json: SSBqdXN0IHdhbm5hIHRlbGwgeW91IGhvdyBJJ20gZmVlbGluZwpHb3R0YSBtYWtlIHlvdSB1bmRlcnN0YW5kCk5ldmVyIGdvbm5hIGdpdmUgeW91IHVwCk5ldmVyIGdvbm5hIGxldCB5b3UgZG93bgpOZXZlciBnb25uYSBydW4gYXJvdW5kIGFuZCBkZXNlcnQgeW91Ck5ldmVyIGdvbm5hIG1ha2UgeW91IGNyeQpOZXZlciBnb25uYSBzYXkgZ29vZGJ5ZQpOZXZlciBnb25uYSB0ZWxsIGEgbGllIGFuZCBodXJ0IHlvdQ== image: ghcr.io/estuary/materialize-bigquery:dev bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection} shards: minTxnDuration: 2m  "},{"title":"Delta updates​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your BigQuery table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in BigQuery won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"Elasticsearch","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Elasticsearch/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#prerequisites","content":"To use this connector, you'll need: An Elastic cluster with a known endpoint If the cluster is on the Elastic Cloud, you'll need an Elastic user with a role that grants all privileges on indices you plan to materialize to within the cluster. See Elastic's documentation on defining roles andsecurity privileges. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an Elasticsearch materialization, which will direct the contents of these Flow collections into Elasticsearch indices. By default, the connector attempts to map each field in the Flow collection to the most appropriate Elasticsearch field type. However, because each JSON field type can map to multiple Elasticsearch field types, you may want to override the defaults. You can configure this by adding field_overrides to the collection's binding in the materialization specification. To do so, provide a JSON pointer to the field in the collection schema, choose the output field type, and specify additional properties, if necessary. "},{"title":"Properties​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/endpoint\tEndpoint\tEndpoint host or URL. If using Elastic Cloud, this follows the format https://CLUSTER_ID.REGION.CLOUD_PLATFORM.DOMAIN:PORT.\tstring\tRequired /password\tPassword\tPassword to connect to the endpoint.\tstring /username\tUsername\tUser to connect to the endpoint.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\tfalse /field_overrides\tField overrides\tAssign Elastic field type to each collection field.\tarray /field_overrides/-/es_type\tElasticsearch type\tThe overriding Elasticsearch data type of the field.\tobject /field_overrides/-/es_type/date_spec\tDate specifications\tConfiguration for the date field, effective if field_type is 'date'. See Elasticsearch docs.\tobject /field_overrides/-/es_type/date_spec/format\tDate format\tFormat of the date. See Elasticsearch docs.\tstring /field_overrides/-/es_type/field_type\tField type\tThe Elasticsearch field data types. Supported types include: boolean, date, double, geo_point, geo_shape, keyword, long, null, text.\tstring /field_overrides/-/es_type/keyword_spec\tKeyword specifications\tConfiguration for the keyword field, effective if field_type is 'keyword'. See Elasticsearch docs\tobject /field_overrides/-/es_type/keyword_spec/ignore_above\tIgnore above\tStrings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs\tinteger /field_overrides/-/es_type/text_spec\tText specifications\tConfiguration for the text field, effective if field_type is 'text'.\tobject /field_overrides/-/es_type/text_spec/dual_keyword\tDual keyword\tWhether or not to specify the field as text/keyword dual field.\tboolean /field_overrides/-/es_type/text_spec/keyword_ignore_above\tIgnore above\tEffective only if Dual Keyword is enabled. Strings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs.\tinteger /field_overrides/-/pointer\tPointer\tA '/'-delimited json pointer to the location of the overridden field.\tstring /index\tindex\tName of the ElasticSearch index to store the materialization results.\tstring\tRequired /number_of_replicas\tNumber of replicas\tThe number of replicas in ElasticSearch index. If not set, default to be 0. For single-node clusters, make sure this field is 0, because the Elastic search needs to allocate replicas on different nodes.\tinteger\t0 /number_of_shards\tNumber of shards\tThe number of shards in ElasticSearch index. Must set to be greater than 0.\tinteger\t1 "},{"title":"Sample​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-elasticsearch:dev config: endpoint: https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 username: flow_user password: secret # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: index: last-updated delta_updates: false field_overrides: - pointer: /updated-date es_type: field_type: date date_spec: format: yyyy-MM-dd source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#delta-updates","content":"This connector supports both standard and delta updates. You must choose an option for each binding. Learn more about delta updates and the implications of using each update type. "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#prerequisites","content":"To use this connector, you'll need: At least one Flow collection. If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. The Google spreadsheet URL. Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Google Service Account parameter when you configure the connector. Share your Google spreadsheet with the service account, granting edit access. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Sheets materialization. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/googleCredentials\tGoogle Service Account\tService account JSON key to use as Application Default Credentials\tstring\tRequired /spreadsheetURL\tSpreadsheet URL\tURL of the spreadsheet to materialize into, which is shared with the service account.\tstring\tRequired Bindings​ Configure a separate binding for each collection you want to materialize to a sheet. Note that the connector will add an addition column to the beginning of each sheet; this is to track the internal state of the data. Property\tTitle\tDescription\tType\tRequired/Default/sheet\tSheet Name\tName of the spreadsheet sheet to materialize into\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: googleCredentials: &lt;secret&gt; spreadsheetID: &lt;string&gt; image: ghcr.io/estuary/materialize-google-sheets:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: sheet: my_sheet source: ${PREFIX}/${source_collection}  "},{"title":"Firebolt","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Firebolt/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#prerequisites","content":"To use this connector, you'll need: A Firebolt database with at least one engine The engine must be started before creating the materialization.It's important that the engine stays up throughout the lifetime of the materialization. To ensure this is the case, select Edit Engine on your engine. In the engine settings, set Auto-stop engine after to Always On. An S3 bucket where JSON documents will be stored prior to loading The bucket must be in a supported AWS region matching your Firebolt database.The bucket may be public, or may be accessible by an IAM user. To configure your IAM user, see the steps below. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#setup","content":"For non-public buckets, you'll need to configure access in AWS IAM. Follow the Firebolt documentation to set up an IAM policy and role, and add it to the external table definition. Create a new IAM user. During setup: Choose Programmatic (access key) access. This ensures that an access key ID and secret access key are generated. You'll use these to configure the connector. On the Permissions page, choose Attach existing policies directly and attach the policy you created in step 1. After creating the user, download the IAM credentials file. Take note of the access key ID and secret access key and use them to configure the connector. See the Amazon docs if you lose your credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Firebolt materialization, which will direct Flow data to your desired Firebolt tables via an external table. "},{"title":"Properties​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/aws_key_id\tAWS key ID\tAWS access key ID for accessing the S3 bucket.\tstring /aws_region\tAWS region\tAWS region the bucket is in.\tstring /aws_secret_key\tAWS secret access key\tAWS secret key for accessing the S3 bucket.\tstring /database\tDatabase\tName of the Firebolt database.\tstring\tRequired /engine_url\tEngine URL\tEngine URL of the Firebolt database, in the format: &lt;engine-name&gt;.&lt;organization&gt;.&lt;region&gt;.app.firebolt.io.\tstring\tRequired /password\tPassword\tFirebolt password.\tstring\tRequired /s3_bucket\tS3 bucket\tName of S3 bucket where the intermediate files for external table will be stored.\tstring\tRequired /s3_prefix\tS3 prefix\tA prefix for files stored in the bucket.\tstring /username\tUsername\tFirebolt username.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the Firebolt table to store materialized results in. The external table will be named after this table with an _external suffix.\tstring\tRequired /table_type\tTable type\tType of the Firebolt table to store materialized results in. See the Firebolt docs for more details.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: my-db engine_url: my-db-my-engine-name.my-organization.us-east-1.app.firebolt.io password: secret # For public S3 buckets, only the bucket name is required s3_bucket: my-bucket username: firebolt-user # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-firebolt:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: table-name table_type: fact source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#delta-updates","content":"The Firebolt connector operates only in delta updates mode. Firebolt stores all deltas — the unmerged collection documents — directly. In some cases, this will affect how materialized views look in Firebolt compared to other systems that use standard updates. "},{"title":"Reserved words​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#reserved-words","content":"Firebolt has a list of reserved words, which my not be used in identifiers. Collections with field names that include a reserved word will automatically be quoted as part of a Firebolt materialization. Reserved words all\tfalse\tor alter\tfetch\torder and\tfirst\touter array\tfloat\tover between\tfrom\tpartition bigint\tfull\tprecision bool\tgenerate\tprepare boolean\tgroup\tprimary both\thaving\tquarter case\tif\tright cast\tilike\trow char\tin\trows concat\tinner\tsample copy\tinsert\tselect create\tint\tset cross\tinteger\tshow current_date\tintersect\ttext current_timestamp\tinterval\ttime database\tis\ttimestamp date\tisnull\ttop datetime\tjoin\ttrailing decimal\tjoin_type\ttrim delete\tleading\ttrue describe\tleft\ttruncate distinct\tlike\tunion double\tlimit\tunknown_char doublecolon\tlimit_distinct\tunnest dow\tlocaltimestamp\tunterminated_string doy\tlong\tupdate drop\tnatural\tusing empty_identifier\tnext\tvarchar epoch\tnot\tweek except\tnull\twhen execute\tnumeric\twhere exists\toffset\twith explain\ton extract\tonly\t "},{"title":"Apache Parquet in S3","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Parquet/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#prerequisites","content":"To use this connector, you'll need: An AWS root or IAM user with access to the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a materialization, which will direct the contents of these Flow collections to Parquet files in S3. "},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS Access Key ID\tAWS credential used to connect to S3.\tstring\tRequired /awsSecretAccessKey\tAWS Secret Access Key\tAWS credential used to connect to S3.\tstring\tRequired /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a S3-compatible API that isn't provided by AWS\tstring. /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring /uploadIntervalInSeconds\tUpload Interval in Seconds\tTime interval, in seconds, at which to upload data from Flow to S3.\tinteger\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/compressionType\tCompression type\tThe method used to compress data in Parquet.\tstring /pathPrefix\tPath prefix\tThe desired Parquet file path within the bucket as determined by an S3 prefix.\tstring\tRequired The following compression types are supported: snappygziplz4zstd "},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: config: awsAccessKeyId: AKIAIOSFODNN7EXAMPLE awsSecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYSECRET bucket: my-bucket uploadIntervalInSeconds: 300 # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-s3-parquet:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: pathPrefix: /my-prefix source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#delta-updates","content":"This connector uses only delta updates mode. Collection documents are converted to Parquet format and stored in their unmerged state. "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#prerequisites","content":"To use this connector, you'll need: A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances. To connect securely, you must use an SSH tunnel. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#setup","content":"You must configure your database to allow connections from Estuary. The recommended method is to whitelist Estuary Flow's IP address. Amazon RDS: Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. Google Cloud SQL: Enable public IP on your database and add 34.121.207.128 as an authorized IP address. Azure Database For PostgreSQL: Create a new firewall rule that grants access to the IP address 34.121.207.128. Alternatively, you can allow secure connections via SSH tunneling. To do so: Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the additional of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. SSH Configuration Tip You can find the values for forwardHost and forwardPort in the following locations in each platform's console: Amazon RDS: forwardHost as Endpoint; forwardPort as Port.Google Cloud SQL: forwardHost as Private IP Address; forwardPort is always 5432. You may need to configure private IP on your database.Azure Database: forwardHost as Server Name; forwardPort under Connection Strings (usually 5432). "},{"title":"Reserved words​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#reserved-words","content":"PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation. These reserve words are listed in the table below. Flow automatically quotes fields that are in this list. Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear "},{"title":"Rockset","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Rockset/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites","content":"To use this connector, you'll need: A Rockset account with an API key generated from the web UIA Rockset workspace Optional; if none exist, one will be created by the connector. A Rockset collection Optional; if none exist, one will be created by the connector. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections. "},{"title":"Properties​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI key\tThe key used to authenticate to the Rockset API.\tString\tRequired Bindings​ The binding configuration for this connector includes two optional sections.Backfill from S3 allows you to backfill historical data from an S3 bucket, as detailed below.Advanced collection settings includes settings that may help optimize your resulting Rockset collections: Clustering fields: You can specify clustering fields for your Rockset collection's columnar index to help optimize specific query patterns. See the Rockset docs for more information.Event time field: All Rockset documents have an associated _event_time field, which is created for each collection. You can specify an existing integer or timestamp field in your data to be used for _event_time. See the Rockset docs for more information.Insert only: Disallows updates and deletes. The materialization will fail if there are documents with duplicate keys, but indexing in Rockset will be more efficient.Retention period: Amount of time before data is purged, in seconds. A low value will keep the amount of data indexed in Rockset smaller. Property\tTitle\tDescription\tType\tRequired/Default/advancedCollectionSettings\tAdvanced Collection Settings object /advancedCollectionSettings/clustering_key\tClustering Key\tList of clustering fields\tarray /advancedCollectionSettings/clustering_key/-/field_name\tField Name\tThe name of a field\tstring /advancedCollectionSettings/event_time_info\tEvent Time Info object /advancedCollectionSettings/event_time_info/field\tField Name\tName of the field containing the event time\tstring /advancedCollectionSettings/event_time_info/format\tFormat\tFormat of the time field\tstring /advancedCollectionSettings/event_time_info/time_zone\tTimezone\tDefault timezone\tstring /advancedCollectionSettings/insert_only\tInsert Only\tIf true disallows updates and deletes. The materialization will fail if there are documents with duplicate keys.\tboolean /advancedCollectionSettings/retention_secs\tRetention Period\tNumber of seconds after which data is purged based on event time\tinteger /collection\tRockset Collection\tThe name of the Rockset collection (will be created if it does not exist)\tstring /initializeFromS3\tBackfill from S3 object /initializeFromS3/bucket\tBucket\tThe name of the S3 bucket to load data from.\tstring /initializeFromS3/integration\tIntegration Name\tThe name of the integration that was previously created in the Rockset UI\tstring /initializeFromS3/pattern\tPattern\tA regex that is used to match objects to be ingested\tstring /initializeFromS3/prefix\tPrefix\tPrefix of the data within the S3 bucket. All files under this prefix will be loaded. Optional. Must not be set if 'pattern' is defined.\tstring /initializeFromS3/region\tRegion\tThe AWS region in which the bucket resides. Optional.\tstring /workspace\tWorkspace\tThe name of the Rockset workspace (will be created if it does not exist)\tstring\t "},{"title":"Sample​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: api_key: supersecret # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-rockset:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: workspace: ${namespace_name} collection: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates and reduction strategies​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#delta-updates-and-reduction-strategies","content":"The Rockset connector operates only in delta updates mode. This means that Rockset, rather than Flow, performs the document merge. In some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates. Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of RFC 7396 - JSON merge. This differs from how Flow would reduce documents, most notably in that Rockset will not honor any reduction strategies defined in your Flow schema. For consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations in Flow mirror Rockset's semantics. To accomplish this, ensure that your collection schema has the following data reductions defined in its schema: A top-level reduction strategy of mergeA strategy of lastWriteWins for all nested values (this is the default) "},{"title":"Bulk ingestion for large backfills of historical data​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#bulk-ingestion-for-large-backfills-of-historical-data","content":"You can backfill large amounts of historical data into Rockset using a bulk ingestion. Bulk ingestion must originate in S3 and requires additional steps in your dataflow. This workflow is supported using the flowctl CLI. "},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites-1","content":"Before completing this workflow, make sure you have: A working catalog spec including at least one Flow collection.A production or development environment tip The following is an intermediate workflow. As needed, refer to this guide for the basic steps to create and deploy a catalog spec using the GitOps workflow. "},{"title":"How to perform a bulk ingestion​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#how-to-perform-a-bulk-ingestion","content":"A bulk ingestion from a Flow collection into Rockset is essentially a two-step process. First, Flow writes your historical data into an S3 bucket using Estuary's S3-Parquet materialization connector. Once the data is caught up, it uses the Rockset connector to backfill the data from S3 into Rockset and then switches to the Rockset Write API for the continuous materialization of new data. graph TD A[Create an S3 integration in Rockset] --&gt; B B[Create Flow materialization into S3 bucket] --&gt; C C[Wait for S3 materialization to catch up with historical data] --&gt;|When ready to bulk ingest into Rockset| D D[Disable S3 materialization shards] --&gt; E E[Update same materialization to use the Rockset connector with the integration created in first step] --&gt; F F[Rockset connector automatically continues materializing after the bulk ingestion completes] To set this up, use the following procedure as a guide, substituting example/flow/collection for your collection: You'll need an S3 integration in Rockset. To create one, follow the instructions here, but do not create the Rockset collection yet.Create and activate a materialization of example/flow/collection into a unique prefix within an S3 bucket of your choosing. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-s3-parquet:dev config: bucket: example-s3-bucket region: us-east-1 awsAccessKeyId: &lt;your key&gt; awsSecretAccessKey: &lt;your secret&gt; uploadIntervalInSeconds: 300 bindings: - resource: pathPrefix: example/s3-prefix/ source: example/flow/collection Once the S3 materialization is caught up with your historical data, you'll switch to the Rockset write API for your future data. To make the switch, first disable the S3 materialization by setting shards to disabled in the definition, and re-deploy the catalog. This is necessary to ensure correct ordering of documents written to Rockset. materializations: example/toRockset: shards: disable: true # ...the remainder of the materialization yaml remains the same as above Update the materialization to use the materialize-rockset connector, and re-enable the shards. Here you'll provide the name of the Rockset S3 integration you created above, as well as the bucket and prefix that you previously materialized into. It's critical that the name of the materialization remains the same as it was for materializing into S3. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-rockset:dev config: api_key: &lt;your rockset API key here&gt; bindings: - resource: workspace: &lt;your rockset workspace name&gt; collection: &lt;your rockset collection name&gt; initializeFromS3: integration: &lt;rockset integration name&gt; bucket: example-s3-bucket region: us-east-1 prefix: example/s3-prefix/ source: example/flow/collection When you activate the new materialization, the connector will create the Rockset collection using the given integration, and wait for it to ingest all of the historical data from S3 before it continues. Once this completes, the Rockset connector will automatically switch over to the incoming stream of new data. "},{"title":"Snowflake","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#prerequisites","content":"To use this connector, you'll need: A Snowflake account that includes: A target database, to which you'll materialize dataA schema — a logical grouping of database objects — within the target databaseA virtual warehouseA user with a role assigned that grants the appropriate access levels to these resources. See the script below for details. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#setup","content":"To meet the prerequisites, copy and paste the following script into the Snowflake SQL editor, replacing the variable names in the first six lines with whatever you'd like. Check the All Queries check box, and click Run. set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_user); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- Create snowflake DB create database if not exists identifier($database_name); -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;  "},{"title":"Configuration​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Snowflake materialization, which will direct one or more of your Flow collections to new Snowflake tables. "},{"title":"Properties​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account\tAccount\tThe Snowflake account identifier\tstring\tRequired /database\tDatabase\tName of the Snowflake database to which to materialize\tstring\tRequired /password\tPassword\tSnowflake user password\tstring\tRequired /cloud_provider\tCloud Provider\tCloud Provider where the account is located\tstring\tRequired /region\tRegion\tRegion where the account is located\tstring\tRequired /role\tRole\tRole assigned to the user\tstring /schema\tSchema\tSnowflake schema within the database to which to materialize\tstring\tRequired /user\tUser\tSnowflake username\tstring\tRequired /warehouse\tWarehouse\tName of the data warehouse that contains the database\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean /table\tTable\tTable name\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret cloud_provider: aws region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your Snowflake table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Snowflake won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"Optimizing performance for standard updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#optimizing-performance-for-standard-updates","content":"When using standard updates for a large dataset, the collection key you choose can have a significant impact on materialization performance and efficiency. Snowflake uses micro partitions to physically arrange data within tables. Each micro partition includes metadata, such as the minimum and maximum values for each column. If you choose a collection key that takes advantage of this metadata to help Snowflake prune irrelevant micro partitions, you'll see dramatically better performance. For example, if you materialize a collection with a key of /user_id, it will tend to perform far worse than a materialization of /date, /user_id. This is because most materializations tend to be roughly chronological over time, and that means that data is written to Snowflake in roughly /date order. This means that updates of keys /date, /user_id will need to physically read far fewer rows as compared to a key like /user_id, because those rows will tend to live in the same micro-partitions, and Snowflake is able to cheaply prune micro-partitions that aren't relevant to the transaction. "},{"title":"Reserved words​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#reserved-words","content":"Snowflake has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Snowflake's documentation here and in the table below. caution In Snowflake, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Snowflake docs. Reserved words account\tfrom\tqualify all\tfull\tregexp alter\tgrant\trevoke and\tgroup\tright any\tgscluster\trlike as\thaving\trow between\tilike\trows by\tin\tsample case\tincrement\tschema cast\tinner\tselect check\tinsert\tset column\tintersect\tsome connect\tinto\tstart connection\tis\ttable constraint\tissue\ttablesample create\tjoin\tthen cross\tlateral\tto current\tleft\ttrigger current_date\tlike\ttrue current_time\tlocaltime\ttry_cast current_timestamp\tlocaltimestamp\tunion current_user\tminus\tunique database\tnatural\tupdate delete\tnot\tusing distinct\tnull\tvalues drop\tof\tview else\ton\twhen exists\tor\twhenever false\torder\twhere following\torganization\twith for  "},{"title":"Organizing a Flow catalog","type":0,"sectionRef":"#","url":"reference/organizing-catalogs/","content":"","keywords":""},{"title":"import​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#import","content":"Flow's import directive can help you easily handle all of these scenarios while keeping your catalogs well organized. Each catalog spec file may import any number of other files, and each import may refer to either relative or an absolute URL. When you use import in a catalog spec, you're conceptually bringing the entirety of another catalog — as well as the schemas and typescript files it uses — into your catalog. Imports are also transitive, so when you import another catalog, you're also importing everything that other catalog has imported. This allows you to keep your catalogs organized, and is flexible enough to support collaboration between separate teams and organizations. Perhaps the best way of explaining this is with some examples. Example: Organizing collections​ Let's look at a relatively simple case in which you want to organize your collections into multiple catalog files. Say you work for Acme Corp on the team that's introducing Flow. You might start with the collections and directory structure below: acme/customers/customerInfo acme/products/info/manufacturers acme/products/info/skus acme/products/inventory acme/sales/pending acme/sales/complete  acme ├── flow.yaml ├── customers │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml ├── products │ ├── flow.yaml │ ├── info │ │ ├── flow.ts │ │ ├── flow.yaml │ │ └── schemas.yaml │ └── inventory │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml schemas.yaml └── sales ├── flow.ts ├── flow.yaml └── schemas.yaml  It's immediately clear where each of the given collections is defined, since the directory names match the path segments in the collection names. This is not required by theflowctl CLI, but is strongly recommended, since it makes your catalogs more readable and maintainable. Each directory contains a catalog spec (flow.yaml), which will import all of the catalogs from child directories. So, the top-level catalog spec, acme/flow.yaml, might look something like this: import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml  This type of layout has a number of other advantages. During development, you can easily work with a subset of collections using, for example, flowctl test --source acme/products/flow.yaml to run only the tests for product-related collections. It also allows other imports to be more granular. For example, you might want a derivation under sales to read from acme/products/info. Since info has a separate catalog spec, acme/sales/flow.yaml can import acme/products/info/flow.yaml without creating a dependency on the inventory collection. Example: Separate environments​ It's common to use separate environments for tiers like development, staging, and production. Flow catalog specs often necessarily include endpoint configuration for external systems that will hold materialized views. Let's say you want your production environment to materialize views to Snowflake, but you want to develop locally on SQLite. We might modify the Acme example slightly to account for this. acme ├── dev.flow.yaml ├── prod.flow.yaml ... the remainder is the same as above  Each of the top-level catalog specs might import all of the collections and define an endpoint called ourMaterializationEndpoint that points to the desired system. The import block might be the same for each system, but each file may use a different configuration for the endpoint, which is used by any materializations that reference it. Our configuration for our development environment will look like: dev.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml ourMaterializationEndpoint: # dev.flow.yaml sqlite: path: dev-materializations.db  While production will look like: prod.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml endpoints: snowflake: account: acme_production role: admin schema: snowflake.com/acmeProd user: importantAdmin password: abc123 warehouse: acme_production  When we test the draft locally, we'll work with dev.flow.yaml, but we'll publish prod.flow.yaml. Everything will continue to work because in our development environment we'll be binding collections to our local SQLite DB and in production we'll use Snowflake. Example: Cross-team collaboration​ When working across teams, it's common for one team to provide a data product for another to reference and use. Flow is designed for cross-team collaboration, allowing teams and users to reference each other's full catalog or schema.  Again using the Acme example, let's imagine we have two teams. Team Web is responsible for Acme's website, and Team User is responsible for providing a view of Acme customers that's always up to date. Since Acme wants a responsive site that provides a good customer experience, Team Web needs to pull the most up-to-date information from Team User at any point. Let's look at Team User's collections: teamUser.flow.yaml import: - userProfile.flow.yaml  Which references: userProfile.flow.yaml collection: userProfile: schema: -&quot;/userProfile/schema&quot; key: [/id]  Team User references files in their directory, which they actively manage in both their import and schema sections. If Team Web wants to access user data (and they have access), they can use a relative path or a URL-based path given that Team User publishes their data to a URL for access: teamWeb.flow.yaml import: -http://www.acme.com/teamUser#userProfile.flow.yaml -webStuff.flow.yaml  Now Team Web has direct access to collections (referenced by their name) to build derived collections on top of. They can also directly import schemas: webStuff.flow.yaml collection: webStuff: schema: -http://acme.com/teamUser#userProfile/#schema key: [/id]  "},{"title":"Global namespace​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#global-namespace","content":"Every Flow collection has a name, and that name must be unique within a running Flow system. Flow collections should be thought of as existing within a global namespace. Keeping names globally unique makes it easy to import catalogs from other teams, or even other organizations, without having naming conflicts or ambiguities. For example, imagine your catalog for the inside sales team has a collection just named customers. If you later try to import a catalog from the outside sales team that also contains a customers collection, 💥 there's a collision. A better collection name would be acme/inside-sales/customers. This allows a catalog to include customer data from separate teams, and also separate organizations. Learn more about the Flow namespace. "},{"title":"Reduction strategies","type":0,"sectionRef":"#","url":"reference/reduction-strategies/","content":"","keywords":""},{"title":"Reduction guarantees​","type":1,"pageTitle":"Reduction strategies","url":"reference/reduction-strategies/#reduction-guarantees","content":"In Flow, documents that share the same collection key and are written to the same logical partition have a total order, meaning that one document is universally understood to have been written before the other. This isn't true of documents of the same key written to different logical partitions. These documents can be considered “mostly” ordered: Flow uses timestamps to understand the relative ordering of these documents, and while this largely produces the desired outcome, small amounts of re-ordering are possible and even likely. Flow guarantees exactly-once semantics within derived collections and materializations (so long as the target system supports transactions), and a document reduction will be applied exactly one time. Flow does not guarantee that documents are reduced in sequential order, directly into a base document. For example, documents of a single Flow capture transaction are combined together into one document per collection key at capture time – and that document may be again combined with still others, and so on until a final reduction into the base document occurs. Taken together, these total-order and exactly-once guarantees mean that reduction strategies must be associative [as in (2 + 3) + 4 = 2 + (3 + 4) ], but need not be commutative [ 2 + 3 = 3 + 2 ] or idempotent [ S u S = S ]. They expand the palette of strategies that can be implemented, and allow for more efficient implementations as compared to, for example CRDTs. In this documentation, we’ll refer to the “left-hand side” (LHS) as the preceding document and the “right-hand side” (RHS) as the following one. Keep in mind that both the LHS and RHS may themselves represent a combination of still more ordered documents because, for example, reductions are applied associatively. "},{"title":"append","type":0,"sectionRef":"#","url":"reference/reduction-strategies/append/","content":"append append works with arrays, and extends the left-hand array with items from the right-hand side. collections: - name: example/reductions/append schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Append only works with type &quot;array&quot;. # Others will throw an error at build time. type: array reduce: { strategy: append } required: [key] key: [/key] tests: &quot;Expect we can append arrays&quot;: - ingest: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2] } - { key: &quot;key&quot;, value: [3, null, &quot;abc&quot;] } - verify: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2, 3, null, &quot;abc&quot;] } The right-hand side must always be an array. The left-hand side may be null, in which case the reduction is treated as a no-op and its result remains null. This can be combined with schema conditionals to toggle whether reduction-reduction should be done or not.","keywords":""},{"title":"Composing with conditionals","type":0,"sectionRef":"#","url":"reference/reduction-strategies/composing-with-conditionals/","content":"Composing with conditionals Reduction strategies are JSON Schema annotations. As such, their applicability at a given document location can be controlled through the use of conditional keywords within the schema, like oneOf or if/then/else. This means Flow’s built-in strategies can be combined with schema conditionals to construct a wider variety of custom reduction behaviors. For example, here’s a reset-able counter: collections: - name: example/reductions/sum-reset schema: type: object properties: key: { type: string } value: { type: number } required: [key] # Use oneOf to express a tagged union over &quot;action&quot;. oneOf: # When action = reset, reduce by taking this document. - properties: { action: { const: reset } } reduce: { strategy: lastWriteWins } # When action = sum, reduce by summing &quot;value&quot;. Keep the LHS &quot;action&quot;, # preserving a LHS &quot;reset&quot;, so that resets are properly associative. - properties: action: const: sum reduce: { strategy: firstWriteWins } value: { reduce: { strategy: sum } } reduce: { strategy: merge } key: [/key] tests: &quot;Expect we can sum or reset numbers&quot;: - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: sum, value: 5 } - { key: &quot;key&quot;, action: sum, value: -1.2 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 3.8 } - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: reset, value: 0 } - { key: &quot;key&quot;, action: sum, value: 1.3 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 1.3 } ","keywords":""},{"title":"firstWriteWins and lastWriteWins","type":0,"sectionRef":"#","url":"reference/reduction-strategies/firstwritewins-and-lastwritewins/","content":"firstWriteWins and lastWriteWins firstWriteWins always takes the first value seen at the annotated location. Likewise, lastWriteWins always takes the last. Schemas that don’t have an explicit reduce annotation default to lastWriteWins behavior. collections: - name: example/reductions/fww-lww schema: type: object reduce: { strategy: merge } properties: key: { type: string } fww: { reduce: { strategy: firstWriteWins } } lww: { reduce: { strategy: lastWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can track first- and list-written values&quot;: - ingest: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;one&quot; } - { key: &quot;key&quot;, fww: &quot;two&quot;, lww: &quot;two&quot; } - verify: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;two&quot; } ","keywords":""},{"title":"merge","type":0,"sectionRef":"#","url":"reference/reduction-strategies/merge/","content":"merge merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays. If both sides are objects, merge performs a deep merge of each property. If LHS and RHS are both arrays, items at each index of both sides are merged together, extending the shorter of the two sides by taking items off the longer: collections: - name: example/reductions/merge schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Merge only works with types &quot;array&quot; or &quot;object&quot;. # Others will throw an error at build time. type: [array, object] reduce: { strategy: merge } # Deeply merge sub-locations (items or properties) by summing them. items: type: number reduce: { strategy: sum } additionalProperties: type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can merge arrays by index&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [1, 1] } - { key: &quot;key&quot;, value: [2, 2, 2] } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [3, 3, 2] } &quot;Expect we can merge objects by property&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;b&quot;: 1 } } - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;c&quot;: 1 } } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 2, &quot;b&quot;: 1, &quot;c&quot;: 1 } } Merge may also take a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. Arrays must be pre-sorted and de-duplicated by the key, and merge itself always maintains this invariant. Note that you can use a key of [“”] for natural item ordering, such as merging sorted arrays of scalars. collections: - name: example/reductions/merge-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: type: array reduce: strategy: merge key: [/k] items: { reduce: { strategy: firstWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can merge sorted arrays&quot;: - ingest: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }] } - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 2 }, { k: &quot;c&quot;, v: 2 }] } - verify: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }, { k: &quot;c&quot;, v: 2 }], } As with append, the LHS of merge may be null, in which case the reduction is treated as a no-op and its result remains null.","keywords":""},{"title":"minimize and maximize","type":0,"sectionRef":"#","url":"reference/reduction-strategies/minimize-and-maximize/","content":"minimize and maximize minimize and maximize reduce by taking the smallest or largest seen value, respectively. collections: - name: example/reductions/min-max schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: { reduce: { strategy: minimize } } max: { reduce: { strategy: maximize } } required: [key] key: [/key] tests: &quot;Expect we can min/max values&quot;: - ingest: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;abc&quot; } - { key: &quot;key&quot;, min: 42, max: &quot;def&quot; } - verify: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;def&quot; } minimize and maximize can also take a key, which is one or more JSON pointers that are relative to the reduced location. Keys make it possible to minimize and maximize over complex types by ordering over an extracted composite key. In the event that a right-hand side document key equals the current left-hand side minimum or maximum, the documents are deeply merged. This can be used to, for example, track not just the minimum value but also the number of times it’s been seen: collections: - name: example/reductions/min-max-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: $anchor: min-max-value type: array items: - type: string - type: number reduce: { strategy: sum } reduce: strategy: minimize key: [/0] max: $ref: &quot;#min-max-value&quot; reduce: strategy: maximize key: [/0] required: [key] key: [/key] tests: &quot;Expect we can min/max values using a key extractor&quot;: - ingest: collection: example/reductions/min-max-key documents: - { key: &quot;key&quot;, min: [&quot;a&quot;, 1], max: [&quot;a&quot;, 1] } - { key: &quot;key&quot;, min: [&quot;c&quot;, 2], max: [&quot;c&quot;, 2] } - { key: &quot;key&quot;, min: [&quot;b&quot;, 3], max: [&quot;b&quot;, 3] } - { key: &quot;key&quot;, min: [&quot;a&quot;, 4], max: [&quot;a&quot;, 4] } - verify: collection: example/reductions/min-max-key documents: # Min of equal keys [&quot;a&quot;, 1] and [&quot;a&quot;, 4] =&gt; [&quot;a&quot;, 5]. - { key: &quot;key&quot;, min: [&quot;a&quot;, 5], max: [&quot;c&quot;, 2] } ","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"reference/reduction-strategies/sum/","content":"sum sum reduces two numbers or integers by adding their values. collections: - name: example/reductions/sum schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sum only works with types &quot;number&quot; or &quot;integer&quot;. # Others will throw an error at build time. type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can sum two numbers&quot;: - ingest: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 5 } - { key: &quot;key&quot;, value: -1.2 } - verify: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 3.8 } ","keywords":""},{"title":"set","type":0,"sectionRef":"#","url":"reference/reduction-strategies/set/","content":"set set interprets the document location as an update to a set. The location must be an object having only “add&quot;, “intersect&quot;, and “remove” properties. Any single “add&quot;, “intersect&quot;, or “remove” is always allowed. A document with “intersect” and “add” is allowed, and is interpreted as applying the intersection to the left-hand side set, followed by a union with the additions. A document with “remove” and “add” is also allowed, and is interpreted as applying the removals to the base set, followed by a union with the additions. “remove” and “intersect” within the same document are prohibited. Set additions are deeply merged. This makes sets behave like associative maps, where the “value” of a set member can be updated by adding it to the set again, with a reducible update. Sets may be objects, in which case the object property serves as the set item key: collections: - name: example/reductions/set schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: { strategy: set } # Schema for &quot;add&quot;, &quot;intersect&quot;, and &quot;remove&quot; properties # (each a map of keys and their associated sums): additionalProperties: type: object additionalProperties: type: number reduce: { strategy: sum } # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } required: [key] key: [/key] tests: &quot;Expect we can apply set operations to incrementally build associative maps&quot;: - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 } } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: { &quot;b&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;d&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 2, &quot;c&quot;: 1, &quot;d&quot;: 1 } } } - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: { &quot;a&quot;: 0, &quot;d&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;e&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 3, &quot;d&quot;: 1, &quot;e&quot;: 1 } } } Sets can also be sorted arrays, which are ordered using a provide key extractor. Keys are given as one or more JSON pointers, each relative to the item. As with merge, arrays must be pre-sorted and de-duplicated by the key, and set reductions always maintain this invariant. Use a key extractor of [“”] to apply the natural ordering of scalar values. Whether array or object types are used, the type must always be consistent across the “add” / “intersect” / “remove” terms of both sides of the reduction. collections: - name: example/reductions/set-array schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: strategy: set key: [/0] # Schema for &quot;add&quot;, &quot;intersect&quot;, &amp; &quot;remove&quot; properties # (each a sorted array of [key, sum] 2-tuples): additionalProperties: type: array # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } # Schema for contained [key, sum] 2-tuples: items: type: array items: - type: string - type: number reduce: { strategy: sum } reduce: { strategy: merge } required: [key] key: [/key] tests: ? &quot;Expect we can apply operations of sorted-array sets to incrementally build associative maps&quot; : - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;b&quot;, 1], [&quot;c&quot;, 1]] } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: [[&quot;b&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;d&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 2], [&quot;c&quot;, 1], [&quot;d&quot;, 1]] } } - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: [[&quot;a&quot;, 0], [&quot;d&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;e&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 3], [&quot;d&quot;, 1], [&quot;e&quot;, 1]] } } ","keywords":""},{"title":"Working with logs and statistics","type":0,"sectionRef":"#","url":"reference/working-logs-stats/","content":"","keywords":""},{"title":"Accessing logs​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs","content":"You can access logs by materializing them to an external endpoint, or from the command line. "},{"title":"Accessing logs from the command line​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-from-the-command-line","content":"Beta The flowctl logs subcommand is not currently available due to ongoing development. Command line support will be added back soon. Contact Estuary Support for more information. For now, use a materialization to view logs. The flowctl logs subcommand allows you to print logs from the command line. This method allows more flexibility and is ideal for debugging. You can retrieve logs for any task that is part of a catalog that is currently deployed. Printing logs for a specific task​ You can print logs for a given deployed task using the flag --task followed by the task name. flowctl logs --task acmeCo/anvils/capture-one  Printing all logs for a prefix​ You can print all logs for currently deployed catalogs of a given prefix using the flag --tenant. flowctl logs --tenant acmeCo  This is the same as printing the entire contents of the collection ops/acmeCo/logs. Printing logs by task type​ Within a given prefix, you can print logs for all deployed tasks of a given type using the flag --task-type followed by one of capture, derivation, or materialization. flowctl logs --tenant acmeCo --task-type capture  "},{"title":"Accessing logs by materialization​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-by-materialization","content":"You can materialize your logs collection to an external system. This is typically the preferred method if you’d like to continuously work with or monitor logs. It's easiest to materialize the whole collection, but you can use a partition selector to only materialize specific tasks, as the logs collection is partitioned on tasks. caution Be sure to add a partition selector to exclude the logs of the materialization itself. Otherwise, you could trigger an infinite loop in which the connector materializes its own logs, logs that event, and so on. acmeCo/anvils/logs: endpoint: connector: image: ghcr.io/estuary/materialize-webhook:dev config: address: my.webhook.com bindings: - resource: relativePath: /log/wordcount source: ops/acmeCo/logs # Exclude the logs of this materialization to avoid an infinite loop. partitions: exclude: name: ['acmeCo/anvils/logs']  "}]