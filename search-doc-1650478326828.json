[{"title":"Concepts","type":0,"sectionRef":"#","url":"concepts/","content":"","keywords":""},{"title":"Catalogs​","type":1,"pageTitle":"Concepts","url":"concepts/#catalogs","content":"A catalog comprises all the components that describe how your data pipelines function and behave: captures, collections, derivations, materializations, tests, and more. For example: How to capture data from endpoints into collectionsThe schemas of those collections, which Flow enforcesHow to derive collections as transformations of other source collectionsMaterializations of collections into your endpointsYour tests of schema and derivation behaviors Together the captures, collections, derivations, and materializations of your catalog form a graph of your data flows: graph LR; capture/two--&gt;collection/D; capture/one--&gt;collection/C; capture/one--&gt;collection/A; collection/A--&gt;derivation/B; collection/D--&gt;derivation/E; collection/C--&gt;derivation/E; derivation/B--&gt;derivation/E; collection/D--&gt;materialization/one; derivation/E--&gt;materialization/two; "},{"title":"Namespace​","type":1,"pageTitle":"Concepts","url":"concepts/#namespace","content":"All catalog entities, like collections, are identified by a namesuch as acmeCo/teams/manufacturing/anvils. Names have directory-like prefixes and every name within Flow is globally unique. Thus all catalog entities exist together in a single namespace, much like how all files in S3 are uniquely identified by their bucket and file name. note Prefixes of the namespace, like acmeCo/teams/manufacturing/, are the foundation for Flow's authorization model. If you've ever used database schemas to organize your tables and authorize access, you can think of name prefixes as being akin to database schemas with arbitrary nesting. "},{"title":"Builds​","type":1,"pageTitle":"Concepts","url":"concepts/#builds","content":"Catalog entities like collections are very long-lived and may evolve over time. A collection's schema might be extended with new fields, or a transformation might be updated with a bug fix. When one or more catalog entities are updated, a catalog build validates their definitions and prepares them for execution by Flow's runtime. Every build is assigned a unique identifier called a build ID, and the build ID is used to reconcile which version of a catalog entity is being executed by the runtime. A catalog build is activated into Flow's runtime to deploy its captures, collections, and so on, possibly replacing an older build under which they had been running. "},{"title":"Specifications​","type":1,"pageTitle":"Concepts","url":"concepts/#specifications","content":"A catalog build begins from a set of catalog specificationswhich define the behavior of your catalog: the entities it contains, like captures, collections, and derivations, and their specific behaviors and configuration. You define catalog specifications using either Flow's interactive UI, or by directly creating and editing YAML or JSON files which are typically managed in a Git repository using familiar developer workflows (often called &quot;GitOps&quot;). These files use the extension *.flow.yaml or simply flow.yaml by convention. As a practical benefit, using this extension activates Flow's VS Code integration and auto-complete. Flow integrates with VS Code for development environment support, like auto-complete, tooltips, and inline documentation. Depending on your catalog, you may also have TypeScript modules, JSON schemas, or test fixtures which are also managed in your Git repository. Whether you use the UI or Git-managed specifications is up to you, and teams can switch back and forth depending on what's more familiar. note Flow's UI is under rapid development and expected to be generally available by end of Q1 2022.  "},{"title":"Collections​","type":1,"pageTitle":"Concepts","url":"concepts/#collections","content":"Collections are a collection of documents having a common key and schema. They are the fundamental representation for datasets within Flow, much like a database table. They are best described as a real-time data lake: documents are stored as an organized layout of JSON files in your cloud storage bucket. If Flow needs to read historical data — say, as part of creating a new materialization — it does so by reading from your bucket. You can use regular bucket lifecycle policies to manage the deletion of data from a collection. However, capturing into a collection or materializing from a collection happens within milliseconds. Learn more about collections "},{"title":"Journals​","type":1,"pageTitle":"Concepts","url":"concepts/#journals","content":"Journals provide the low-level storage for Flow collections. Each logical and physical partition of a collection is backed by a journal. Task shards also use journals to provide for their durability and fault tolerance. Each shard has an associated recovery log, which is a journal into which internal checkpoint states are written. Learn more about journals  "},{"title":"Captures​","type":1,"pageTitle":"Concepts","url":"concepts/#captures","content":"A capture is a catalog task which connects to an endpoint and binds one or more of its resources to collections. As documents become available for any of the bindings, Flow validates their schema and adds them to their bound collection. There are two categories of captures: Pull captures which pull documents from an endpoint using a connector.Push captures which expose an URL endpoint which can be directly written into, such as via a Webhook POST. caution Push captures are under development. Learn more about captures  "},{"title":"Materializations​","type":1,"pageTitle":"Concepts","url":"concepts/#materializations","content":"A materialization is a catalog task that connects to an endpoint and binds one or more collections to corresponding endpoint resources. Materializations are the conceptual inverse of captures. As documents become available within bound collections, the materialization keeps endpoint resources, like database tables, up to date using precise, incremental updates. Like captures, materializations are powered by connectors. Learn more about materializations  "},{"title":"Derivations​","type":1,"pageTitle":"Concepts","url":"concepts/#derivations","content":"A derivation is a collection that continuously derives its documents from transformations that are applied to one or more source collections. You can use derivations to map, reshape, and filter documents. They can also be used to tackle complex stateful streaming workflows, including joins and aggregations, and are not subject to the windowing and scaling limitations that are common to other systems. Learn more about derivations  "},{"title":"Schemas​","type":1,"pageTitle":"Concepts","url":"concepts/#schemas","content":"All collections in Flow have an associatedJSON schemaagainst which documents are validated every time they're written or read. Schemas are key to how Flow ensures the integrity of your data. Flow validates your documents to ensure that bad data doesn't make it into your collections — or worse, into downstream data products! Flow pauses catalog tasks when documents don't match the collection schema, alerting you to the mismatch and allowing you to fix it before it creates a bigger problem. "},{"title":"Constraints​","type":1,"pageTitle":"Concepts","url":"concepts/#constraints","content":"JSON schema is a flexible standard for representing structure, invariants, and other constraints over your documents. Schemas can be very permissive, highly exacting, or somewhere in between. JSON schema goes far beyond checking basic document structure. It also supports conditionals and invariants like &quot;I expect all items in this array to be unique&quot;, or &quot;this string must be an email&quot;, or &quot;this integer must be between a multiple of 10 and in the range 0-100&quot;. "},{"title":"Projections​","type":1,"pageTitle":"Concepts","url":"concepts/#projections","content":"Flow leverages your JSON schemas to produce other types of schemas as needed, such as TypeScript types and SQL CREATE TABLE statements. In many cases these projections provide comprehensive end-to-end type safety of Flow catalogs and their TypeScript transformations, all statically verified when the catalog is built. "},{"title":"Reductions​","type":1,"pageTitle":"Concepts","url":"concepts/#reductions","content":"Flow collections have a defined key, which is akin to a database primary key declaration and determines how documents of the collection are grouped. When a collection is materialized into a database table, its key becomes the SQL primary key of the materialized table. This of course raises the question: what happens if multiple documents of a given key are added to a collection? You might expect that the last-written document is the effective document for that key. This &quot;last write wins&quot; treatment is how comparable systems behave, and is also Flow's default. Flow also offers schema extensionsthat give you substantially more control over how documents are combined and reduced.reduce annotations let you deeply merge documents, maintain running counts, and achieve other complex aggregation behaviors. "},{"title":"Key strategies​","type":1,"pageTitle":"Concepts","url":"concepts/#key-strategies","content":"Reduction annotations change the common patterns for how you think about collection keys. Suppose you are building a reporting fact table over events of your business. Today you would commonly consider a unique event ID to be its natural key. You would load all events into your warehouse and perform query-time aggregation. When that becomes too slow, you periodically refresh materialized views for fast-but-stale queries. With Flow, you instead use a collection key of your fact table dimensions, and use reduce annotations to define your metric aggregations. A materialization of the collection then maintains a database table which is keyed on your dimensions, so that queries are both fast and up to date. Learn more about schemas  "},{"title":"Tasks​","type":1,"pageTitle":"Concepts","url":"concepts/#tasks","content":"Captures, derivations, and materializations are collectively referred to as catalog tasks. They are the &quot;active&quot; components of a catalog, each running continuously and reacting to documents as they become available. Collections, by way of comparison, are inert. They reflect data at rest, and are acted upon by catalog tasks: A capture adds documents to a collection pulled from an endpoint.A derivation updates a collection by applying transformations to other source collections.A materialization reacts to changes of a collection to update an endpoint. "},{"title":"Task shards​","type":1,"pageTitle":"Concepts","url":"concepts/#task-shards","content":"Task shards are the unit of execution for a catalog task. A single task can have many shards, which allow the task to scale across many machines to achieve more throughput and parallelism. Shards are created and managed by the Flow runtime. Each shard represents a slice of the overall work of the catalog task, including its processing status and associated internal checkpoints. Catalog tasks are created with a single shard, which can be repeatedly subdivided at any time — with no downtime — to increase the processing capacity of the task. Learn more about shards  "},{"title":"Endpoints​","type":1,"pageTitle":"Concepts","url":"concepts/#endpoints","content":"Endpoints are the external systems that you connect using Flow. All kinds of systems can be endpoints: databases, key/value stores, streaming pub/sub systems, SaaS APIs, and cloud storage locations. Captures pull or ingest data from an endpoint, while materializations push data into an endpoint. There's an essentially unbounded number of different systems and APIs to which Flow might need to capture or materialize data. Rather than attempt to directly integrate them all, Flow's runtime communicates with endpoints through plugin connectors. "},{"title":"Resources​","type":1,"pageTitle":"Concepts","url":"concepts/#resources","content":"An endpoint resource is an addressable collection of data within an endpoint. The exact meaning of a resource is up to the endpoint and its connector. For example: Resources of a database endpoint might be its individual tables.Resources of a Kafka cluster might be its topics.Resources of a SaaS connector might be its various API feeds. "},{"title":"Connectors​","type":1,"pageTitle":"Concepts","url":"concepts/#connectors","content":"There are lots of potential endpoints where you want to work with data. Though Flow is a unified platform for data synchronization, it's impractical for any single company — Estuary included — to provide an integration for every possible endpoint in the growing landscape of data solutions. Connectors are plugin components that bridge the gap between Flow’s runtime and the various endpoints from which you capture or materialize data. They're packaged as Docker images, each encapsulating the details of working with a particular kind of endpoint. The connector then interacts with Flow's runtime through common and open protocols for configuration, introspection of endpoint resources, and to coordinate the movement of data into and out of the endpoint. Crucially, this means Flow doesn't need to know about new types of endpoint ahead of time: so long as a connector is available Flow can work with the endpoint, and it's relatively easy to build a connector yourself. "},{"title":"Discovery​","type":1,"pageTitle":"Concepts","url":"concepts/#discovery","content":"Connectors offer discovery APIs for understanding how a connector should be configured, and what resources are available within an endpoint. Flow works with connector APIs to provide a guided discovery workflow which makes it easy to configure the connector, and select from a menu of available endpoint resources you can capture. Learn more about endpoints and connectors  "},{"title":"Tests​","type":1,"pageTitle":"Concepts","url":"concepts/#tests","content":"You use tests to verify the end-to-end behavior of your collections and derivations. A test is a sequence of ingestion or verification steps. Ingestion steps ingest one or more document fixtures into a collection, and verification steps assert that the contents of another derived collection match a test expectation. Learn more about tests  "},{"title":"Storage mappings​","type":1,"pageTitle":"Concepts","url":"concepts/#storage-mappings","content":"Flow collections use cloud storage buckets for the durable storage of data. Storage mappings define how Flow maps your various collections into your storage buckets and prefixes. Learn more about storage mappings  "},{"title":"flowctl​","type":1,"pageTitle":"Concepts","url":"concepts/#flowctl","content":"flowctl is Flow's command-line interface. It can be used to develop and test Flow catalogs, and to deploy them into a Flow runtime. Learn more about flowctl "},{"title":"Logs and statistics","type":0,"sectionRef":"#","url":"concepts/advanced/logs-stats/","content":"","keywords":""},{"title":"Logs​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#logs","content":"Each organization, or tenant, that uses Flow has a logs collection under the global ops prefix. For the tenant acmeCo, it would have the name ops/acmeCo/logs. These can be thought of as standard application logs: they store information about events that occur at runtime. They’re distinct from recovery logs, which track the state of various task shards. Regardless of how many Flow catalogs your organization has, all logs are stored in the same collection, which is read-only and logically partitioned on tasks. Logs are collected from events that occur within the Flow runtime, as well as the capture and materialization connectors your catalog is using. "},{"title":"Log level​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#log-level","content":"You can set the log level for each catalog task to control the level of detail at which logs are collected for that task. The available levels, listed from least to most detailed, are: error: Non-recoverable errors from the Flow runtime or connector that are critical to know aboutwarn: Errors that can be re-tried, but likely require investigationinfo: Task lifecycle events, or information you might want to collect on an ongoing basisdebug: Details that will help debug an issue with a tasktrace: Maximum level of detail that may yield gigabytes of logs The default log level is info. You can change a task’s log level by adding the shards keyword to its definition in the catalog spec: materializations: acmeCo/debugMaterialization: shards: logLevel: debug endpoint: {} Copy To learn more about working with logs and statistics, see their reference documentation. "},{"title":"Projections","type":0,"sectionRef":"#","url":"concepts/advanced/projections/","content":"","keywords":""},{"title":"Logical partitions​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#logical-partitions","content":"Projections can also be used to logically partition a collection, specified as a longer-form variant of a projection definition: collections: acmeCo/user-sessions: schema: session.schema.yaml key: [/user/id, /timestamp] projections: country: location: /country partition: true device: location: /agent/type partition: true network: location: /agent/network partition: true Copy Logical partitions isolate the storage of documents by their differing values for partitioned fields. Flow extracts partitioned fields from each document, and every unique combination of partitioned fields is a separate logical partition. Every logical partition has one or more physical partitionsinto which their documents are written, which in turn controls how files are arranged within cloud storage. For example, a document of &quot;acmeCo/user-sessions&quot; like: {&quot;country&quot;: &quot;CA&quot;, &quot;agent&quot;: {&quot;type&quot;: &quot;iPhone&quot;, &quot;network&quot;: &quot;LTE&quot;}, ...} Copy Might produce files in cloud storage like: s3://bucket/example/sessions/country=CA/device=iPhone/network=LTE/pivot=00/utc_date=2020-11-04/utc_hour=16/&lt;name&gt;.gz Copy info country, device, and network together identify a logical partition, while pivot identifies a physical partition.utc_date and utc_hour is the time at which the journal fragment was created. Learn more about physical partitions. "},{"title":"Partition selectors​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#partition-selectors","content":"When reading from a collection, Flow catalog entities like derivations, materializations, and tests can provide a partition selector, which identifies the subset of partitions that should be read from a source collection: # Partition selectors are included as part of a larger entity, # such as a derivation or materialization. partitions: # `include` selects partitioned fields and corresponding values that # must be matched in order for a partition to be processed. # All of the included fields must be matched. # Default: All partitions are included. type: object include: # Include partitions from North America. country: [US, CA] # AND where the device is a mobile phone. device: [iPhone, Android] # `exclude` selects partitioned fields and corresponding values which, # if matched, exclude the partition from being processed. # A match of any of the excluded fields will exclude the partition. # Default: No partitions are excluded. type: object exclude: # Skip sessions which were over a 3G network. network: [&quot;3G&quot;] Copy Partition selectors are efficient as they allow Flow to altogether avoid reading documents that aren’t needed. "},{"title":"Journals","type":0,"sectionRef":"#","url":"concepts/advanced/journals/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#specification","content":"Flow collections can control some aspects of how their contents are mapped into journals through the journals stanza: collections: acmeCo/orders: schema: orders.schema.yaml key: [/id] journals: # Configuration for journal fragments. # Required, type: object. fragments: # Codec used to compress fragment files. # One of ZSTANDARD, SNAPPY, GZIP, or NONE. # Optional. Default is GZIP. compressionCodec: GZIP # Maximum flush delay before in-progress fragment buffers are closed # and persisted. Default uses no flush interval. # Optional. Given as a time duration. flushInterval: 15m # Desired content length of each fragment, in megabytes before compression. # Default is 512MB. # Optional, type: integer. length: 512 # Duration for which historical files of the collection should be kept. # Default is forever. # Optional. Given as a time duration. retention: 720h Copy Your storage mappings determine which of your cloud storage buckets is used for storage of collection fragment files. "},{"title":"Physical partitions​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#physical-partitions","content":"Every logical partition of a Flow collection is created with a single physical partition. Later and as required, new physical partitions are added in order to increase the write throughput of the collection. Each physical partition is responsible for all new writes covering a range of keys occurring in collection documents. Conceptually, if keys range from [A-Z] then one partition might cover [A-F] while another covers [G-Z]. The pivot of a partition reflects the first key in its covered range. One physical partition is turned into more partitions by subdividing its range of key ownership. For instance, a partition covering [A-F]is split into partitions [A-C] and [D-F]. Physical partitions are journals. The relationship between the journal and its specific collection and logical partition are encoded withinits journal specification. "},{"title":"Fragment files​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#fragment-files","content":"Journal fragment files each hold a slice of your collection's content, stored as a compressed file of newline-delimited JSON documents in your cloud storage bucket. Files are flushed to cloud storage periodically, typically after they reach a desired size threshold. They use a content-addressed naming scheme which allows Flow to understand how each file stitches into the overall journal. Consider a fragment file path like: s3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/0000000000000000-00000000201a3f27-1ec69e2de187b7720fb864a8cd6d50bb69cc7f26.gz This path has the following components: Component\tExampleStorage prefix of physical partition\ts3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/ Supplemental time pseudo-partitions\tutc_date=2022-01-07/utc_hour=19/ Beginning content offset\t0000000000000000 Ending content offset\t00000000201a3f27 SHA content checksum\t1ec69e2de187b7720fb864a8cd6d50bb69cc7f26 Compression codec\t.gz The supplemental time pseudo-partitions are not logical partitions, but are added to each fragment file path to facilitate integration with external tools that understand Hive layouts. "},{"title":"Hive layouts​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#hive-layouts","content":"As we've seen, collection fragment files are written to cloud storage with path components like/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/. If you've used tools within the Apache Hive ecosystem, this layout should feel familiar. Flow organizes files in this way to make them directly usable by tools that understand Hive partitioning, like Spark and Hive itself. Collections can also be integrated as Hive-compatible external tables in tools likeSnowflakeandBigQueryfor ad-hoc analysis. SQL queries against these tables can even utilize predicate push-down, taking query predicates over category, utc_date, and utc_hourand pushing them down into the selection of files that must be read to answer the query — often offering much faster and more efficient query execution because far less data must be read. "},{"title":"Task shards","type":0,"sectionRef":"#","url":"concepts/advanced/shards/","content":"","keywords":""},{"title":"Shard splits​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#shard-splits","content":"When a task is first created, it is initialized with a single shard. Later and as required, a shard can be split into two shards. Once initiated, the split may require up to a few minutes to complete, but it doesn't require downtime and the selected shard continues to run until the split occurs. This process can be repeated as needed until your required throughput is achieved. TODO This section is incomplete. See flowctl shards split --help for further details. "},{"title":"Recovery logs​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#recovery-logs","content":"info Shard stores and associated states are transparent to you, the Flow user. This section is informational only, to provide a sense of how Flow works. All task shards have associated state, which is managed in the shard's store. Capture tasks must track incremental checkpoints of their endpoint connectors.Derivation tasks manage a potentially very large index of registers, as well as read checkpoints of sourced collection journals.Materialization tasks track incremental checkpoints of their endpoint connectors, as well as read checkpoints of sourced collection journals. Shard stores userecovery logsto replicate updates and implement transaction semantics. Recovery logs are regular journals, but hold binary data and are not intended for direct use. However, they can hold your user data. Recovery logs of derivations hold your derivation register values. Recovery logs are stored in your cloud storage bucket, and must have a configured storage mapping. "},{"title":"Captures","type":0,"sectionRef":"#","url":"concepts/captures/","content":"","keywords":""},{"title":"Pull captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#pull-captures","content":"Pull captures pull documents from an endpoint using a connector: # A set of captures to include in the catalog. # Optional, type: object captures: # The name of the capture. acmeCo/example/source-s3: # Endpoint defines how to connect to the source of the capture. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image which implements the capture connector. image: ghcr.io/estuary/source-s3:dev # File which provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how collections are populated from the data source. # A capture may bind multiple resources to different collections. # Required, type: array bindings: - # The target collection to capture into. # This may be defined in a separate, imported catalog source file. # Required, type: string target: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and capture a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: stream: a-bucket/and-prefix # syncMode should be set to incremental for all Estuary connectors syncMode: incremental - target: acmeCo/example/another-collection resource: stream: a-bucket/another-prefix syncMode: incremental Copy "},{"title":"Estuary sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#estuary-sources","content":"Estuary builds and maintains many real-time connectors for various technology systems, such as database change data capture (CDC) connectors. See the source connector reference documentation. "},{"title":"Airbyte sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#airbyte-sources","content":"Flow also natively supports Airbyte source connectors. These connectors tend to focus on SaaS APIs, and do not offer real-time streaming integrations. Flow runs the connector at regular intervals to capture updated documents. A list of third-party connectors can be found on theAirbyte docker hub. You can use any item whose name begins with source-. "},{"title":"Discovery​","type":1,"pageTitle":"Captures","url":"concepts/captures/#discovery","content":"Flow offers a CLI tool flowctl discover --image connector/image:tag which provides a guided workflow for creating a correctly configured capture. "},{"title":"Push captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#push-captures","content":"Push captures expose an endpoint to which documents may be pushed using a supported ingestion protocol: captures: # The name of the capture. acmeCo/example/webhook-ingest: endpoint: # This endpoint is an ingestion. ingest: {} bindings: - # The target collection to capture into. target: acmeCo/example/webhooks # The resource configures the specific behavior of the ingestion endpoint. resource: name: webhooks Copy caution Push captures are under development. Estuary intends to offer Webhook, Websocket, and Kafka-compatible APIs for capturing into collections. Specification details are likely to exist. "},{"title":"Collections","type":0,"sectionRef":"#","url":"concepts/collections/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Collections","url":"concepts/collections/#specification","content":"Collections are expressed within a Flow catalog specification: # A set of collections to include in the catalog. # Optional, type: object collections: # The unique name of the collection. acmeCo/products/anvils: # The schema of the collection, against which collection documents # are validated. This may be an inline definition or a relative URL # reference. # Required, type: string (relative URL form) or object (inline form) schema: anvils.schema.yaml # The key of the collection, specified as JSON pointers of one or more # locations within collection documents. If multiple fields are given, # they act as a composite key, equivalent to a SQL table PRIMARY KEY # with multiple table columns. # Required, type: array key: [/product/id] # Projections and logical partitions for this collection. # See the &quot;Projections&quot; concept page to learn more. # Optional, type: object projections: # Derivation that builds this collection from others through transformations. # See the &quot;Derivations&quot; concept page to learn more. # Optional, type: object derivation: Copy "},{"title":"Schemas​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schemas","content":"Every Flow collection must declare a schema, and will never accept documents that do not validate against the schema. This helps ensure the quality of your data products and the reliability of your derivations and materializations. Schema specifications are flexible: yours could be exactingly strict, extremely permissive, or somewhere in between. Schemas may either be declared inline, or provided as a reference to a file. References can also include JSON pointers as a URL fragment to name a specific schema of a larger schema document: InlineFile referenceReference with pointer collections: acmeCo/collection: schema: type: object required: [id] properties: id: string key: [/id] Copy Learn more about schemas "},{"title":"Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#keys","content":"Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. For example: flow.yamlschema.yaml collections: acmeCo/users: schema: schema.yaml key: [/userId] Copy Suppose the following JSON documents are captured into acmeCo/users: {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;William&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} Copy As its key is [/userId], a materialization of the collection into a database table will reduce to a single row: userId | name 1 | Will Copy If its key were instead [/name], there would be two rows in the table: userId | name 1 | Will 1 | William Copy "},{"title":"Schema restrictions​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schema-restrictions","content":"Keyed document locations may be of a limited set of allowed types: booleanintegerstring Excluded types are: arraynullobjectFractional number Keyed fields also must always exist in collection documents. Flow performs static inference of the collection schema to verify the existence and types of all keyed document locations, and will report an error if the location could not exist, or could exist with the wrong type. Flow itself doesn't mind if a keyed location could have multiple types, so long as they're each of the allowed types: an integer or string for example. Some materialization connectors, however, may impose further type restrictions as required by the endpoint. For example, SQL databases do not support multiple types for a primary key. "},{"title":"Composite Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#composite-keys","content":"A collection may have multiple locations which collectively form a composite key. This can include locations within nested objects and arrays: flow.yamlschema.yaml collections: acmeCo/compound-key: schema: schema.yaml key: [/foo/a, /foo/b, /foo/c/0, /foo/c/1] Copy "},{"title":"Key behaviors​","type":1,"pageTitle":"Collections","url":"concepts/collections/#key-behaviors","content":"A collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. Flow also performs opportunistic local reductions over windows of documents (also called &quot;combines&quot;) to improve its performance and reduce the volumes of data at each processing stage. An important subtlety is that the underlying storage of a collection will potentially retain many documents of a given key. In the acmeCo/users example, each of the &quot;Will&quot; or &quot;William&quot; variants is likely represented in the collection's storage — so long as they didn't arrive so closely together that they were locally combined by Flow. If desired, a derivation could re-key the collection on [/userId, /name] to materialize the various /names seen for a /userId. This property makes keys less lossy than they might otherwise appear, and it is generally good practice to chose a key that reflects how you wish to query a collection, rather than an exhaustive key that's certain to be unique for every document. "},{"title":"Projections​","type":1,"pageTitle":"Collections","url":"concepts/collections/#projections","content":"Projections are named locations within a collection document that may be used for logical partitioning or directly exposed to databases into which collections are materialized. Many projections are automatically inferred from the collection schema. The projections stanza can be used to provide additional projections, and to declare logical partitions: collections: acmeCo/products/anvils: schema: anvils.schema.yaml key: [/product/id] # Projections and logical partitions for this collection. # Keys name the unique projection field, and values are its JSON Pointer # location within the document and configure logical partitioning. # Optional, type: object projections: # Short form: define a field &quot;product_id&quot; with document pointer /product/id. product_id: &quot;/product/id&quot; # Long form: define a field &quot;metal&quot; with document pointer /metal_type # which is a logical partition of the collection. metal: location: &quot;/metal_type&quot; partition: true Copy Learn more about projections. "},{"title":"Storage​","type":1,"pageTitle":"Collections","url":"concepts/collections/#storage","content":"Collections are real-time data lakes. Historical documents of the collection are stored as an organized layout of regular JSON files in your cloud storage bucket. Reads of that history are served by directly reading files from your bucket. Your storage mappingsdetermine how Flow collections are mapped into your cloud storage buckets. Unlike a traditional data lake, however, it's very efficient to read collection documents as they are written. Derivations and materializations that source from a collection are notified of its new documents within milliseconds of their being published. Learn more about journals, which provide storage for collections "},{"title":"flowctl","type":0,"sectionRef":"#","url":"concepts/flowctl/","content":"","keywords":""},{"title":"Build directory​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#build-directory","content":"When building Flow catalogs, flowctl uses a build directorywhich is typically the root directory of your project, and is controlled by flag --directory. Within this directory, flowctl creates a number of files and sub-directories. Except where noted, it's recommended that these outputs be committed within your GitOps project. flow_generated/: ♻ Directory of generated files, including TypeScript classes and interfaces. See Typescript code generation. *.flow.ts: ⚓ TypeScript modules that accompany your catalog source files. A stub is generated for you if your catalog source uses a TypeScript lambda, and a module doesn't yet exist. See Typescript code generation andlearn how TypeScript modules are imported. dist/: ♻ Holds JavaScript and source map files produced during TypeScript compilation.dist/ should be added to your .gitignore. node_modules/: ♻ Location where npm will download your TypeScript dependencies.node_modules/ should be added to your .gitignore. package.json and package-lock.json: ♻ Files used by npm to manage dependencies and your catalog's associated JavaScript project. You may customize package.json, but its dependencies stanza will be overwritten by thenpmDependenciesof your catalog source files. .eslintrc.js: ⚓ Configures the TypeScript linter that's run as part of the catalog build process. Linting supplements TypeScript compilation to catch additional common mistakes and errors at build time. .prettierrc.js: ⚓ Configures the formatter that's used to format your TypeScript files. Legend ⚓: Generated only if it does not exist. Never modified or deleted by flowctl. ♻: flowctl re-generates and overwrites contents. "},{"title":"TypeScript code generation​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#typescript-code-generation","content":"As part of the catalog build process, Flow translates yourschemasinto equivalent TypeScript types on your behalf. These definitions live within flow_generated/ in your catalog build directory, and are frequently over-written by invocations of flowctl. Files in this subdirectory are human-readable and stable. You may want to commit them as part of a GitOps-managed project, but this isn't required. Flow also generates TypeScript module stubs for Flow catalog sources, which reference a TypeScript lambda, if that particular Flow catalog source doesn't yet have an accompanying TypeScript module. Generated stubs include implementations of the required TypeScript interfaces, with all method signatures filled out for you: acmeBank.flow.yamlacmeBank.flow.ts (generated stub) collections: acmeBank/balances: schema: balances.schema.yaml key: [/account] derivation: transform: fromTransfers: source: { name: acmeBank/transfers } publish: { lambda: typescript } Copy If a TypeScript module exists, flowctl will never over-write it, even if you update or expand your catalog sources such that the required interfaces have changed. tip If you make changes to a catalog source file my.flow.yaml that substantially change the required TypeScript interfaces, try re-naming an existingmy.flow.ts to another name like old.flow.ts. Then run flowctl check to re-generate a new implementation stub, which will have correct interfaces and can be updated from the definitions of old.flow.ts. "},{"title":"Imports","type":0,"sectionRef":"#","url":"concepts/import/","content":"","keywords":""},{"title":"Fetch behavior​","type":1,"pageTitle":"Imports","url":"concepts/import/#fetch-behavior","content":"Flow resolves, fetches, and validates all imports during the catalog build process, and then includes their fetched contents within the built catalog. The built catalog is thus a self-contained snapshot of all resourcesas they were at the time the catalog was built. This means it's both safe and recommended to directly reference an authoritative source of a resource, such as a third-party JSON schema. It will be fetched and verified only at catalog build time, and thereafter that fetched version will be used for execution, regardless of whether the authority URL itself later changes or errors. "},{"title":"Import types​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-types","content":"Almost always, the import stanza is used to import other Flow catalog source files. This is the default when given a string path: import: - path/to/source/catalog.flow.yaml Copy A long-form variant also accepts a content type of the imported resource: import: - url: path/to/source/catalog.flow.yaml contentType: CATALOG Copy Other permitted content types include JSON_SCHEMA and TYPESCRIPT_MODULE, but these are not typically used and are needed only for advanced use cases. "},{"title":"JSON Schema $ref​","type":1,"pageTitle":"Imports","url":"concepts/import/#json-schema-ref","content":"Certain catalog entities, like collections, commonly reference JSON schemas. It's not necessary to explicitly add these to the import section; they are automatically resolved and treated as an import. You can think of this as an analog to the JSON Schema $ref keyword, which is used to reference a schema that may be contained in another file. The one exception is schemas that use the $id keyword at their root to define an alternative canonical URL. In this case, the schema must be referenced through its canonical URL, and then explicitly added to the import section with JSON_SCHEMA content type. "},{"title":"TypeScript modules​","type":1,"pageTitle":"Imports","url":"concepts/import/#typescript-modules","content":"Certain entities in your catalog spec — typically derivations — may use TypeScript lambda definitions. These lambdas are conventionally defined in TypeScript modules that accompany the specific catalog spec file. Flow looks for and automatically imports TypeScript modules which live alongside a Flow catalog spec file. Given a Flow catalog spec at /path/to/my.flow.yaml, Flow automatically imports the TypeScript module /path/to/my.flow.ts. This is conventionally the module which implements all TypeScript lambdas related to catalog entities defined in my.flow.yaml. You do not need to add my.flow.ts to the import stanza. However, Flow must know of all additional TypeScript modules that are part of the catalog. If other modules are needed, they must be added as a to the import section with TYPESCRIPT_MODULE content type. "},{"title":"NPM dependencies​","type":1,"pageTitle":"Imports","url":"concepts/import/#npm-dependencies","content":"Your TypeScript modules may depend on otherNPM packages, which can be be imported through the npmDependenciesstanza of a Flow catalog spec. For example, moment is a common library for working with times: catalog.flow.yamlcatalog.flow.ts npmDependencies: moment: &quot;^2.24&quot; collections: { ... } Copy Use any version string understood by package.json, which can include local packages, GitHub repository commits, and more. See package.json documentation. During the catalog build process, Flow gathers NPM dependencies across all catalog source files and patches them into the catalog's managed package.json. Flow organizes its generated TypeScript project structure for a seamless editing experience out of the box with VS Code and other common editors. "},{"title":"Import paths​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-paths","content":"If a catalog source file foo.flow.yaml references a collection in bar.flow.yaml, for example as a target of a capture, there must be an import path where either foo.flow.yamlimports bar.flow.yaml or vice versa. Import paths can be direct: graph LR; foo.flow.yaml--&gt;bar.flow.yaml; Or they can be indirect: graph LR; bar.flow.yaml--&gt;other.flow.yaml; other.flow.yaml--&gt;foo.flow.yaml; The sources must still have an import path even if referenced from a common parent. The following would not work: graph LR; parent.flow.yaml--&gt;foo.flow.yaml; parent.flow.yaml--&gt;bar.flow.yaml; These rules make your catalog sources more self-contained and less brittle to refactoring and reorganization. Consider what might otherwise happen if foo.flow.yamlwere imported in another project without bar.flow.yaml. "},{"title":"Schemas","type":0,"sectionRef":"#","url":"concepts/schemas/","content":"","keywords":""},{"title":"JSON Schema​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#json-schema","content":"JSON Schemais an expressive open standard for defining the schema and structure of documents. Flow uses it for all schemas defined within a Flow catalog. JSON Schema goes well beyond basic type information and can modeltagged unions, recursion, and other complex, real-world composite types. Schemas can also define rich data validations like minimum and maximum values, regular expressions, dates, timestamps, email addresses, and other formats. Together, these features let schemas represent structure as well asexpectations and constraints that are evaluated and must hold true for every collection document before it’s added to the collection. They’re a powerful tool for ensuring end-to-end data quality: for catching data errors and mistakes early, before they can impact your production data products. "},{"title":"Generation​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#generation","content":"When capturing data from an external system, Flow is often able to generate suitable JSON schemas on your behalf. Learn more about using connectors "},{"title":"Translations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#translations","content":"You must only provide Flow a model of a given dataset one time, as a JSON schema. Having done that, Flow leverages static inference over your schemas to perform many build-time validations of your catalog entities, helping you catch potential problems early. Schema inference is also used to provide translations into other schema flavors: Most projections of a collection are automatically inferred from its schema. Materializations use your projections to create appropriate representations in your endpoint system. A SQL connector will create table definitions with appropriate columns, types, and constraints.Flow generates TypeScript definitions from schemas to provide compile-time type checks of user lambda functions. These checks are immensely helpful for surfacing mismatched expectations around, for example, whether a field could ever be null or is misspelt — which, if not caught, might otherwise fail at runtime. "},{"title":"Annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#annotations","content":"The JSON Schema standard introduces the concept ofannotations, which are keywords that attach metadata to a location within a validated JSON document. For example, title and description can be used to annotate a schema with its meaning: properties: myField: title: My Field description: A description of myField Copy Flow extends JSON Schema with additional annotation keywords, which provide Flow with further instruction of how documents should be processed. What’s especially powerful about annotations is that they respond toconditionals within the schema. Consider a schema validating a positive or negative number: type: number oneOf: - exclusiveMinimum: 0 description: A positive number. - exclusiveMaximum: 0 description: A negative number. - const: 0 description: Zero. Copy Here, the activated description of this schema location depends on whether the integer is positive, negative, or zero. "},{"title":"Writing schemas​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#writing-schemas","content":"Your schema can be quite permissive or as strict as you wish. There are a few things to know, however. The top-level type must be object. Flow adds a bit of metadata to each of your documents under the _meta property, which can only be done with a top-level object. Any fields that are part of the collection's key must provably exist in any document that validates against the schema. Put another way, every document within a collection must include all of the fields of the collection's key, and the schema must guarantee that. For example, the following collection schema would be invalid because the id field, which is used as its key, is not required, so it might not actually exist in all documents: collections: acmeCo/whoops: schema: type: object required: [value] properties: id: {type: integer} value: {type: string} key: [/id] Copy To fix the above schema, change required to [id, value]. Learn more of how schemas can be expressed within collections. "},{"title":"Organization​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#organization","content":"JSON schema has a $ref keyword which is used to reference a schema stored elsewhere. Flow resolves $ref as a relative URL of the current file, and also supportsJSON fragment pointersfor referencing a specific schema within a larger schema document, such as ../my/widget.schema.yaml#/path/to/schema. It's recommended to use references in order to organize your schemas for reuse. $ref can also be used in combination with other schema keywords to further refine a base schema. Here's an example that uses references to organize and further tighten the constraints of a reused base schema: flow.yamlschemas.yaml collections: acmeCo/coordinates: key: [/id] schema: schemas.yaml#/definitions/coordinate acmeCo/integer-coordinates: key: [/id] schema: schemas.yaml#/definitions/integer-coordinate acmeCo/positive-coordinates: key: [/id] schema: # Compose a restriction that `x` &amp; `y` must be positive. $ref: schemas.yaml#/definitions/coordinate properties: x: {exclusiveMinimum: 0} y: {exclusiveMinimum: 0} Copy tip You can write your JSON schemas as either YAML or JSON across any number of files, all referenced from Flow catalog files or other schemas. Schema references are always resolved as URLs relative to the current file, but you can also use absolute URLs to a third-party schema likeschemastore.org. "},{"title":"Reductions​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reductions","content":"Flow collections have keys, and multiple documents may be added to collections that share a common key. When this happens, Flow will opportunistically merge all such documents into a single representative document for that key through a process known as reduction. Flow's default is simply to retain the most recent document of a given key, which is often the behavior that you're after. Schema reduce annotations allow for far more powerful behaviors. The Flow runtime performs reductions frequently and continuously to reduce the overall movement and cost of data transfer and storage. A torrent of input collection documents can often become a trickle of reduced updates that must be stored or materialized into your endpoints. info Flow never delays processing in order to batch or combine more documents, as some systems do (commonly known as micro-batches, or time-based polling). Every document is processed as quickly as possible, from end to end. Instead Flow uses optimistic transaction pipelining to do as much useful work as possible, while it awaits the commit of a previous transaction. This natural back-pressure affords plenty of opportunity for data reductions while minimizing latency. "},{"title":"reduce annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reduce-annotations","content":"Reduction behaviors are defined by reduceJSON schema annotationswithin your document schemas. These annotations provide Flow with the specific reduction strategies to use at your various document locations. If you're familiar with the map and reduce primitives present in Python, Javascript, and many other languages, this should feel familiar. When multiple documents map into a collection with a common key, Flow reduces them on your behalf by using your reduce annotations. Here's an example that sums an integer: type: integer reduce: { strategy: sum } # 1, 2, -1 =&gt; 2 Copy Or deeply merges a map: type: object reduce: { strategy: merge } # {&quot;a&quot;: &quot;b&quot;}, {&quot;c&quot;: &quot;d&quot;} =&gt; {&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: &quot;d&quot;} Copy Learn more in thereductions strategiesreference documentation. Composition with conditionals​ Like any other JSON Schema annotation,reduce annotations respond to schema conditionals. Here we compose append and lastWriteWins strategies to reduce an appended array which can also be cleared: type: array oneOf: # If the array is non-empty, reduce by appending its items. - minItems: 1 reduce: { strategy: append } # Otherwise, if the array is empty, reset the reduced array to be empty. - maxItems: 0 reduce: { strategy: lastWriteWins } # [1, 2], [3, 4, 5] =&gt; [1, 2, 3, 4, 5] # [1, 2], [], [3, 4, 5] =&gt; [3, 4, 5] # [1, 2], [3, 4, 5], [] =&gt; [] Copy Combining schema conditionals with annotations can be used to buildrich behaviors. "},{"title":"Storage mappings","type":0,"sectionRef":"#","url":"concepts/storage-mappings/","content":"","keywords":""},{"title":"Recovery logs​","type":1,"pageTitle":"Storage mappings","url":"concepts/storage-mappings/#recovery-logs","content":"Flow tasks — captures, derivations, and materializations — use recovery logs to durably store their processing context. Recovery logs are an opaque binary log, but may contain user data and are stored within the user’s buckets. They must have a defined storage mapping. The recovery logs of a task are always prefixed by recovery/, and a task named acmeCo/produce-TNT would require a storage mapping like: storageMappings: recovery/acmeCo/: stores: - provider: S3 bucket: acmeco-recovery Copy You may wish to use a separate bucket for recovery logs, distinct from the bucket where collection data is stored. Buckets holding collection data are free to use a bucket lifecycle policy to manage data retention; for example, to remove data after six months. This is not true of buckets holding recovery logs. Flow prunes data from recovery logs once it is no longer required. warning Deleting data from recovery logs while it is still in use can cause Flow processing tasks to fail permanently. "},{"title":"Materializations","type":0,"sectionRef":"#","url":"concepts/materialization/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#specification","content":"Materializations are expressed within a Flow catalog specification: # A set of materializations to include in the catalog. # Optional, type: object materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image which implements the materialization connector. image: ghcr.io/estuary/materialize-postgres:dev # File which provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how one or more collections map to materialized endpoint resources. # A single materialization may include many collections and endpoint resources, # each defined as a separate binding. # Required, type: object bindings: - # The source collection to materialize. # This may be defined in a separate, imported catalog source file. # Required, type: string source: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and materialize a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: # The materialize-postgres connector expects a `table` key # which names a table to materialize into. table: example_table Copy "},{"title":"Continuous materialized views​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#continuous-materialized-views","content":"Flow materializations are continuous materialized views. They maintain a representation of the collection within the endpoint system as a resource that is updated in near real-time. It's indexed on thecollection key. As the materialization runs, it ensures that all collection documents and their accumulated reductions are reflected in this managed endpoint resource. For example, consider a collection and its materialization:  collections: acmeCo/colors: key: [/color] schema: type: object required: [color, total] reduce: {strategy: merge} properties: color: {enum: [red, blue, purple]} total: type: integer reduce: {strategy: sum} materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/colors resource: { table: colors } Copy Suppose documents are periodically added to the collection: {&quot;color&quot;: &quot;red&quot;, &quot;total&quot;: 1} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 2} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 3} Copy Its materialization into a database table will have a single row for each unique color. As documents arrive in the collection, the row total is updated within the materialized table so that it reflects the overall count:  When you first declare a materialization, Flow back-fills the endpoint resource with the historical documents of the collection. Once caught up, Flow applies new collection documents using incremental and low-latency updates. As collection documents arrive, Flow: Reads previously materialized documents from the endpoint for the relevant keysReduces new documents into these read documentsWrites updated documents back into the endpoint resource, indexed by their keys Flow does not keep separate internal copies of collection or reduction states, as some other systems do. The endpoint resource is the one and only place where state &quot;lives&quot; within a materialization. This makes materializations very efficient and scalable to operate. They are able to maintain very large tables stored in highly scaled storage systems like OLAP warehouses, BigTable, or DynamoDB. "},{"title":"Projected fields​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#projected-fields","content":"Many systems are document-oriented and can directly work with collections of JSON documents. Others systems are table-oriented and require an up-front declaration of columns and types to be most useful, such as a SQL CREATE TABLE definition. Flow uses collection projections to relate locations within a hierarchical JSON document to equivalent named fields. A materialization can in turn select a subset of available projected fields where, for example, each field becomes a column in a SQL table created by the connector. It would be tedious to explicitly list projections for every materialization, though you certainly can if desired. Instead, Flow and the materialization connector negotiate a recommended field selection on your behalf, which can be fine-tuned. For example, a SQL database connector will typically require that fields comprising the primary key be included, and will recommend that scalar values be included, but will by default exclude document locations that don't have native SQL representations, such as locations which can have multiple JSON types or are arrays or maps. materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/example/collection resource: { table: example_table } # Select (or exclude) projections of the collection for materialization as fields. # If not provided, the recommend fields of the endpoint connector are used. # Optional, type: object fields: # Whether to include fields that are recommended by the endpoint connector. # If false, then fields can still be added using `include`. # Required, type: boolean recommended: true # Fields to exclude. This is useful for deselecting a subset of recommended fields. # Default: [], type: array exclude: [myField, otherField] # Fields to include. This can supplement recommended fields, or can # designate explicit fields to use if recommended fields are disabled. # # Values of this map are used to customize connector behavior on a per-field basis. # They are passed directly to the connector and are not interpreted by Flow. # Consult your connector's documentation for details of what customizations are available. # This is an advanced feature and is not commonly used. # # default: {}, type: object include: {goodField: {}, greatField: {}} Copy "},{"title":"Partition selectors​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#partition-selectors","content":"Partition selectors let you materialize only a subset of a collection that haslogical partitions. For example, you might have a large collection that is logically partitioned on each of your customers: collections: acmeCo/anvil/orders: key: [/id] schema: orders.schema.yaml projections: customer: location: /order/customer partition: true Copy A large customer asks if you can provide an up-to-date accounting of their orders. This can be accomplished with a partition selector: materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/anvil/orders resource: { table: coyote_orders } # Process partitions where &quot;Coyote&quot; is the customer. partitions: include: customer: [Coyote] Copy Learn more about partition selectors. "},{"title":"SQLite endpoint​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#sqlite-endpoint","content":"In addition to materialization connectors, Flow offers a built-in SQLite endpoint for local testing and development. SQLite is not suitable for materializations running within a managed data plane. materializations: acmeCo/example/database-views: endpoint: # A SQLite endpoint is specified using `sqlite` instead of `connector`. sqlite: # The SQLite endpoint requires the `path` of the SQLite database to use, # specified as a file path. It may include URI query parameters; # See: https://www.sqlite.org/uri.html and https://github.com/mattn/go-sqlite3#connection-string path: example/database.sqlite?_journal_mode=WAL Copy "},{"title":"Backpressure​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#backpressure","content":"Flow processes updates in transactions, as quickly as the endpoint can handle them. This might be milliseconds in the case of a fast key/value store, or many minutes in the case of an OLAP warehouse. If the endpoint is also transactional, Flow integrates its internal transactions with those of the endpoint for integrated end-to-end “exactly once” semantics. The materialization is sensitive to back pressure from the endpoint. As a database gets busy, Flow adaptively batches and combines documents to consolidate updates: In a given transaction, Flow reduces all incoming documents on the collection key. Multiple documents combine and result in a single endpoint read and write during the transaction.As a target database becomes busier or slower, transactions become larger. Flow does more reduction work within each transaction, and each endpoint read or write accounts for an increasing volume of collection documents. This allows you to safely materialize a collection with a high rate of changes into a small database, so long as the cardinality of the materialization is of reasonable size. "},{"title":"Delta updates​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#delta-updates","content":"As described above, Flow's standard materialization mechanism involves querying the target system for data state before reducing new documents directly into it. For these standard updates to work, the endpoint must be a stateful system, like a relational database. However, other systems — like Webhooks and Pub/Sub — may also be endpoints. None of these typically provide a state representation that Flow can query. They are write-only in nature, so Flow cannot use their endpoint state to help it fully reduce collection documents on their keys. Even some stateful systems are incompatible with Flow's standard updates due to their unique design and architecture. For all of these endpoints, Flow offers a delta-updates mode. When using delta updates, Flow does not attempt to maintain full reductions of each unique collection key. Instead, Flow locally reduces documents within each transaction (this is often called a &quot;combine&quot;), and then materializes onedelta document per key to the endpoint. In other words, when delta updates are used, Flow sends information about data changes by key, and further reduction is left up to the endpoint system. Some systems may reduce documents similar to Flow; others use a different mechanism; still others may not perform reductions at all. A given endpoint may support standard updates, delta updates, or both. This depends on the materialization connector. Expect that a connector will use standard updates only unless otherwise noted in its documentation. "},{"title":"Connectors","type":0,"sectionRef":"#","url":"concepts/connectors/","content":"","keywords":""},{"title":"Why an open connector architecture?​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#why-an-open-connector-architecture","content":"Historically, data platforms have directly implemented integrations to external systems with which they interact. Today, there are simply so many systems and APIs that companies use, that it’s not feasible for a company to provide all possible integrations. Users are forced to wait indefinitely while the platform works through their prioritized integration list. An open connector architecture removes Estuary — or any company — as a bottleneck in the development of integrations. Estuary contributes open-source connectors to the ecosystem, and in turn is able to leverage connectors implemented by others. Users are empowered to write their own connectors for esoteric systems not already covered by the ecosystem. Furthermore, implementing a Docker-based community specification brings other important qualities to Estuary connectors: Cross-platform interoperability between Flow, Airbyte, and any other platform that supports the protocolThe abilities to write connectors in any language and run them on any machineBuilt-in solutions for version management (through image tags) and distributionThe ability to integrate connectors from different sources at will, without the centralized control of a single company, thanks to container image registries "},{"title":"Using connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#using-connectors","content":"Connectors are packaged as Open Container (Docker) images, and can be discovered, tagged, and pulled usingDocker Hub,GitHub Container registry, or any other public image registry provider. To interface with a connector, the Flow runtime needs to know: The specific image to use, through an image name such as ghcr.io/estuary/source-postgres:dev. Notice that the image name also conveys the specific image registry and version tag to use. Endpoint configuration such as a database address and account, with meaning that is specific to the connector. Resource configuration such as a specific database table to capture, which is also specific to the connector. To integrate a connector within your dataflow, you define all three components within your catalog specification: materializations: acmeCo/postgres-views: endpoint: connector: # 1: Provide the image that implements your endpoint connector. image: ghcr.io/estuary/materialize-postgres:dev # 2: Provide endpoint configuration that the connector requires. config: host: localhost password: password database: postgres user: postgres port: 5432 bindings: - source: acmeCo/products/anvils # 3: Provide resource configuration for the binding between the Flow # collection and the endpoint resource. This connector interfaces # with a SQL database and its resources are database tables. Here, # we provide a table to create and materialize which is bound to the # `acmeCo/products/anvils` source collection. resource: table: anvil_products # Multiple resources can be configured through a single connector. # Bind additional collections to tables as part of this connector instance: - source: acmeCo/products/TNT resource: table: tnt_products - source: acmeCo/customers resource: table: customers Copy In some cases, you may be comfortable writing out the required configuration of your connector. Often, you don't know what configuration a connector requires ahead of time, or you may simply prefer a more guided workflow. For this reason, connectors offer APIs that specify the configuration they may require, or the resources they may have available. Flow uses these APIs to offer guided workflows for easy configuration and usage of connectors. The different processes you can use to implement connectors are each described below in general terms. Configuration details for each connector are described on their individual pages. info Estuary is implementing better UI-driven workflows to easily configure and use connectors, expected by Q2 2022. The support offered today is rudimentary. "},{"title":"flowctl discover​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#flowctl-discover","content":"The flowctl command-line tool offers a rudimentary guided workflow for creating a connector instance through the discover sub-command discover generates a catalog source file. It includes the capture specification as well as recommended collections, which are bound to each captured resource of the endpoint. This makes the discover workflow a quick way to start setting up a new data flow. Limitation flowctl discover can fully scaffold catalog captures. You can also use flowctl discoverto create stub configuration files for materialization connectors, but the remainder of the materialization must be written manually. Step 1: Generate a configuration file stub​ In your terminal, run: $ flowctl discover --image ghcr.io/estuary/${connector_name}:dev --prefix acmeCo/anvils Creating a connector configuration stub at /workspaces/flow/acmeCo/anvils/source-postgres.config.yaml. Edit and update this file, and then run this command again. Copy This command takes a connector Docker --imageand creates a configuration file stub, which by default is written to the --prefix subdirectory of your current directory — in this case ./acmeCo/anvils/source-postgres.config.yaml. tip A list of connector images can be found here. Step 2: Update your stubbed configuration file​ Open and edit the generated config file. It is pre-populated with configuration required by the connector, their default values, and descriptive comments: database: postgres # Logical database name to capture from. # [string] (required) host: &quot;&quot; # Host name of the database to connect to. # [string] (required) password: &quot;&quot; # User password configured within the database. # [string] (required) port: 5432 # Host port of the database to connect to. # [integer] (required) Copy Step 3: Discover resources from your endpoint​ Run the same command to use your configuration file to complete the discovery workflow. Flow creates (or overwrites) a catalog source file within your directory, which includes a capture definition with one or more bindings, definitions of collections to support each binding, and associated collection schemas: $ flowctl discover --image ghcr.io/estuary/${connector_name}:dev --prefix acmeCo/anvils Created a Flow catalog at /workspaces/flow/acmeCo/anvils/source-postgres.flow.yaml with discovered collections and capture bindings. Copy The generated ${connector_name}.flow.yaml is the source file for your capture. It will include a capture definition with one or more bindings, and the collection(s) created to contain documents from each bound endpoint resource. The capture and all collections are named using your chosen --prefix: File Listingsource-postgres.flow.yaml $ find acmeCo/ acmeCo/ acmeCo/anvils acmeCo/anvils/source-postgres.flow.yaml acmeCo/anvils/source-postgres.config.yaml acmeCo/anvils/my_table.schema.yaml acmeCo/anvils/my_other_table.schema.yaml Copy You can repeat this step any number of times, to re-generate and update your catalog sources so that they reflect the endpoint's current resources. Step 4: Inspect and trim your catalog​ If you notice an undesired resources from the endpoint was included in the catalog spec, you can remove its binding and corresponding collection to remove it from your catalog. "},{"title":"Editing with flowctl check​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#editing-with-flowctl-check","content":"You can directly write your capture or materialization in a catalog source file, and use flowctl check to provide a fast feedback loop to determine what configuration may be missing or incorrect. This is the current supported path for creating materializations. Typically, you will have already have a catalog spec with a capture and collections using discover. Now, you're simply adding a materialization to complete the dataflow. Find your materialization connectorand use the provided code sample as a template.Fill in the required values and other values, if desired.Add as many additional bindings as you need. As with captures, each collection in your catalog must have an individual binding to be connected to the endpoint system.Run flowctl check to verify that the connector can reach the endpoint system, and that all configuration is correct. tip Flow integrates with VS Code and other editors to offer auto-complete within catalog source files, which makes it easier to write and structure your files. "},{"title":"Config Manager​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#config-manager","content":"This method is for Beta clients using Flow as a managed service. The Estuary Config Manager acts and feels like a simple user interface. In practice, it's a secure way to collect the configurations details for your use case, so that Estuary engineers can create and start your dataflow. To use it, simply select your desired connector from the drop-down menu and fill out the required fields. "},{"title":"Flow UI​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#flow-ui","content":"Beta Flow UI is still undergoing development and will be available, with detailed documentation, in Q2 2022. The Flow user interface is an alternative to the GitOps workflow, but both provide the same results and can be used interchangeably to work with the same Flow catalog. In the UI, you select the connector you want to use and populate the fields that appear. "},{"title":"Configuration​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#configuration","content":"Connectors interface with external systems and universally require endpoint configuration, such as a database hostname or account credentials, which must be provided to the connector for it to function. When directly working with catalog source files, you have the option of inlining the configuration into your connector or storing it in separate files: InlineReferenced file my.flow.yaml materializations: acmeCo/postgres-views: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: host: localhost password: password database: postgres user: postgres port: 5432 bindings: [] Copy Storing configuration in separate files serves two important purposes: Re-use of configuration across multiple captures or materializationsThe ability to protect sensitive credentials "},{"title":"Protecting secrets​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#protecting-secrets","content":"Most endpoint systems require credentials of some kind, such as a username or password. Directly storing secrets in files that are versioned in Git is poor practice. Similarly, sensitive credentials should be protected while not in use within Flow's runtime as well. The only time a credential needs to be directly accessed is when it's required by Flow's runtime for the purposes of instantiating the connector. Flow integrates with Mozilla’s sops tool, which can encrypt and protect credentials within a GitOps-managed catalog. Flow's runtime similarly stores a sops-protected configuration in its encrypted form, and decrypts it only when invoking a connector on the user’s behalf. sops, short for “Secrets Operations,” is a tool that encrypts the values of a JSON or YAML document against a key management system (KMS) such as Google Cloud Platform KMS, Azure Key Vault, or Hashicorp Vault. Encryption or decryption of a credential with sops is an active process: it requires that the user (or the Flow runtime identity) have a current authorization to the required KMS, and creates a request trace which can be logged and audited. It's also possible to revoke access to the KMS, which immediately and permanently removes access to the protected credential. Example: Protect a configuration​ Suppose you're given a connector configuration: config.yaml host: my.hostname password: &quot;this is sensitive!&quot; user: my-user Copy You can protect it using a Google KMS key that you own: # Login to Google Cloud and initialize application default credentials used by `sops`. $ gcloud auth application-default login # Use `sops` to re-write the configuration document in place, protecting its values. $ sops --encrypt --in-place --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml Copy sops re-writes the file, wrapping each value in an encrypted envelope and adding a sops metadata section: config.yaml host: ENC[AES256_GCM,data:K/clly65pThTg2U=,iv:1bNmY8wjtjHFBcXLR1KFcsNMGVXRl5LGTdREUZIgcEU=,tag:5GKcguVPihXXDIM7HHuNnA==,type:str] password: ENC[AES256_GCM,data:IDDY+fl0/gAcsH+6tjRdww+G,iv:Ye8st7zJ9wsMRMs6BoAyWlaJeNc9qeNjkkjo6BPp/tE=,tag:EPS9Unkdg4eAFICGujlTfQ==,type:str] user: ENC[AES256_GCM,data:w+F7MMwQhw==,iv:amHhNCJWAJnJaGujZgjhzVzUZAeSchEpUpBau7RVeCg=,tag:62HguhnnSDqJdKdwYnj7mQ==,type:str] sops: # Some items omitted for brevity: gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T15:49:45Z&quot; enc: CiQAW8BC2GDYWrJTp3ikVGkTI2XaZc6F4p/d/PCBlczCz8BZiUISSQCnySJKIptagFkIl01uiBQp056c lastmodified: &quot;2022-01-05T15:49:45Z&quot; version: 3.7.1 Copy You then use this config.yaml within your Flow catalog. The Flow runtime knows that this document is protected by sopswill continue to store it in its protected form, and will attempt a decryption only when invoking a connector on your behalf. If you need to make further changes to your configuration, edit it using sops config.yaml. It's not required to provide the KMS key to use again, as sops finds it within its metadata section. important When deploying catalogs onto the managed Flow runtime, you must grant access to decrypt your GCP KMS key to the Flow runtime service agent, which is: flow-258@helpful-kingdom-273219.iam.gserviceaccount.com Copy Example: Protect portions of a configuration​ Endpoint configurations are typically a mix of sensitive and non-sensitive values. It can be cumbersome when sops protects an entire configuration document as you lose visibility into non-sensitive values, which you might prefer to store as cleartext for ease of use. You can use the encrypted-suffix feature of sops to selectively protect credentials: config.yaml host: my.hostname password_sops: &quot;this is sensitive!&quot; user: my-user Copy Notice that password in this configuration has an added _sops suffix. Next, encrypt only values which have that suffix: $ sops --encrypt --in-place --encrypted-suffix &quot;_sops&quot; --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml Copy sops re-writes the file, wrapping only values having a &quot;_sops&quot; suffix and adding its sops metadata section: config.yaml host: my.hostname password_sops: ENC[AES256_GCM,data:dlfidMrHfDxN//nWQTPCsjoG,iv:DHQ5dXhyOOSKI6ZIzcUM67R6DD/2MSE4LENRgOt6GPY=,tag:FNs2pTlzYlagvz7vP/YcIQ==,type:str] user: my-user sops: # Some items omitted for brevity: encrypted_suffix: _sops gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T16:06:36Z&quot; enc: CiQAW8BC2Au779CGdMFUjWPhNleCTAj9rL949sBvPQ6eyAC3EdESSQCnySJKD3eWX8XrtrgHqx327 lastmodified: &quot;2022-01-05T16:06:37Z&quot; version: 3.7.1 Copy You then use this config.yaml within your Flow catalog. Flow looks for and understands the encrypted_suffix, and will remove this suffix from configuration keys before passing them to the connector. "},{"title":"Connecting to endpoints on secure networks​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#connecting-to-endpoints-on-secure-networks","content":"In some cases, your source or destination endpoint may be within a secure network, and you may not be able to allow direct access to its port due to your organization's security policy. SHH tunneling, or port forwarding, provides a means for Flow to access the port indirectly through an SSH server. To set up and configure the SSH server, see the guide. Beta Currently, Flow supports SSH tunneling for all materialization connectors. Tunneling is only supported on certain capture connectors; consult the appropriate capture connector's documentation to check if it is supported. The SSH configurations for captures and materializations are somewhat different. Estuary plans to support SSH tunneling on all connectors in the future using the same configuration. After verifying that the connector is supported, you can add the configuration to the capture or materialization definition to enable SSH tunneling. Capture connectorsMaterialization connectors source-postgres-ssh-tunnel.flow.yaml captures: acmeCo/postgres-capture-ssh: endpoint: connector: image: ghcr.io/estuary/source-postgres:dev config: # When using a proxy like SSH tunneling, set to localhost host: localhost # Specify an open port on your local machine to connect to the proxy. port: 15432 database: flow user: flow_user password: secret proxy: proxyType: ssh_forwarding sshForwarding: # Port on the local machine from which you'll connect to the SSH server. # If a port is specified elsewhere in the connector configuration, it must match. localPort: 15432 # Port of the final endpoint to which you’ll connect via # tunneling from the SSH server. forwardPort: 5432 # Host or IP address of the final endpoint to which you’ll # connect via tunneling from the SSH server forwardHost: 127.0.0.1 # Location of the remote SSH server that supports tunneling. # Formatted as ssh://hostname[:port]. sshEndpoint: ssh://198.21.98.1 # Username to connect to the SSH server. user: sshUser # Private key to connect to the SSH server, formatted as multiline plaintext. # Use the YAML literal block style with the indentation indicator. # See https://yaml-multiline.info/ for details. privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: [] Copy "},{"title":"Available connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#available-connectors","content":"Learn about available connectors in the reference section "},{"title":"Flow tutorials","type":0,"sectionRef":"#","url":"getting-started/flow-tutorials/","content":"Flow tutorials info Flow is being developed rapidly and is in private beta, so our tutorials are currently limited. More will be available at the time of the GA release and the addition of Flow's UI. For more information, you can reach our team via email or sign up for our beta waitlist. Flow tutorials are designed for brand new users to get up and running quickly, while demonstrating important Flow concepts and capabilities. All you need to do first is set up your development environment. We assume you have some engineering background, but anyone with basic technical skills should be able to follow along.","keywords":""},{"title":"Tests","type":0,"sectionRef":"#","url":"concepts/tests/","content":"","keywords":""},{"title":"Ingest​","type":1,"pageTitle":"Tests","url":"concepts/tests/#ingest","content":"ingest steps add documents to a named collection. All documents must validate against the collection'sschema, or a catalog build error will be reported. All documents from a single ingest step are added in one transaction. This means that multiple documents with a common key will be combined priorto their being appended to the collection. Suppose acmeCo/people had key [/id]: tests: acmeCo/tests/greetings: - ingest: description: Zeldas are combined to one added document. collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda One&quot; } - { userId: 1, name: &quot;Zelda Two&quot; } - verify: description: Only one Zelda is greeted. collection: acmeCo/greetings documents: - { userId: 1, greeting: &quot;Hello Zelda Two&quot; } Copy "},{"title":"Verify​","type":1,"pageTitle":"Tests","url":"concepts/tests/#verify","content":"verify steps assert that the current contents of a collection match the provided document fixtures. Verified documents are fully reduced, with one document for each unique key, ordered under the key's natural order. You can verify the contents of both derivations and captured collections. Documents given in verify steps do not need to be comprehensive. It is not an error if the actual document has additional locations not present in the document to verify, so long as all matched document locations are equal. Verified documents also do not need to validate against the collection's schema. They do, however, need to include all fields that are part of the collection's key. tests: acmeCo/tests/greetings: - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda&quot; } - { userId: 2, name: &quot;Link&quot; } - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda Again&quot; } - { userId: 3, name: &quot;Pikachu&quot; } - verify: collection: acmeCo/greetings documents: # greetings are keyed on /userId, and the second greeting is kept. - { userId: 1, greeting: &quot;Hello Zelda Again&quot; } # `greeting` is &quot;Hello Link&quot;, but is not asserted here. - { userId: 2 } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } Copy "},{"title":"Partition selectors​","type":1,"pageTitle":"Tests","url":"concepts/tests/#partition-selectors","content":"Verify steps may include a partition selector to verify only documents of a specific partition: tests: acmeCo/tests/greetings: - verify: collection: acmeCo/greetings description: Verify only documents which greet Nintendo characters. documents: - { userId: 1, greeting: &quot;Hello Zelda&quot; } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } partitions: include: platform: [Nintendo] Copy Learn more about partition selectors. "},{"title":"Tips​","type":1,"pageTitle":"Tests","url":"concepts/tests/#tips","content":"The following tips can aid in testing large or complex derivations. "},{"title":"Testing reductions​","type":1,"pageTitle":"Tests","url":"concepts/tests/#testing-reductions","content":"Reduction annotations are expressive and powerful, and their use should thus be tested thoroughly. An easy way to test reduction annotations on captured collections is to write a two-step test that ingests multiple documents with the same key and then verifies the result. For example, the following test might be used to verify the behavior of a simple sum reduction: tests: acmeCo/tests/sum-reductions: - ingest: description: Ingest documents to be summed. collection: acmeCo/collection documents: - {id: 1, value: 5} - {id: 1, value: 4} - {id: 1, value: -3} - verify: description: Verify value was correctly summed. collection: acmeCo/collection documents: - {id: 1, value: 6} Copy "},{"title":"Reusing common fixtures​","type":1,"pageTitle":"Tests","url":"concepts/tests/#reusing-common-fixtures","content":"When you write a lot of tests, it can be tedious to repeat documents that are used multiple times. YAML supports anchors and references, which you can implement to re-use common documents throughout your tests. One nice pattern is to define anchors for common ingest steps in the first test, which can be re-used by subsequent tests. For example: tests: acmeCo/tests/one: - ingest: &amp;mySetup collection: acmeCo/collection documents: - {id: 1, ...} - {id: 2, ...} ... - verify: ... acmeCo/tests/two: - ingest: *mySetup - verify: ... Copy This allows all the subsequent tests to re-use the documents from the first ingest step without having to duplicate them. "},{"title":"Setting up a development environment","type":0,"sectionRef":"#","url":"getting-started/installation/","content":"","keywords":""},{"title":"Using GitHub Codespaces​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#using-github-codespaces","content":"GitHub codespaces provides VM-backed, portable development environments that are ideal for getting started with Flow in minutes. Currently, Codespaces is available to GitHub Teams and Enterprise customers, as well as individuals enrolled in the beta. If you have access, this is the preferred method — setting up a devcontainer in Codespaces is much quicker than doing so locally. Visit the Flow Template repository, click Code, and choose New Codespace. The VM spins up within a minute or two, and you can immediately begin developing and testing. The template includes a PostgreSQL database for this purpose. "},{"title":"Using Visual Studio Code locally​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#using-visual-studio-code-locally","content":"If you don't have access to Codespaces, or prefer local development, use this method to create a local environment. Download and install the following prerequisites: DockerVS CodeVS Code Remote Containers extension Create a Git repository from the Flow Template ​ Visit the Flow Template repository on GitHub, click on Use this template, and proceed to create your repository. Open in VS Code ​ Clone your repository locally and open it in VS Code. You'll see a popup in the lower right corner asking if you'd like to re-open the repository in a container. Click Re-open in container. It may take several minutes to download components and build the container. "},{"title":"Test your environment​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#test-your-environment","content":"Regardless of the method you used, first test everything is working as expected. The repository contains a sample project, which includes a test. (It also serves as a quick tutorial, which we recommend as a next step). In a terminal window, run: flowctl test --source word-counts.flow.yaml Copy Verify that it returns: Ran 1 tests, 1 passed, 0 failed Copy You're now ready to start using Flow! Proceed to the Flow introductory tutorial. "},{"title":"Configure connections with SSH tunneling","type":0,"sectionRef":"#","url":"guides/connect-network/","content":"","keywords":""},{"title":"General setup​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#general-setup","content":"Activate an SSH implementation on a server, if you don't have one already. Consult the documentation for your server's operating system and/or cloud service provider, as the steps will vary. Configure the server to your organization's standards, or reference the SSH documentation for basic configuration options. Referencing the config files and shell output, collect the following information: The SSH endpoint for the SSH server, formatted as ssh://hostname[:port]. This may look like the any of following: ssh://ec2-198-21-98-1.compute-1.amazonaws.comssh://198.21.98.1ssh://198.21.98.1:22 The SSH user, which will be used to log into the SSH server, for example, sshuser. You may choose to create a new user for this workflow. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa Copy If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Copy Taken together, these configuration details would allow you to log into the SSH server from your local machine. They'll allow the connector to do the same. Configure your internal network to allow the SSH server to access your capture or materialization endpoint. Note the internal host and port; these are necessary to open the connection. Configure your network to expose the SSH server endpoint to eternal traffic. The method you use depends on your organization's IT policies. Currently, Estuary doesn't provide a list of static IPs for whitelisting purposes, but if you require one, contact Estuary support. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for AWS​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-aws","content":"To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa Copy If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Copy Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Amazon RDS, you can find these in the RDS console as Endpoint and Port. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for Google Cloud​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-google-cloud","content":"To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM). Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa Copy If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Copy If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Copy Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Google Cloud SQL, you can find the host in the Cloud Console as Public IP Address. Use 5432 as the port. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for Azure​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-azure","content":"To allow SSH tunneling to a database instance hosted on Azure, you'll need to create a virtual machine (VM) in the same virtual network as your endpoint database. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa Copy If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Copy Create and connect to a VM in a virtual network, and add the endpoint database to the network. Create a new virtual network and subnet. Create a Linux or Windows VM within the virtual network, directing the SSH public key source to the public key you generated previously. Note the VM's public IP; you'll need this later. Create a service endpoint for your database in the same virtual network as your VM. Instructions for Azure Database For PostgreSQL can be found here; note that instructions for other database engines may be different. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Azure, you can find the host as Server Name, and the port under Connection Strings (usually 5432). Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Configuration​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#configuration","content":"After you've completed the prerequisites, you should have the following parameters: sshEndpoint: the SSH server's hostname, or public IP address, formatted as ssh://hostname[:port]privateKey: the contents of the PEM fileuser: the username used to connect to the SSH server.forwardHost: the capture or materialization endpoint's hostforwardPort: the capture or materialization endpoint's portlocalPort: the port on the localhost used to connect to the SSH server Use these to add SSH tunneling to your capture or materialization definition, either by filling in the corresponding fields in a web app, or by working with the YAML directly. Reference the Connectors page for a YAML sample. Proxies like SSH are always run on an open port on your localhost, so you'll need to re-configure other fields in your capture or materialization definition. Set the connector's host property to localhost. If the connector has a port property, set it to the same value as localPort in the SSH configuration. "},{"title":"Create a simple data flow","type":0,"sectionRef":"#","url":"guides/create-dataflow/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#prerequisites","content":"This guide assumes a basic understanding of Flow and its key concepts. Before you begin, it's recommended that you read the high level concepts documentation. "},{"title":"Introduction​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#introduction","content":"The simplest Flow catalog comprises three types of entities that define your data flow: a data capture from an external source, one or more collections, which store that data in the Flow runtime, and a materialization, to push them to an external destination. In the majority of cases, the capture and materialization each rely on a plug-in connector. Here, we'll walk through how to leverage various connectors, configure them in your catalog specification, and run the catalog in a temporary data plane. "},{"title":"Steps​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#steps","content":"Set up a development environment. We recommend a VM-backed environment using GitHub Codespaces, if you have access. Otherwise, you can set up a local environment. Follow the setup requirements here. Next, you'll create your catalog spec. Rather than starting from scratch, you'll use the guided flowctl discover process to generate one that is pre-configured for the capture connector you're using. tip You may notice the template you cloned in step 1 comes with a catalog spec. It's an example, so you can disregard it unless you choose to run the tutorial. Refer to the capture connectors list and find your data source system. Click on its configuration link, set up prerequisites as necessary, and follow the instructions to generate a catalog spec with flowctl discover. A generalized version of the discover workflow is as follows: In your terminal, run: flowctl discover --image=ghcr.io/estuary/&lt;connector-name&gt;:dev --prefix your/subdirectoryIn subdirectory you specified, find the generated file called discover-source-&lt;connector-name&gt;-config.yaml and fill in the required values.Re-run the command. A catalog spec called discover-source-&lt;connector-name&gt;.flow.yaml is generated. You now have a catalog spec that contains a capture and one or more collections. In a production workflow, your collections would be stored in a cloud storage bucket. In the development workflow, cloud storage isn't used, but you must supply a placeholder storage mapping. Copy and paste the following placeholder section at the top of your catalog spec. storageMappings: &quot;&quot;: stores: - bucket: &quot;my-bucket&quot; provider: &quot;S3&quot; Copy To complete your end-to-end dataflow, you'll now add a materialization. Like your capture, materializations are configured differently depending on the connector and endpoint system; however, they are configured manually. Go to the materialization connectors list. Find your destination system, open its configuration page, and follow the sample to configure your materialization. Launch the system locally: flowctl temp-data-plane Copy Copy the two lines of output that start with export BROKER_ADDRESS and export CONSUMER_ADDRESS for use in the next step. Leave that process running and open a new shell window. There, deploy your catalog, substituting your file location and broker and consumer addresses: export BROKER_ADDRESS=http://localhost:8080 export CONSUMER_ADDRESS=http://localhost:9000 flowctl deploy --source=your_file.flow.yaml --wait-and-cleanup Copy You'll now be able to see data flowing between your source and destination systems. When you're done, press Ctrl-C to exit and clean up. "},{"title":"What's next?​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#whats-next","content":"With Flow, you can build a wide range of scalable real-time data integrations, with optional transformations. You can add multiple captures and materializations to the same catalog spec. Check back regularly; we frequently add new connectors.You can add derivations.Current beta customers can work with the Estuary team to set up production-level pipelines. "},{"title":"Your first data flow","type":0,"sectionRef":"#","url":"getting-started/flow-tutorials/hello-flow/","content":"","keywords":""},{"title":"Word count in a continuous PostgreSQL materialized view​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#word-count-in-a-continuous-postgresql-materialized-view","content":"PostgreSQL is a great open-source database that supports materialized views, but it doesn't offer continuous materialized views. In this tutorial, you'll build one with Flow. How many times have you managed text documents in PostgreSQL and thought to yourself: &quot;Gee-whiz, self, I wish I had a table of word counts that was always up-to-date!&quot; ... basically never, right? Well, it's a simple way to get familiar with a powerful concept, so let's do it anyway! Our data flow will: Capture data from a documents table in the PostgreSQL database.Use a derivation to compute word and document frequency updates.Materialize the results back into the table in a a word_counts table. These three processes — captures, derivations, and materializations — comprise the possible tasks in any data flow. They're configured in YAML files known as a catalog specification. For this example, they've been configured for you. If you'd like, you can check out flow.yaml and word-counts.flow.yaml to get oriented. "},{"title":"Set up​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#set-up","content":"These instructions assume you've set up a development environment either using Codespaces or on your local machine. Go back and do that first, if necessaary. "},{"title":"Verify tests​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#verify-tests","content":"All cutting-edge word count projects should have tests. Let's make sure the words are, um, counted. Run the following: $ flowctl test --source word-counts.flow.yaml Copy Wait until you see: Running 1 tests... ✔️ word-counts.flow.yaml :: acmeCo/tests/word-counts-from-documents Ran 1 tests, 1 passed, 0 failed Copy Your test performed as expected; now you can deploy the catalog. "},{"title":"Run it locally​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#run-it-locally","content":"Start a local, temporary Flow data plane: $ flowctl temp-data-plane Copy A data plane is a long-lived, multi-tenant, scale-out component that usually runs in a data center. Fortunately it also shrinks down to your laptop. This returns a couple exported addresses. Copy these; you'll need them in a moment: export BROKER_ADDRESS=http://localhost:8080 export CONSUMER_ADDRESS=http://localhost:9000 Copy Now, deploy the catalog to your data plane by running: $ export BROKER_ADDRESS=http://localhost:8080 $ export CONSUMER_ADDRESS=http://localhost:9000 $ flowctl deploy --wait-and-cleanup --source flow.yaml Copy After a moment, you'll see: Deployment done. Waiting for Ctrl-C to clean up and exit. Copy Flow is now watching the documents table, and materializing to word_counts. Start a new terminal window to begin working with the database. $ psql --host localhost psql (13.5 (Debian 13.5-0+deb11u1), server 13.2 (Debian 13.2-1.pgdg100+1)) Type &quot;help&quot; for help. Copy The documents table is still empty, so you'll populate it with a few phrases: flow=# insert into documents (body) values ('The cat in the hat.'), ('hat Cat CAT!'); INSERT 0 2 Copy Now, you'll take a look at word_counts to see the results: flow=# select word, count, doc_count from word_counts; word | count | doc_count ------+-------+----------- cat | 3 | 2 hat | 2 | 2 in | 1 | 1 the | 2 | 1 (4 rows) Copy Say you made a typo in that second value. You can immediately update it: flow=# update documents set body = 'cat Cat CAT!' where id = 2; UPDATE 1 flow=# select word, count, doc_count from word_counts order by word; word | count | doc_count ------+-------+----------- cat | 4 | 2 hat | 1 | 1 in | 1 | 1 the | 2 | 1 (4 rows) Copy Now, let's clean up the table: flow=# delete from documents ; DELETE 2 flow=# select word, count, doc_count from word_counts order by word; word | count | doc_count ------+-------+----------- cat | 0 | 0 hat | 0 | 0 in | 0 | 0 the | 0 | 0 (4 rows) Copy The updates you push to documents are materialized to word_counts with millisecond latency. In effect, you've added a new, powerful capability to PostgreSQL that has a multitude of real-world and business applications (far beyond counting cats and hats). When you're done with your testing, exit the data flow by returning to your first console window and pressing Ctrl-C. "},{"title":"Connectors","type":0,"sectionRef":"#","url":"reference/Connectors/","content":"Connectors A current list and configuration details for Estuary's connectors can be found on the following pages: Capture connectorsMaterialization connectors Many additional connectors are available from Airbyte. They function similarly but are limited to batch workflows, which Flow will run at a regular and configurable cadence. You can learn more about how connectors work and how to use them in their conceptual documentation.","keywords":""},{"title":"Who should use Flow?","type":0,"sectionRef":"#","url":"overview/who-should-use-flow/","content":"","keywords":""},{"title":"Benefits​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#benefits","content":"These characteristics set Flow apart from other data integration workflows and address the pain points listed above. "},{"title":"Fully integrated pipelines​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#fully-integrated-pipelines","content":"With Flow, you can build, test, and evolve pipelines that continuously capture, transform, and materialize data across all of your systems. With one tool, you can power workflows that have historically required you to first piece together services, then integrate and operate them in-house to meet your needs. To achieve comparable capabilities to Flow you would need: A low-latency streaming system, such as AWS KenesisData lake build-out, such as Kenesis Firehose to S3Custom ETL application development, such as Spark, Flink, or AWS λSupplemental data stores for intermediate transformation statesETL job management and execution, such as a self-hosting or Google Cloud DataflowCustom reconciliation of historical vs streaming datasets, including onerous backfills of new streaming applications from historical data Flow's declarative GitOps workflow is a dramatic simplification from this inherent complexity. It saves you time and costs, catches mistakes before they hit production, and keeps your data fresh across all the places you use it. "},{"title":"Efficient architecture​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#efficient-architecture","content":"Flow mixes a variety of architectural techniques to deliver great throughput, avoid latency, and minimize operating costs. These include: Leveraging reductions to reduce the amount of data that must be ingested, stored, and processed, often dramaticallyExecuting transformations predominantly in-memoryOptimistic pipelining and vectorization of internal remote procedure calls (RPCs) and operationsA cloud-native design that optimizes for public cloud pricing models Flow also makes it easy to materialize focused data rollups as views directly into your warehouse, so you don't need to repeatedly query the much larger source datasets. This can dramatically lower warehouse costs. "},{"title":"Powerful transformations​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#powerful-transformations","content":"With Flow, you can build pipelines that join a current event with an event that happened days, weeks, even years in the past. Flow can model arbitrary stream-to-stream joins without the windowing constraints imposed by other systems, which limit how far back in time you can join. Flow transforms data in durable micro-transactions, meaning that an outcome, once committed, won't be silently re-ordered or changed due to a crash or machine failure. This makes Flow uniquely suited for operational workflows, like assigning a dynamic amount of available inventory to a stream of requests — decisions that, once made, should not be forgotten. You can also evolve transformations as business requirements change, enriching them with new datasets or behaviors without needing to re-compute from scratch. "},{"title":"Data integrity​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#data-integrity","content":"Flow supports strong schematization, durable transactions with exactly-once semantics, and easy end-to-end testing to ensure that your data is accurate and that changes don't break pipelines. JSON schemas are verified with every document read or written. If a document violates its schema, Flow pauses the pipeline, giving you a chance to fix the error.Schemas can encode constraints, like that a latitude value must be between +90 and -90 degrees, or that a field must be a valid email address.Flow projects JSON schema into other flavors, like TypeScript types or SQL tables. Strong type checking catches bugs before they're applied to production.Flow's declarative tests verify the integrated, end-to-end behavior of processing pipelines. "},{"title":"Dynamic scaling​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#dynamic-scaling","content":"The Flow runtime scales from a single process for local development up to a large Kubernetes cluster for high-volume production deployments. Processing tasks are quickly reassigned upon any machine failure for high availability. Each process can also be scaled independently, at any time, and without downtime. This is unique to Flow. Comparable systems require that an arbitrary data partitioning be decided upfront, a crucial performance knob that's awkward and expensive to change. Instead, Flow can repeatedly split a running task into two new tasks, each half the size, without stopping it or impacting its downstream uses. "},{"title":"Comparisons","type":0,"sectionRef":"#","url":"overview/comparisons/","content":"","keywords":""},{"title":"Apache Beam and Google Cloud Dataflow​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#apache-beam-and-google-cloud-dataflow","content":"Flow’s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there’s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences. Like Beam, Flow’s primary primitive is a collection. You build a processing graph (called a pipeline in Beam and a catalog in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow’s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas. Unlike Beam, Flow doesn’t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types. Also, while Beam allows you the option to define combine operators, Flow’s runtime always applies combine operators. These are built using the declared semantics of the document’s schema, which makes it much more efficient and cost-effective to work with streaming data. Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow’s modeling of state – via its per-key register concept – is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer. "},{"title":"Kafka​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#kafka","content":"Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem. Flow is built on Gazette, a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette’s consumer framework, which has similarities to Kafka consumers. Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics. Journals in Gazette and Flow are roughly analogous to Kafka partitions. Each journal is a single append-only log. Gazette has no native notion of a topic, but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called fragments, which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently. Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called collections. Flow collections are somewhat similar to Kafka streams, but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning. Flow tasks are most similar to Kafka stream processors, but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls shards, to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling. "},{"title":"Spark​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#spark","content":"Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems. You can think of a Flow collection as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can materialize to push the data into some external system like cloud storage or a database. You can also create a collection that is derived by applying stateful transformations to one or more source collections. Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data. Spark's processing primitives, applications, jobs, and tasks, don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the catalog. A Flow catalog is a composition of Flow tasks, which are quite different from tasks in Spark. In Flow, a task is a logical unit of work that does one of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow shard. In Flow, a task is a logical unit of work, and shards represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling. Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations. "},{"title":"Hadoop, HDFS, and Hive​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#hadoop-hdfs-and-hive","content":"There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow. To gain an understanding of Flow's processing model for derivations, see this blog post about MapReduce in Flow. HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive. For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing collections, rather than creating a new one each time a job is run. In fact, Flow has no notion of a job like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. This blog post explores those differences in greater depth. To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries. In Flow, you instead define a capture of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow derivations, which again apply the transformations incrementally and in real time. While you could actually use tools like Hive to directly query data from Flow collections — the layout of collection data in cloud storage is intentionally compatible with this — you could also materialize a view of your transformation results to any database, which is also kept up to date in real time. "},{"title":"Fivetran, Airbyte, and other ELT solutions​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#fivetran-airbyte-and-other-elt-solutions","content":"Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the worlds in terms of independent scheduled jobs that copy data from source to destination, Flow catalogs model a directed graph ofcaptures (reads from sources),derivations (transforms), andmaterializations (writes to destinations). Collectively, these are called tasks. Tasks in Flow are only indirectly linked. Captures read data from a source and output to collections. Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing*. Like Airbyte, Flow uses connectors for interacting with external systems in captures and materializations. For captures, Flow integrates the Airbyte specification, so all Airbyte source connectors can be used with Flow. For materializations, Flow uses its own protocol which is not compatible with the Airbyte spec. In either case the usage of connectors is pretty similar. In terms of technical capabilities, Flow can do everything that these tools can and more, but those tools are more mature while Flow is currently in private beta. Both Fivetran and Airbyte both currently have graphical interfaces that make them much easier for non-technical users to configure, while Flow's UI is under development. Currently, Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow. * Some materialization endpoints can only make at-least-once guarantees. "},{"title":"dbt​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#dbt","content":"dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively. In addition to – and perhaps more important than – its transform capability, dbt brought an entirely new workflow for working with data: one that prioritizes version control, testing, local development, documentation, composition, and re-use. Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse. Flow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else. These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse. Within the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to productionize important insights as materialized views or by pushing to another destination. Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in a separate, dedicated tool like dbt. While Flow and dbt don’t interact directly, both offer easy integration through your data warehouse. "},{"title":"Materialize, Rockset, ksqlDB, and other real-time databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#materialize-rockset-ksqldb-and-other-real-time-databases","content":"Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers, and can keep SQL views up to date in real time. These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not. However, this doesn't mean that Flow should always replace these systems in your stack. In fact, it can be optimal to use Flow to feed data into them. Doing so allows you to transactionally capture data from a variety of sources and transform it along the way. For further explanation, read the section below on OLAP databases. "},{"title":"Snowflake, BigQuery, and other OLAP databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#snowflake-bigquery-and-other-olap-databases","content":"Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by materializing views into it. Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow catalogs define collections. These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key. Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run. Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date. A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow’s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant. "},{"title":"Capture connectors","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/","content":"","keywords":""},{"title":"Available capture connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#available-capture-connectors","content":"Amazon Kinesis ConfigurationPackage — ghcr.io/estuary/source-kinesis:dev Amazon S3 ConfigurationPackage — ghcr.io/estuary/source-s3:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/source-kafka:dev Google Cloud Storage ConfigurationPackage — ghcr.io/estuary/source-gcs:dev MySQL ConfigurationPackage - ghcr.io/estuary/source-mysql:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/source-postgres:dev "},{"title":"Amazon Kinesis","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-kinesis/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#prerequisites","content":"You'll need one or more Amazon Kinesis streams. For a given capture, all streams must: Contain JSON data onlyBe accessible from a single root user or IAM user in AWSBe in the same AWS region You'll also need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. Beta Your root or IAM user in AWS must have appropriate permissions. Additional details will be added to this article soon. In the meantime, you can contact Estuary Support if you encounter unexpected behavior. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample below provide configuration details specific to the Amazon Kinesis source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-secret-access-key&quot; /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a kinesis-compatible API that isn't provided by AWS.\tstring /region\tAWS region\tThe name of the AWS region where the Kinesis stream is located.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tStream name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#sample","content":"A minimal capture definition will look like the following: captures: ${TENANT}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kinesis:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: ${STREAM_NAME} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition will likely be more complex, with additional bindings for each Kinesis stream. Learn more about capture definitions.. "},{"title":"Amazon S3","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-s3/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#prerequisites","content":"To use this connector, either your S3 bucket must be public, or you must have access via a root or IAM user. For public buckets, verify that the access policy allows anonymous reads.For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample in this section provide configuration details specific to the Amazon S3 source connector. tip You might organize your S3 bucket using prefixes to emulate a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring\t&quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring\t&quot;example-aws-secret-access-key&quot; /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a S3-compatible API that isn't provided by AWS.\tstring /matchKeys\tMatch keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#sample","content":"captures: ${TENANT}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-s3:dev config: bucket: &quot;my-bucket&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition may be more complex, with additional bindings for different S3 prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Derivations","type":0,"sectionRef":"#","url":"concepts/derivations/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#specification","content":"A derivation is specified as a regular collection with an additional derivation stanza: collections: # The unique name of the derivation. acmeCo/my/derivation: schema: my-schema.yaml key: [/key] # Presence of a `derivation` stanza makes this collection a derivation. # Type: object derivation: # Register definition of the derivation. # If not provided, registers have an unconstrained schema # and initialize to the `null` value. # Optional, type: object register: # JSON Schema of register documents. As with collection schemas, # this is either an inline definition or a relative URL reference. # Required, type: string (relative URL form) or object (inline form) schema: type: integer # Initial value taken by a register which has never been updated before. # Optional, default: null initial: 0 # Transformations of the derivation, # specified as a map of named transformations. transform: # Unique name of the transformation, containing only Unicode # Letters and Numbers (no spaces or punctuation). myTransformName: # Source collection read by this transformation. # Required, type: object source: # Name of the collection to be read. # Required. name: acmeCo/my/source/collection # JSON Schema to validate against the source collection. # If not set, the schema of the source collection is used. # Optional, type: string (relative URL form) or object (inline form) schema: {} # Partition selector of the source collection. # Optional. Default is to read all partitions. partitions: {} # Delay applied to sourced documents before being processed # by this transformation. # Default: No delay, pattern: ^\\\\d+(s|m|h)$ readDelay: &quot;48h&quot; # Shuffle determines the key by which source documents are # shuffled (mapped) to a register. # Optional, type: object. # If not provided, documents are shuffled on the source collection key. shuffle: # Key is a composite key which is extracted from documents # of the source. key: [/shuffle/key/one, /shuffle/key/two] # Update lambda of the transformation. # Optional, type: object update: {lambda: typescript} # Publish lambda of the transformation. # Optional, type: object publish: {lambda: typescript} # Priority applied to processing documents of this transformation # relative to other transformations of the derivation. # Default: 0, integer &gt;= 0 priority: 0 Copy "},{"title":"Background​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#background","content":"The following sections will refer to the following common example to illustrate concepts. Suppose you have an application through which users send one another some amount of currency, like in-game tokens or dollars or digital kittens: transfers.flow.yamltransfers.schema.yaml collections: # Collection of 💲 transfers between accounts: # {id: 123, sender: alice, recipient: bob, amount: 32.50} acmeBank/transfers: schema: transfers.schema.yaml key: [/id] Copy There are many views over this data that you might require, such as summaries of sender or receiver activity, or current account balances within your application. "},{"title":"Transformations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#transformations","content":"A transformation binds a source collection to a derivation. As documents of the source collection arrive, the transformation processes the document to publish new documents,update aregister, or both. Read source documents are shuffled on a shuffle key to co-locate the processing of documents that have equal shuffle keys. The transformation then processes documents by invoking lambdas: user-defined functions that accept documents as arguments and return documents in response. A derivation may have many transformations, and each transformation has a long-lived and stable name. Each transformation independently reads documents from its source collection and tracks its own read progress. More than one transformation can read from the same source collection, and transformations may also source from their own derivation, enabling cyclic data-flows and graph algorithms. Transformations may be added to or removed from a derivation at any time. This makes it possible to, for example, add a new collection into an existing multi-way join, or gracefully migrate to a new source collection without incurring downtime. However, renaming a running transformation is not possible. If attempted, the old transformation is dropped and a new transformation under the new name is created, which begins reading its source collection all over again. graph LR; d[Derivation]; t[Transformation]; r[Registers]; p[Publish λ]; u[Update λ]; c[Sourced Collection]; d-- has many --&gt;t; t-- reads from --&gt;c; t-- invokes --&gt;u; t-- invokes --&gt;p; u-- updates --&gt;r; r-- reads --&gt;p; d-- indexes --&gt;r; "},{"title":"Sources​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#sources","content":"The source of a transformation is a collection. As documents are published into the source collection, they are continuously read and processed by the transformation. A partition selector may be provided to process only a subset of the source collection's logical partitions. Selectors are efficient: only partitions that match the selector are read, and Flow can cheaply skip over partitions that don't. Derivations re-validate their source documents against the source collection's schema as they are read. This is because collection schemas may evolve over time, and could have inadvertently become incompatible with historical documents of the source collection. Upon a schema error, the derivation will pause and give you an opportunity to correct the problem. You may also provide an alternative source schema. Source schemas aide in processing third-party sources of data that you don't control, which can have unexpected schema changes without notice. You may want to capture this data with a minimal and very permissive schema. Then, a derivation can apply a significantly stricter source schema, which verifies your current expectations of what the data should be. If those expectations turn out to be wrong, little harm is done: your derivation is paused but the capture continues to run. You must simply update your transformations to account for the upstream changes and then continue without any data loss. "},{"title":"Shuffles​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#shuffles","content":"As each source document is read, it's shuffled — or equivalently, mapped — on an extracted key. If you're familiar with data shuffles in tools like MapReduce, Apache Spark, or Flink, the concept is very similar. Flow catalog tasks scale to run across many machines at the same time, where each machine processes a subset of source documents. Shuffles let Flow know how to group documents so that they're co-located, which can increase processing efficiency and reduce data volumes. They are also used to map source documents to registers. graph LR; subgraph s1 [Source Partitions] p1&gt;acmeBank/transfers/part-1]; p2&gt;acmeBank/transfers/part-2]; end subgraph s2 [Derivation Task Shards] t1([task/shard-1]); t2([task/shard-2]); end p1-- sender: alice --&gt;t1; p1-- sender: bob --&gt;t2; p2-- sender: alice --&gt;t1; p2-- sender: bob --&gt;t2; If you don't provide a shuffle key, Flow will shuffle on the source collection key, which is typically what you want. If a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string or integer) as well as the number of components in a composite key. For example, one transformation couldn't shuffle transfers on [/id]while another shuffles on [/sender], because sender is a string andid an integer. Similarly mixing a shuffle of [/sender] alongside [/sender, /recipient]is prohibited because the keys have different numbers of components. However, one transformation can shuffle on [/sender]while another shuffles on [/recipient], as in the examples below. "},{"title":"Publish lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#publish-lambdas","content":"A publish lambda publishes documents into the derived collection. To illustrate first with an example, suppose you must know the last transfer from each sender that was over $100: last-large-send.flow.yamllast-large-send.flow.tslast-large-send-test.flow.yaml import: - transfers.flow.yaml collections: acmeBank/last-large-send: schema: transfers.schema.yaml key: [/sender] derivation: transform: fromTransfers: source: name: acmeBank/transfers publish: lambda: typescript Copy This transformation defines a TypeScript publish lambda, which is implemented in an accompanying TypeScript module. The lambda is invoked as each source transfer document arrives. It is given the source document, and also includes the a _register and _previous register, which are not used here.Registers are discussed in depth below. The lambda outputs zero or more documents, each of which must conform to the derivation's schema. As this derivation's collection is keyed on /sender, the last published document (the last large transfer) of each sender is retained. If it were instead keyed on /id, then all transfers with large amounts would be retained. In SQL terms, the collection key acts as a GROUP BY. tip Flow will initialize a TypeScript module for your lambdas if one doesn't exist, with stubs of the required interfaces and TypeScript types that match your schemas. You just write the function body. Learn more about TypeScript generation  Derivation collection schemas may havereduction annotations, and publish lambdas can be combined with reductions in interesting ways. You may be familiar with map and reduce functions built into languages likePython,JavaScript; and many others, or have used tools like MapReduce or Spark. In functional terms, lambdas you write within Flow are &quot;mappers,&quot; and reductions are always done by the Flow runtime using your schema annotations. Suppose you need to know the runningaccount balancesof your users given all of their transfers thus far. Tackle this by reducing the final account balance for each user from all of the credit and debit amounts of their transfers: balances.flow.yamlbalances.flow.tsbalances-test.flow.yaml import: - transfers.flow.yaml collections: acmeBank/balances: schema: type: object required: [user] reduce: { strategy: merge } properties: user: { type: string } balance: type: number reduce: { strategy: sum } key: [/user] derivation: transform: fromTransfers: source: name: acmeBank/transfers publish: lambda: typescript Copy "},{"title":"Registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#registers","content":"Registers are the internal memory of a derivation. They are a building block that enable derivations to tackle advanced stateful streaming computations like multi-way joins, windowing, and transaction processing. As we've already seen, not all derivations require registers, but they are essential for a variety of important use cases. Each register is a document with a user-definedschema. Registers are keyed, and every derivation maintains an index of keys and their corresponding register documents. Every source document is mapped to a specific register document through its extracted shuffle key. For example, when shuffling acmeBank/transfers on [/sender], each account (&quot;alice&quot;, &quot;bob&quot;, or &quot;carol&quot;) is allocated its own register. If you instead shuffle on [/sender, /recipient] then eachpair of accounts (&quot;alice -&gt; bob&quot;, &quot;alice -&gt; carol&quot;, &quot;bob -&gt; carol&quot;) is allocated a register. Registers are best suited for relatively small, fast-changing documents that are shared within and across the transformations of a derivation. The number of registers indexed within a derivation may be very large, and if a register has never before been used, it starts with a user-defined initial value. From there, registers may be modified through an update lambda. info Under the hood, registers are backed by replicated, embedded RocksDB instances, which co-locate with the lambda execution contexts that Flow manages. As contexts are assigned and re-assigned, their register databases travel with them. If any single RocksDB instance becomes too large, Flow is able to perform an online split, which subdivides its contents into two new databases — and paired execution contexts — which are re-assigned to other machines. "},{"title":"Update lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#update-lambdas","content":"An update lambda transforms a source document into an update of the source document's register. To again illustrate through an example, suppose your compliance department wants you to flag the first transfer a sender sends to a new recipient. You achieve this by shuffling on pairs of[/sender, /recipient] and using a register to track whether this account pair has been seen before: first-send.flow.yamlfirst-send.flow.tsfirst-send-test.flow.yaml import: - transfers.flow.yaml collections: acmeBank/first-send: schema: transfers.schema.yaml key: [/id] derivation: # We'll store a `true/false` boolean in our register documents, # which is initially `false` and becomes `true` after the first transfer. register: schema: { type: boolean } initial: false transform: fromTransfers: source: name: acmeBank/transfers # Shuffle so that each account pair # is allocated its own register. shuffle: key: [/sender, /recipient] update: lambda: typescript publish: lambda: typescript Copy This transformation uses both a publish and an update lambda, implemented in an accompanying TypeScript module. The update lambda is invoked first for each source document, and it returns zero or more documents, which each must conform to the derivation's register schema (in this case, a simple boolean). The publish lambda is invoked next, and is given the sourcedocument as well as the before (previous) and after (_register) values of the updated register. In this case, we don't need the after value: our update lambda implementation implies that it's always true. The before value, however, tells us whether this was the very first update of this register, and by implication was the first transfer for this pair of accounts. sequenceDiagram autonumber Derivation-&gt;&gt;Update λ: update({sender: alice, recipient: bob})? Update λ--&gt;&gt;Derivation: return &quot;true&quot; Derivation-&gt;&gt;Registers: lookup(key = [alice, bob])? Registers--&gt;&gt;Derivation: not found, initialize as &quot;false&quot; Derivation--&gt;&gt;Derivation: Register: &quot;false&quot; =&gt; &quot;true&quot; Derivation-)Registers: store(key = [alice, bob], value = &quot;true&quot;) Derivation-&gt;&gt;Publish λ: publish({sender: alice, recipient: bob}, register = &quot;true&quot;, previous = &quot;false&quot;)? Publish λ--&gt;&gt;Derivation: return {sender: alice, recipient: bob} FAQ Why not have one lambda that can return a register update and derived documents? Performance.Update and publish are designed to be parallelized and pipelined over many source documents simultaneously, while still giving the appearance and correctness of lambdas are invoked in strict serial order. Notice that (1) above doesn't depend on actually knowing the register value, which doesn't happen until (4). Many calls like (1) can also happen in parallel, so long as their applications to the register value (5) happen in the correct order. In comparison, a single-lambda design would require Flow to await each invocation before it can begin the next.  Register schemas may also havereduction annotations, and documents returned by update lambdas are reduced into the current register value. The compliance department reached out again, and this time they need you to identify transfers where the sender's account had insufficient funds. You manage this by tracking the running credits and debits of each account in a register. Then, you enrich each transfer with the account's current balance and whether the account was overdrawn: flagged-transfers.flow.yamlflagged-transfers.flow.tsflagged-transfers-test.flow.yaml import: - transfers.flow.yaml collections: acmeBank/flagged-transfers: schema: # Extend transfer schema with `balance` and `overdrawn` fields. $ref: transfers.schema.yaml required: [balance, overdrawn] properties: balance: { type: number } overdrawn: { type: boolean } key: [/id] projections: # Logically partition on transfers which are flagged as overdrawn. overdrawn: location: /overdrawn partition: true derivation: # Registers track the current balance of each account. register: schema: type: number reduce: { strategy: sum } initial: 0 transform: fromTransferSender: source: { name: acmeBank/transfers } shuffle: { key: [/sender] } # Debit the sender's register balance. update: { lambda: typescript } # Publish transfer enriched with current sender balance. publish: { lambda: typescript } fromTransferRecipient: source: { name: acmeBank/transfers } shuffle: { key: [/recipient] } # Credit the recipient's register balance. update: { lambda: typescript } Copy Source transfers are read twice. The first read shuffles on /recipientto track account credits, and the second shuffles on /senderto track account debits and to publish enriched transfer events. Update lambdas return the amount of credit or debit, and these amounts are summed into a derivation register keyed on the account. sequenceDiagram autonumber Derivation-&gt;&gt;Registers: lookup(key = alice)? Registers--&gt;&gt;Derivation: not found, initialize as 0 Derivation-&gt;&gt;Update λ: update({recipient: alice, amount: 50, ...})? Update λ--&gt;&gt;Derivation: return +50 Derivation-&gt;&gt;Update λ: update({sender: alice, amount: 75, ...})? Update λ--&gt;&gt;Derivation: return -75 Derivation--&gt;&gt;Derivation: Register: 0 + 50 =&gt; 50 Derivation--&gt;&gt;Derivation: Register: 50 - 75 =&gt; -25 Derivation-&gt;&gt;Publish λ: publish({sender: alice, amount: 75, ...}, register = -25, previous = 50)? Publish λ--&gt;&gt;Derivation: return {sender: alice, amount: 75, balance: -25, overdrawn: true} "},{"title":"Processing order​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#processing-order","content":"Derivations may have multiple transformations that simultaneously read from different source collections, or even multiple transformations that read from the same source collection. Roughly speaking, the derivation will globally process transformations and their source documents in the time-based order in which the source documents were originally written to their source collections. This means that a derivation started a month ago and a new copy of the derivation started today, will process documents in the same order and arrive at the same result. Derivations are repeatable. More precisely, processing order is stable for each individual shuffle key, though different shuffle keys may process in different orders if more than one task shard is used. Processing order can be attenuated through a read delayor differing transformation priority. "},{"title":"Read delay​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-delay","content":"A transformation can define a read delay, which will hold back the processing of its source documents until the time delay condition is met. For example, a read delay of 15 minutes would mean that a source document cannot be processed until it was published at least 15 minutes ago. If the derivation is working through a historical backlog of source documents, than a delayed transformation will respect its ordering delay relative to the publishing times of other historical documents also being read. Event-driven workflows are a great fit for reacting to events as they occur, but aren’t terribly good at taking action when something hasn’t happened: A user adds a product to their cart, but then doesn’t complete a purchase.A temperature sensor stops producing its expected, periodic measurements. A common pattern for tackling these workflows in Flow is to read a source collection without a delay and update a register. Then, read a collection with a read delay and determine whether the desired action has happened or not. For example, source from a collection of sensor readings and index the last timestamp of each sensor in a register. Then, source the same collection again with a read delay: if the register timestamp isn't more recent than the delayed source reading, the sensor failed to produce a measurement. Flow read delays are very efficient and scale better than managing very large numbers of fine-grain timers. Learn more from the Citi Bike idle bikes example "},{"title":"Read priority​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-priority","content":"Sometimes it's necessary for all documents of a source collection to be processed by a transformation before any documents of some other source collection are processed, regardless of their relative publishing time. For example, a collection may have corrections that should be applied before the historical data of another collection is re-processed. Transformation priorities allow you to express the relative processing priority of a derivation's various transformations. When priorities are not equal, all available source documents of a higher-priority transformation are processed before any source documents of a lower-priority transformation. "},{"title":"Where to accumulate?​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#where-to-accumulate","content":"When you build a derived collection, you must choose where accumulation will happen: whether Flow will reduce into documents held within your materialization endpoint, or within the derivation's registers. These two approaches can produce equivalent results, but they do so in very different ways. "},{"title":"Accumulate in your database​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-your-database","content":"To accumulate in your materialization endpoint, such as a database, you define a derivation with a reducible schema and use only publish lambdas and no registers. The Flow runtime uses your reduction annotations to combine published documents, which are written to the collection. It then fully reduces collection documents into the values stored in the database. This keeps the materialized table up to date. A key insight is that the database is the only stateful system in this scenario, and Flow uses reductions in two steps: To combine many published documents into intermediate delta documents, which are the documents written to collection storage.To reduce delta states into the final database-stored document. For example, consider a collection that’s summing a value: Time\tDB\tLambdas\tDerived DocumentT0\t0\tpublish(2, 1, 2)\t5 T1\t5\tpublish(-2, 1)\t-1 T2\t4\tpublish(3, -2, 1)\t2 T3\t6\tpublish()\t This works especially well when materializing into a transactional database. Flow couples its processing transactions with corresponding database transactions, ensuring end-to-end “exactly once” semantics. When materializing into a non-transactional store, Flow is only able to provide weaker “at least once” semantics; it’s possible that a document may be combined into a database value more than once. Whether that’s a concern depends a bit on the task at hand. Some reductions can be applied repeatedly without changing the result (they're &quot;idempotent&quot;), while in other use cases approximations are acceptable. For the summing example above, &quot;at-least-once&quot; semantics could give an incorrect result. "},{"title":"Accumulate in registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-registers","content":"To accumulate in registers, you use a derivation that defines a reducible register schema that's updated through update lambdas. The Flow runtime allocates, manages, and scales durable storage for registers; you don’t have to. Then you use publish lambdas to publish a snapshot of your register value into your collection. Returning to our summing example: Time\tRegister\tLambdas\tDerived DocumentT0\t0\tupdate(2, 1, 2), publish(register)\t5 T1\t5\tupdate(-2, 1), publish(register)\t4 T2\t4\tupdate(3, -2, 1), publish(register)\t6 T3\t6\tupdate()\t Register derivations are a great solution for materializations into non-transactional stores because the documents they produce can be applied multiple times without breaking correctness. They’re also well-suited for materializations into endpoints that aren't stateful, such as pub/sub systems or Webhooks, because they can produce fully reduced values as stand-alone updates. Learn more in the derivation pattern examples of Flow's repository "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/apache-kafka/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#prerequisites","content":"A Kafka cluster with: bootstrap.servers configured so that clients may connect via the desired host and portAn authentication mechanism of choice set up (highly recommended for production environments)Connection security enabled with TLS (highly recommended for production environments) "},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#authentication-and-connection-security","content":"Neither authentication nor connection security are enabled by default in your Kafka cluster, but both are important considerations. Similarly, Flow's Kafka connectors do not strictly require authentication or connection security mechanisms. You may choose to omit them for local development and testing; however, both are strongly encouraged for production environments. A wide variety of authentication methods is available in Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. Behavior using other authentication methods is not guaranteed. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol. If you don't already have authentication enabled on your cluster, Estuary recommends either of listed SASL/SCRAM methods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications. For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentation for details. Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample below provide configuration details specific to the Apache Kafka source connector. "},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to. The Kafka client will be informed of the rest of the cluster nodes by connecting to one of these nodes.\tarray\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /authentication\tAuthentication\tConnection details used to authenticate a client connection to Kafka via SASL.\tnull, object /authentication/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /authentication/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /authentication/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tKafka topic name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#sample","content":"captures: ${TENANT}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: [localhost:9093] tls: system_certificates authentication: mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman bindings: - resource: stream: ${TOPIC_NAME} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition will likely be more complex, with additional bindings for each Kafka topic. Learn more about capture definitions.. "},{"title":"Google Cloud Storage","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/gcs/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#prerequisites","content":"To use this connector, either your GCS bucket must be public, or you must have access via a Google service account. For public buckets, verify that objects in the bucket are publicly readable.For buckets accessed by a Google Service Account: Ensure that the user has been assigned a role with read access.Create a JSON service account key. Google's Application Default Credentials will use this file for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample in this section provide configuration details specific to the GCS source connector. tip You might use prefixes to organize your GCS bucket in a way that emulates a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /bucket\tBucket\tName of the Google Cloud Storage bucket\tstring\tRequired /googleCredentials\tGoogle service account\tService account JSON file. Required unless the bucket is public.\tobject /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose key (relative to the prefix) matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#sample","content":"A minimal capture definition will look like the following: captures: ${TENANT}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gcs:dev config: bucket: &quot;my-bucket&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition may be more complex, with additional bindings for different GCS prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Materialization connectors","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/","content":"","keywords":""},{"title":"Available materialization connectors​","type":1,"pageTitle":"Materialization connectors","url":"reference/Connectors/materialization-connectors/#available-materialization-connectors","content":"Apache Parquet in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-parquet:dev Elasticsearch ConfigurationPackage — ghcr.io/estuary/materialize-elasticsearch:dev Firebolt ConfigurationPackage - ghcr.io/estuary/materialize-firebolt:dev Google BigQuery ConfigurationPackage — ghcr.io/estuary/materialize-bigquery:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/materialize-postgres:dev Rockset ConfigurationPackage — ghcr.io/estuary/materialize-rockset:dev Snowflake ConfigurationPackage — ghcr.io/estuary/materialize-snowflake:dev "},{"title":"MySQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/MySQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#prerequisites","content":"To use this connector, you'll need a MySQL database setup with the following: binlog_row_metadatasystem variable set to FULL. Note that this can be done on a dedicated replica even if the primary database has it set to MINIMAL. Binary log expiration period of at at least seven days. If possible, it's recommended to keep the default setting of 2592000 seconds (30 days).A watermarks table. The watermarks table is a small &quot;scratch space&quot; to which the connector occasionally writes a small amount of data (a UUID, specifically) to ensure accuracy when backfilling preexisting table contents. The default name is &quot;flow.watermarks&quot;, but this can be overridden in config.json. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to insert, update, and delete on the watermarks table.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. "},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#setup","content":"To meet these requirements, do the following: Create the watermarks table. This table can have any name and be in any database, so long as config.json is modified accordingly. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); Copy Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture'; Copy Configure the binary log to record complete table metadata. SET PERSIST binlog_row_metadata = 'FULL'; Copy Configure the binary log to retain data for at least seven days, if previously set lower. SET PERSIST binlog_expire_logs_seconds = 604800; Copy "},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample below provide configuration details specific to the MySQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tDatabase address in the format host:port.\tstring\tRequired, &quot;127.0.0.1:3306&quot; /dbname\tDatabase name\tName of the database to capture from.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /server_id\tServer ID\tUnique value used for replication. Cannot be 0.\tinteger\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired, &quot;flow_capture&quot; /watermarks_table\tWatermarks table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in &lt;schema&gt;.&lt;table&gt; form.\tstring\t&quot;flow.watermarks&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace of the table, if used.\tstring /stream\tStream\tName of the table to be created in the database.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#sample","content":"A minimal capture definition will look like the following: captures: ${TENANT}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; dbname: &quot;test&quot; password: &quot;secret&quot; server_id: 12345 user: &quot;flow_capture&quot; watermarks_table: &quot;flow.watermarks&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"Connecting to secure networks​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#connecting-to-secure-networks","content":"beta SSH tunneling on the MySQL source connector is actively being worked on and will be fully supported soon. If you encounter issues with this feature, contact Estuary support. The MySQL source connector supports SSH tunnelingto allow Flow to connect to databases ports in secure networks. To set up and configure your SSH server, see the guide. "},{"title":"MySQL on managed cloud platforms​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#mysql-on-managed-cloud-platforms","content":"In addition to standard MySQL, this connector supports cloud-based MySQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#amazon-rds","content":"You can use this connector for MySQL instances on Amazon RDS using the following setup instructions. Estuary recommends creating a read replicain RDS for use with Flow; however, it's not required. You're able to apply the connector directly to the primary instance if you'd like. Setup​ You'll need to configure secure access to the database to enable the Flow capture. Estuary recommends SSH tunneling to allow this. Follow the guide to configure an SSH server for tunneling. beta SSH tunneling on the MySQL source connector is actively being worked on and will be fully supported soon. If you encounter issues with this feature, contact Estuary support. Create a RDS parameter group to enable replication in MySQL. Create a parameter group. Create a unique name and description and set the following properties: Family: mysql 8.0Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 If using the primary instance (not recommended), associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Create a read replica with the new parameter group applied (recommended). Create a read replicaof your MySQL database. Modify the replicaand set the following: DB parameter group: choose the parameter group you created previouslyBackup retention period: 7 days Reboot the replica to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture'; Copy Run the following command to set the binary log retention to seven days: CALL mysql.rds_set_configuration('binlog retention hours', 168); Copy "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#google-cloud-sql","content":"Google Cloud SQL doesn't currently support the setting binlog_row_metadata: FULL, which this connector requires. As a result, this connector can't be used directly for MySQL instance on Google Cloud. As an alternative, you can create a read replica outside of Google cloud. The replica can be treated as a standard MySQL instance. Set up an external replica. Follow the standard setup instructions for this connector. "},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#azure-database-for-mysql","content":"Azure Database for MySQL doesn't currently support the setting binlog_row_metadata: FULL, which this connector requires. As a result, this connector can't be used for MySQL instance on Azure. Contact your account manager or Estuary support for help using a third-party connector. Note that third party connectors will require you to create a read replica. "},{"title":"Google BigQuery","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/BigQuery/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#prerequisites","content":"To use this connector, you'll need: A new Google Cloud Storage bucket in the same region as the BigQuery destination dataset.A Google Cloud service account with a key file generated and the following roles: roles/bigquery.dataEditor on the destination datasetroles/bigquery.jobUser on the project with which the BigQuery destination dataset is associatedroles/storage.objectAdminon the GCS bucket created above At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a BigQuery materialization, which will direct one or more of your Flow collections to your desired tables within a BigQuery dataset. This configuration assumes a working knowledge of resource organization in BigQuery. You can find introductory documentation in the BigQuery docs. "},{"title":"Properties​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe project ID for the Google Cloud Storage bucket and BigQuery dataset.\tString\tRequired /billing_project_id\tBilling project ID\tThe project ID to which these operations are billed in BigQuery. Typically, you want this to be the same as project_id (the default).\tString\tSame as project_id /dataset\tDataset\tName of the target BigQuery dataset.\tString\tRequired /region\tRegion\tThe GCS region.\tString\tRequired /bucket\tBucket\tName of the GCS bucket.\tString\tRequired /bucket_path\tBucket path\tBase path within the GCS bucket.\tString\tRequired /credentials_json\tCredentials JSON\tBase64-encoded string of the full service account file.\tByte\tRequired To learn more about project billing, see the BigQuery docs. Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name.\tstring\tRequired /delta_updates\tDelta updates.\tWhether to use standard or delta updates\tboolean\tfalse "},{"title":"Sample​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#sample","content":"materializations: ${tenant}/${mat_name}: endpoint: connector: config: project_id: our-bigquery-project dataset: materialized-data region: US bucket: our-gcs-bucket bucket_path: bucket-path/ credentials_json: SSBqdXN0IHdhbm5hIHRlbGwgeW91IGhvdyBJJ20gZmVlbGluZwpHb3R0YSBtYWtlIHlvdSB1bmRlcnN0YW5kCk5ldmVyIGdvbm5hIGdpdmUgeW91IHVwCk5ldmVyIGdvbm5hIGxldCB5b3UgZG93bgpOZXZlciBnb25uYSBydW4gYXJvdW5kIGFuZCBkZXNlcnQgeW91Ck5ldmVyIGdvbm5hIG1ha2UgeW91IGNyeQpOZXZlciBnb25uYSBzYXkgZ29vZGJ5ZQpOZXZlciBnb25uYSB0ZWxsIGEgbGllIGFuZCBodXJ0IHlvdQ== image: ghcr.io/estuary/materialize-bigquery:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${tenant}/${source_collection} Copy "},{"title":"Delta updates​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your BigQuery table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in BigQuery won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${tenant}/${source_collection} Copy "},{"title":"Firebolt","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Firebolt/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#prerequisites","content":"To use this connector, you'll need: A Firebolt database with at least one engineAn S3 bucket where JSON documents will be stored prior to loadingAt least one Flow collectionYou may need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Firebolt materialization, which will direct Flow data to your desired Firebolt tables via an external table. "},{"title":"Properties​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/aws_key_id\tAWS key ID\tAWS access key ID for accessing the S3 bucket.\tstring /aws_region\tAWS region\tAWS region the bucket is in.\tstring /aws_secret_key\tAWS secret access key\tAWS secret key for accessing the S3 bucket.\tstring /database\tDatabase\tName of the Firebolt database.\tstring\tRequired /engine_url\tEngine URL\tEngine URL of the Firebolt database, in the format: &lt;engine-name&gt;.&lt;organization&gt;.&lt;region&gt;.app.firebolt.io.\tstring\tRequired /password\tPassword\tFirebolt password.\tstring\tRequired /s3_bucket\tS3 bucket\tName of S3 bucket where the intermediate files for external table will be stored.\tstring\tRequired /s3_prefix\tS3 prefix\tA prefix for files stored in the bucket.\tstring /username\tUsername\tFirebolt username.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the Firebolt table to store materialized results in. The external table will be named after this table with an _external suffix.\tstring\tRequired /table_type\tTable type\tType of the Firebolt table to store materialized results in. See the Firebolt docs for more details.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#sample","content":"materializations: ${tenant}/${mat_name}: endpoint: connector: config: database: my-db engine_url: my-db-my-engine-name.my-organization.us-east-1.app.firebolt.io password: secret # For public S3 buckets, only the bucket name is required s3_bucket: my-bucket username: firebolt-user # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-firebolt:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: table-name table_type: fact source: ${tenant}/${source_collection} Copy "},{"title":"Delta updates​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#delta-updates","content":"The Firebolt connector operates only in delta updates mode. Firebolt stores all deltas — the unmerged collection documents — directly. In some cases, this will affect how materialized views look in Firebolt compared to other systems that use standard updates. "},{"title":"Apache Parquet in S3","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Parquet/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#prerequisites","content":"To use this connector, you'll need: An AWS root or IAM user with access to the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a materialization, which will direct the contents of these Flow collections to Parquet files in S3. "},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS Access Key ID\tAWS credential used to connect to S3.\tstring\tRequired /awsSecretAccessKey\tAWS Secret Access Key\tAWS credential used to connect to S3.\tstring\tRequired /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a S3-compatible API that isn't provided by AWS\tstring. /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring /uploadIntervalInSeconds\tUpload Interval in Seconds\tTime interval, in seconds, at which to upload data from Flow to S3.\tinteger\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/compressionType\tCompression type\tThe method used to compress data in Parquet.\tstring /pathPrefix\tPath prefix\tThe desired Parquet file path within the bucket as determined by an S3 prefix.\tstring\tRequired The following compression types are supported: snappygziplz4zstd "},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#sample","content":"materializations: tenant/mat_name: endpoint: connector: config: awsAccessKeyId: AKIAIOSFODNN7EXAMPLE awsSecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYSECRET bucket: my-bucket uploadIntervalInSeconds: 300 # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-s3-parquet:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: pathPrefix: /my-prefix source: tenant/source_collection Copy "},{"title":"Delta updates​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#delta-updates","content":"This connector uses only delta updates mode. Collection documents are converted to Parquet format and stored in their unmerged state. "},{"title":"Elasticsearch","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Elasticsearch/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#prerequisites","content":"To use this connector, you'll need: An Elastic cluster with a known endpoint If the cluster is on the Elastic Cloud, you'll need an Elastic user with a role that grants all privileges on indices you plan to materialize to within the cluster. See Elastic's documentation on defining roles andsecurity privileges. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an Elasticsearch materialization, which will direct the contents of these Flow collections into Elasticsearch indices. By default, the connector attempts to map each field in the Flow collection to the most appropriate Elasticsearch field type. However, because each JSON field type can map to multiple Elasticsearch field types, you may want to override the defaults. You can configure this by adding field_overrides to the collection's binding in the materialization specification. To do so, provide a JSON pointer to the field in the collection schema, choose the output field type, and specify additional properties, if necessary. "},{"title":"Properties​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/endpoint\tEndpoint\tEndpoint host or URL. If using Elastic Cloud, this follows the format https://CLUSTER_ID.REGION.CLOUD_PLATFORM.DOMAIN:PORT.\tstring\tRequired /password\tPassword\tPassword to connect to the endpoint.\tstring /username\tUsername\tUser to connect to the endpoint.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\tRequired /field_overrides\tField overrides\tAssign Elastic field type to each collection field.\tarray\tRequired /field_overrides/-/es_type\tElasticsearch type\tThe overriding Elasticsearch data type of the field.\tobject /field_overrides/-/es_type/date_spec\tDate specifications\tConfiguration for the date field, effective if field_type is 'date'. See Elasticsearch docs.\tobject /field_overrides/-/es_type/date_spec/format\tDate format\tFormat of the date. See Elasticsearch docs.\tstring /field_overrides/-/es_type/field_type\tField type\tThe Elasticsearch field data types. Supported types include: boolean, date, double, geo_point, geo_shape, keyword, long, null, text.\tstring /field_overrides/-/es_type/keyword_spec\tKeyword specifications\tConfiguration for the keyword field, effective if field_type is 'keyword'. See Elasticsearch docs\tobject /field_overrides/-/es_type/keyword_spec/ignore_above\tIgnore above\tStrings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs\tinteger /field_overrides/-/es_type/text_spec\tText specifications\tConfiguration for the text field, effective if field_type is 'text'.\tobject /field_overrides/-/es_type/text_spec/dual_keyword\tDual keyword\tWhether or not to specify the field as text/keyword dual field.\tboolean /field_overrides/-/es_type/text_spec/keyword_ignore_above\tIgnore above\tEffective only if Dual Keyword is enabled. Strings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs.\tinteger /field_overrides/-/pointer\tPointer\tA '/'-delimited json pointer to the location of the overridden field.\tstring /index\tindex\tName of the ElasticSearch index to store the materialization results.\tstring\tRequired /number_of_replicas\tNumber of replicas\tThe number of replicas in ElasticSearch index. If not set, default to be 0. For single-node clusters, make sure this field is 0, because the Elastic search needs to allocate replicas on different nodes.\tinteger\t0 /number_of_shards\tNumber of shards\tThe number of shards in ElasticSearch index. Must set to be greater than 0.\tinteger\t1 "},{"title":"Sample​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#sample","content":"materializations: tenant/mat_name: endpoint: connector: # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-elasticsearch:dev config: endpoint: https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 username: flow_user password: secret # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: index: last-updated delta_updates: false field_overrides: - pointer: /updated-date es_type: field_type: date date_spec: format: yyyy-MM-dd source: tenant/source_collection Copy "},{"title":"Delta updates​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#delta-updates","content":"This connector supports both standard and delta updates. You must choose an option for each binding. Learn more about delta updates and the implications of using each update type. "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#prerequisites","content":"To use this connector, you'll need: A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /host\tHost\tHost name of the database.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /port\tPort\tPort on which to connect to the database.\tinteger /user\tUser\tDatabase user to connect as.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#sample","content":"materializations: ${tenant}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow host: localhost password: flow port: 5432 user: flow bindings: - resource: table: ${TABLE_NAME} source: ${TENANT}/${COLLECTION_NAME} Copy "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances. To connect securely, you must use an SSH tunnel. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#setup","content":"Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the additional of the networkProxy stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. tip You can find the values for forwardHost and forwardPort in the following locations in each platform's console: Amazon RDS: forwardHost as Endpoint; forwardPort as Port.Google Cloud SQL: forwardHost as Private IP Address; forwardPort is always 5432. You may need to configure private IP on your database.Azure Database: forwardHost as Server Name; forwardPort under Connection Strings (usually 5432). "},{"title":"Reserved Words​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#reserved-words","content":"PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in PostgreSQL's documentation here as well as here in the connector source code. Flow considers all the reserved words which are marked as &quot;reserved&quot; in either of the columns in the official documentation. "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#prerequisites","content":"This connector supports PostgreSQL versions 10.0 and later. You'll need a PostgreSQL database setup with the following: Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#setup","content":"info These setup instructions are PostgreSQL instances you manage yourself. If you use a cloud-based managed service for your database, different setup steps may be required. Instructions for setup on Amazon RDS can be found here. If you use a different managed service and the standard steps don't work as expected, contact Estuary support. The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role. For a more restricted setup, create a new user with just the required permissions as detailed in the following steps: Create a new user and password: CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION; Copy Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; Copy If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; Copy where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication: CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Copy Set WAL level to logical: ALTER SYSTEM SET wal_level = logical; Copy Restart PostgreSQL to allow the WAL level change to take effect. "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#configuration","content":"There are various ways to configure connectors. See connectors to learn more about these methods. The values and YAML sample below provide configuration details specific to the PostgreSQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the database to capture from.\tstring\tRequired, &quot;postgres&quot; /host\tHost\tHost name of the database.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /port\tPort\tPort of the database.\tinteger\tRequired, 5432 /publicationName\tPublication name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /slotName\tSlot name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired, &quot;postgres&quot; /watermarksTable\tWatermarks table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in &lt;schema&gt;.&lt;table&gt; form.\tstring\t&quot;public.flow_watermarks&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace of the table, if used.\tstring /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#sample","content":"A minimal capture definition will look like the following: captures: ${tenant}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: host: &quot;localhost&quot; port: 5432 database: &quot;flow&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; # slot_name: “flow_slot” # Default # publication_name: “flow_publication” # Default # watermarks_table: “public.flow_watermarks” # Default bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${TENANT}/${COLLECTION_NAME} Copy Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"Connecting to secure networks​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#connecting-to-secure-networks","content":"The PostgreSQL source connector supports SSH tunnelingto allow Flow to connect to databases ports in secure networks. To set up and configure your SSH server, see the guide. "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#amazon-rds","content":"You can use this connector for PostgreSQL instances on Amazon RDS using the following setup instructions. Setup​ You'll need to configure secure access to the database to enable the Flow capture. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. Enable logical replication on your RDS PostgreSQL instance. Create a parameter group. Create a unique name and description and set the following properties: Family: postgres13Type: DB Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the database. Reboot the database to allow the new parameter group to take effect. In the PostgreSQL client, run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Copy Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the forwardHost and forwardPort in the RDS console as the Endpoint and Port, respectively. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#google-cloud-sql","content":"You can use this connector for PostgreSQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow the connector to access your PostgreSQL instance using one of the following methods: Configure secure access. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. You'll need to set up a Google Cloud Virtual Machine to act as a proxy; be sure to follow the prerequisites outlined in the Google Cloud sectionsection of the guide. Configure the instance to allow unsecured connections. In your Cloud SQL settings, disable the requirement for SSL/TLSand enable public IP access, if necessary. Set the cloudsql.logical_decoding flag to on to enable logical replication on your loud SQL PostgreSQL instance. In your PostgreSQL client, issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH REPLICATION IN ROLE cloudsqlsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Copy Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the forwardHost under Public IP Address. The forwardPort is always 5432. "},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#azure-database-for-postgresql","content":"You can use this connector for instances on Azure Database for PostgreSQL using the following setup instructions. Setup​ You'll need to configure secure access to the database to enable the Flow capture. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. In your Azure PostgreSQL instance's server parameters, set wal_level to logical to enable logical replication. In the PostgreSQL client, run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION; GRANT pg_read_all_data TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON information_schema.columns, information_schema.tables, pg_catalog.pg_attribute, pg_catalog.pg_class, pg_catalog.pg_index, pg_catalog.pg_namespace TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Copy Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the host as Server Name, and the port under Connection Strings (usually 5432). "},{"title":"TOASTed values​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#toasted-values","content":"PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately. TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the a value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made. The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update. However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically: When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values "},{"title":"Troubleshooting​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#troubleshooting","content":"If you encounter an issue that you suspect is due to TOASTed values, try the following: Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance. "},{"title":"Rockset","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Rockset/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites","content":"To use this connector, you'll need: A Rockset account with an API key generated from the web UIA Rockset workspace Optional; if none exist, one will be created by the connector. A Rockset collection Optional; if none exist, one will be created by the connector. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections. "},{"title":"Properties​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI key\tRockset API key generated from the web UI.\tString\tRequired /http_logging\tHTTP logging\tEnable verbose logging of the HTTP calls to the Rockset API.\tbool\tfalse /max_concurrent_requests\tMaximum concurrent requests\tThe upper limit on how many concurrent requests will be sent to Rockset.\tint\t1 Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/workspace\tWorkspace\tRockset namespace. If it doesn't exist, the connector will create it.\tstring\tRequired /collection\tCollection\tRockset collection name. If it doesn't exist, the connector will create it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#sample","content":"materializations: ${tenant}/${mat_name}: endpoint: connector: config: api_key: supersecret # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-rockset:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: workspace: ${namespace_name} collection: ${table_name} source: ${tenant}/${source_collection} Copy "},{"title":"Delta updates and reduction strategies​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#delta-updates-and-reduction-strategies","content":"The Rockset connector operates only in delta updates mode. This means that Rockset, rather than Flow, performs the document merge. In some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates. Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of RFC 7396 - JSON merge. This differs from how Flow would reduce documents, most notably in that Rockset will not honor any reduction strategies defined in your Flow schema. For consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations in Flow mirror Rockset's semantics. To accomplish this, ensure that your collection schema has the following data reductions defined in its schema: A top-level reduction strategy of mergeA strategy of lastWriteWins for all nested values (this is the default) "},{"title":"Bulk ingestion for large backfills of historical data​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#bulk-ingestion-for-large-backfills-of-historical-data","content":"You can backfill large amounts of historical data into Rockset using a bulk ingestion. Bulk ingestion must originate in S3 and requires additional steps in your dataflow. Flow's Rockset connector supports this through the GitOps workflow. "},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites-1","content":"Before completing this workflow, make sure you have: A working catalog spec including at least one Flow collection.A production or development environment tip The following is an intermediate workflow. As needed, refer to this guide for the basic steps to create and deploy a catalog spec using the GitOps workflow. "},{"title":"How to perform a bulk ingestion​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#how-to-perform-a-bulk-ingestion","content":"A bulk ingestion from a Flow collection into Rockset is essentially a two-step process. First, Flow writes your historical data into an S3 bucket using Estuary's S3-Parquet materialization connector. Once the data is caught up, it uses the Rockset connector to backfill the data from S3 into Rockset and then switches to the Rockset Write API for the continuous materialization of new data. graph TD A[Create an S3 integration in Rockset] --&gt; B B[Create Flow materialization into S3 bucket] --&gt; C C[Wait for S3 materialization to catch up with historical data] --&gt;|When ready to bulk ingest into Rockset| D D[Disable S3 materialization shards] --&gt; E E[Update same materialization to use the Rockset connector with the integration created in first step] --&gt; F F[Rockset connector automatically continues materializing after the bulk ingestion completes] To set this up, use the following procedure as a guide, substituting example/flow/collection for your collection: You'll need an S3 integration in Rockset. To create one, follow the instructions here, but do not create the Rockset collection yet.Create and activate a materialization of example/flow/collection into a unique prefix within an S3 bucket of your choosing. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-s3-parquet:dev config: bucket: example-s3-bucket region: us-east-1 awsAccessKeyId: &lt;your key&gt; awsSecretAccessKey: &lt;your secret&gt; uploadIntervalInSeconds: 300 bindings: - resource: pathPrefix: example/s3-prefix/ source: example/flow/collection Copy Once the S3 materialization is caught up with your historical data, you'll switch to the Rockset write API for your future data. To make the switch, first disable the S3 materialization by setting shards to disabled in the definition, and re-deploy the catalog. This is necessary to ensure correct ordering of documents written to Rockset. materializations: example/toRockset: shards: disable: true # ...the remainder of the materialization yaml remains the same as above Copy Update the materialization to use the materialize-rockset connector, and re-enable the shards. Here you'll provide the name of the Rockset S3 integration you created above, as well as the bucket and prefix that you previously materialized into. It's critical that the name of the materialization remains the same as it was for materializing into S3. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-rockset:dev config: api_key: &lt;your rockset API key here&gt; max_concurrent_requests: 5 bindings: - resource: workspace: &lt;your rockset workspace name&gt; collection: &lt;your rockset collection name&gt; initializeFromS3: integration: &lt;rockset integration name&gt; bucket: example-s3-bucket region: us-east-1 prefix: example/s3-prefix/ source: example/flow/collection Copy When you activate the new materialization, the connector will create the Rockset collection using the given integration, and wait for it to ingest all of the historical data from S3 before it continues. Once this completes, the Rockset connector will automatically switch over to the incoming stream of new data. "},{"title":"Reduction strategies","type":0,"sectionRef":"#","url":"reference/reduction-strategies/","content":"","keywords":""},{"title":"Reduction guarantees​","type":1,"pageTitle":"Reduction strategies","url":"reference/reduction-strategies/#reduction-guarantees","content":"In Flow, documents that share the same collection key and are written to the same logical partition have a total order, meaning that one document is universally understood to have been written before the other. This isn't true of documents of the same key written to different logical partitions. These documents can be considered “mostly” ordered: Flow uses timestamps to understand the relative ordering of these documents, and while this largely produces the desired outcome, small amounts of re-ordering are possible and even likely. Flow guarantees exactly-once semantics within derived collections and materializations (so long as the target system supports transactions), and a document reduction will be applied exactly one time. Flow does not guarantee that documents are reduced in sequential order, directly into a base document. For example, documents of a single Flow capture transaction are combined together into one document per collection key at capture time – and that document may be again combined with still others, and so on until a final reduction into the base document occurs. Taken together, these total-order and exactly-once guarantees mean that reduction strategies must be associative [as in (2 + 3) + 4 = 2 + (3 + 4) ], but need not be commutative [ 2 + 3 = 3 + 2 ] or idempotent [ S u S = S ]. They expand the palette of strategies that can be implemented, and allow for more efficient implementations as compared to, for example CRDTs. In this documentation, we’ll refer to the “left-hand side” (LHS) as the preceding document and the “right-hand side” (RHS) as the following one. Keep in mind that both the LHS and RHS may themselves represent a combination of still more ordered documents because, for example, reductions are applied associatively. "},{"title":"append","type":0,"sectionRef":"#","url":"reference/reduction-strategies/append/","content":"append append works with arrays, and extends the left-hand array with items from the right-hand side. collections: - name: example/reductions/append schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Append only works with type &quot;array&quot;. # Others will throw an error at build time. type: array reduce: { strategy: append } required: [key] key: [/key] tests: &quot;Expect we can append arrays&quot;: - ingest: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2] } - { key: &quot;key&quot;, value: [3, null, &quot;abc&quot;] } - verify: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2, 3, null, &quot;abc&quot;] } Copy The right-hand side must always be an array. The left-hand side may be null, in which case the reduction is treated as a no-op and its result remains null. This can be combined with schema conditionals to toggle whether reduction-reduction should be done or not.","keywords":""},{"title":"Snowflake","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#prerequisites","content":"To use this connector, you'll need: A Snowflake account that includes: A target database, to which you'll materialize dataA schema — a logical grouping of database objects — within the target databaseA user with a role assigned that grants the MODIFY privilege on the target database At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Snowflake materialization, which will direct one or more of your Flow collections to new Snowflake tables. "},{"title":"Properties​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account\tAccount\tThe Snowflake account identifier\tstring\tRequired /database\tDatabase\tName of the Snowflake database to which to materialize\tstring\tRequired /password\tPassword\tSnowflake user password\tstring\tRequired /region\tRegion\tRegion where the account is located\tstring /role\tRole\tRole assigned to the user\tstring /schema\tSchema\tSnowflake schema within the database to which to materialize\tstring\tRequired /user\tUser\tSnowflake username\tstring\tRequired /warehouse\tWarehouse\tName of the data warehouse that contains the database\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean /table\tTable\tTable name\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#sample","content":" materializations: ${tenant}/${mat_name}: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${tenant}/${source_collection} Copy "},{"title":"Delta updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your Snowflake table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Snowflake won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${tenant}/${source_collection} Copy "},{"title":"Optimizing performance for standard updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#optimizing-performance-for-standard-updates","content":"When using standard updates for a large dataset, the collection key you choose can have a significant impact on materialization performance and efficiency. Snowflake uses micro partitions to physically arrange data within tables. Each micro partition includes metadata, such as the minimum and maximum values for each column. If you choose a collection key that takes advantage of this metadata to help Snowflake prune irrelevant micro partitions, you'll see dramatically better performance. For example, if you materialize a collection with a key of /user_id, it will tend to perform far worse than a materialization of /date, /user_id. This is because most materializations tend to be roughly chronological over time, and that means that data is written to Snowflake in roughly /date order. This means that updates of keys /date, /user_id will need to physically read far fewer rows as compared to a key like /user_id, because those rows will tend to live in the same micro-partitions, and Snowflake is able to cheaply prune micro-partitions that aren't relevant to the transaction. "},{"title":"Reserved words​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#reserved-words","content":"Snowflake has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Snowflake's documentation here and in the table below. caution In Snowflake, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Snowflake docs. Reserved words account\tfrom\tqualify all\tfull\tregexp alter\tgrant\trevoke and\tgroup\tright any\tgscluster\trlike as\thaving\trow between\tilike\trows by\tin\tsample case\tincrement\tschema cast\tinner\tselect check\tinsert\tset column\tintersect\tsome connect\tinto\tstart connection\tis\ttable constraint\tissue\ttablesample create\tjoin\tthen cross\tlateral\tto current\tleft\ttrigger current_date\tlike\ttrue current_time\tlocaltime\ttry_cast current_timestamp\tlocaltimestamp\tunion current_user\tminus\tunique database\tnatural\tupdate delete\tnot\tusing distinct\tnull\tvalues drop\tof\tview else\ton\twhen exists\tor\twhenever false\torder\twhere following\torganization\twith for  "},{"title":"firstWriteWins and lastWriteWins","type":0,"sectionRef":"#","url":"reference/reduction-strategies/firstwritewins-and-lastwritewins/","content":"firstWriteWins and lastWriteWins firstWriteWins always takes the first value seen at the annotated location. Likewise, lastWriteWins always takes the last. Schemas that don’t have an explicit reduce annotation default to lastWriteWins behavior. collections: - name: example/reductions/fww-lww schema: type: object reduce: { strategy: merge } properties: key: { type: string } fww: { reduce: { strategy: firstWriteWins } } lww: { reduce: { strategy: lastWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can track first- and list-written values&quot;: - ingest: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;one&quot; } - { key: &quot;key&quot;, fww: &quot;two&quot;, lww: &quot;two&quot; } - verify: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;two&quot; } Copy","keywords":""},{"title":"Organizing a Flow catalog","type":0,"sectionRef":"#","url":"reference/organizing-catalogs/","content":"","keywords":""},{"title":"import​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#import","content":"Flow's import directive can help you easily handle all of these scenarios while keeping your catalogs well organized. Each catalog spec file may import any number of other files, and each import may refer to either relative or an absolute URL. When you use import in a catalog spec, you're conceptually bringing the entirety of another catalog — as well as the schemas and typescript files it uses — into your catalog. Imports are also transitive, so when you import another catalog, you're also importing everything that other catalog has imported. This allows you to keep your catalogs organized, and is flexible enough to support collaboration between separate teams and organizations. Perhaps the best way of explaining this is with some examples. Example: Organizing collections​ Let's look at a relatively simple case in which you want to organize your collections into multiple catalog files. Say you work for Acme Corp on the team that's introducing Flow. You might start with the collections and directory structure below: acme/customers/customerInfo acme/products/info/manufacturers acme/products/info/skus acme/products/inventory acme/sales/pending acme/sales/complete Copy acme ├── flow.yaml ├── customers │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml ├── products │ ├── flow.yaml │ ├── info │ │ ├── flow.ts │ │ ├── flow.yaml │ │ └── schemas.yaml │ └── inventory │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml schemas.yaml └── sales ├── flow.ts ├── flow.yaml └── schemas.yaml Copy It's immediately clear where each of the given collections is defined, since the directory names match the path segments in the collection names. This is not required by theflowctl CLI, but is strongly recommended, since it makes your catalogs more readable and maintainable. Each directory contains a catalog spec (flow.yaml), which will import all of the catalogs from child directories. So, the top-level catalog spec, acme/flow.yaml, might look something like this: import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml Copy This type of layout has a number of other advantages. During development, you can easily work with a subset of collections using, for example, flowctl test --source acme/products/flow.yaml to run only the tests for product-related collections. It also allows other imports to be more granular. For example, you might want a derivation under sales to read from acme/products/info. Since info has a separate catalog spec, acme/sales/flow.yaml can import acme/products/info/flow.yaml without creating a dependency on the inventory collection. Example: Separate environments​ It's common to use separate environments for tiers like development, staging, and production. Flow catalog specs often necessarily include endpoint configuration for external systems that will hold materialized views. Let's say you want your production environment to materialize views to Snowflake, but you want to develop locally on SQLite. We might modify the Acme example slightly to account for this. acme ├── dev.flow.yaml ├── prod.flow.yaml ... the remainder is the same as above Copy Each of the top-level catalog specs might import all of the collections and define an endpoint called ourMaterializationEndpoint that points to the desired system. The import block might be the same for each system, but each file may use a different configuration for the endpoint, which is used by any materializations that reference it. Our configuration for our development environment will look like: dev.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml ourMaterializationEndpoint: # dev.flow.yaml sqlite: path: dev-materializations.db Copy While production will look like: prod.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml endpoints: snowflake: account: acme_production role: admin schema: snowflake.com/acmeProd user: importantAdmin password: abc123 warehouse: acme_production Copy When we want to test locally, we simply run flowctl test dev.flow.yaml and when we push to production we'll likely run flowctl apply prod.flow.yaml. From there, everything will continue to work because in our development environment we'll be binding collections to our local SQLite DB and in production we'll use Snowflake. Example: Cross-team collaboration​ When working across teams, it's common for one team to provide a data product for another to reference and use. Flow is designed for cross-team collaboration, allowing teams and users to reference each other's full catalog or schema.  Again using the Acme example, let's imagine we have two teams. Team Web is responsible for Acme's website, and Team User is responsible for providing a view of Acme customers that's always up to date. Since Acme wants a responsive site that provides a good customer experience, Team Web needs to pull the most up-to-date information from Team User at any point. Let's look at Team User's collections: teamUser.flow.yaml import: - userProfile.flow.yaml Copy Which references: userProfile.flow.yaml collection: userProfile: schema: -&quot;/userProfile/schema&quot; key: [/id] Copy Team User references files in their directory, which they actively manage in both their import and schema sections. If Team Web wants to access user data (and they have access), they can use a relative path or a URL-based path given that Team User publishes their data to a URL for access: teamWeb.flow.yaml import: -http://www.acme.com/teamUser#userProfile.flow.yaml -webStuff.flow.yaml Copy Now Team Web has direct access to collections (referenced by their name) to build derived collections on top of. They can also directly import schemas: webStuff.flow.yaml collection: webStuff: schema: -http://acme.com/teamUser#userProfile/#schema key: [/id] Copy "},{"title":"Global namespace​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#global-namespace","content":"Every Flow collection has a name, and that name must be unique within a running Flow system. Flow collections should be thought of as existing within a global namespace. Keeping names globally unique makes it easy to import catalogs from other teams, or even other organizations, without having naming conflicts or ambiguities. For example, imagine your catalog for the inside sales team has a collection just named customers. If you later try to import a catalog from the outside sales team that also contains a customers collection, 💥 there's a collision. A better collection name would be acme/inside-sales/customers. This allows a catalog to include customer data from separate teams, and also separate organizations. "},{"title":"Composing with conditionals","type":0,"sectionRef":"#","url":"reference/reduction-strategies/composing-with-conditionals/","content":"Composing with conditionals Reduction strategies are JSON Schema annotations. As such, their applicability at a given document location can be controlled through the use of conditional keywords within the schema, like oneOf or if/then/else. This means Flow’s built-in strategies can be combined with schema conditionals to construct a wider variety of custom reduction behaviors. For example, here’s a reset-able counter: collections: - name: example/reductions/sum-reset schema: type: object properties: key: { type: string } value: { type: number } required: [key] # Use oneOf to express a tagged union over &quot;action&quot;. oneOf: # When action = reset, reduce by taking this document. - properties: { action: { const: reset } } reduce: { strategy: lastWriteWins } # When action = sum, reduce by summing &quot;value&quot;. Keep the LHS &quot;action&quot;, # preserving a LHS &quot;reset&quot;, so that resets are properly associative. - properties: action: const: sum reduce: { strategy: firstWriteWins } value: { reduce: { strategy: sum } } reduce: { strategy: merge } key: [/key] tests: &quot;Expect we can sum or reset numbers&quot;: - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: sum, value: 5 } - { key: &quot;key&quot;, action: sum, value: -1.2 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 3.8 } - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: reset, value: 0 } - { key: &quot;key&quot;, action: sum, value: 1.3 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 1.3 } Copy","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"reference/reduction-strategies/sum/","content":"sum sum reduces two numbers or integers by adding their values. collections: - name: example/reductions/sum schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sum only works with types &quot;number&quot; or &quot;integer&quot;. # Others will throw an error at build time. type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can sum two numbers&quot;: - ingest: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 5 } - { key: &quot;key&quot;, value: -1.2 } - verify: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 3.8 } Copy","keywords":""},{"title":"minimize and maximize","type":0,"sectionRef":"#","url":"reference/reduction-strategies/minimize-and-maximize/","content":"minimize and maximize minimize and maximize reduce by taking the smallest or largest seen value, respectively. collections: - name: example/reductions/min-max schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: { reduce: { strategy: minimize } } max: { reduce: { strategy: maximize } } required: [key] key: [/key] tests: &quot;Expect we can min/max values&quot;: - ingest: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;abc&quot; } - { key: &quot;key&quot;, min: 42, max: &quot;def&quot; } - verify: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;def&quot; } Copy minimize and maximize can also take a key, which is one or more JSON pointers that are relative to the reduced location. Keys make it possible to minimize and maximize over complex types by ordering over an extracted composite key. In the event that a right-hand side document key equals the current left-hand side minimum or maximum, the documents are deeply merged. This can be used to, for example, track not just the minimum value but also the number of times it’s been seen: collections: - name: example/reductions/min-max-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: $anchor: min-max-value type: array items: - type: string - type: number reduce: { strategy: sum } reduce: strategy: minimize key: [/0] max: $ref: &quot;#min-max-value&quot; reduce: strategy: maximize key: [/0] required: [key] key: [/key] tests: &quot;Expect we can min/max values using a key extractor&quot;: - ingest: collection: example/reductions/min-max-key documents: - { key: &quot;key&quot;, min: [&quot;a&quot;, 1], max: [&quot;a&quot;, 1] } - { key: &quot;key&quot;, min: [&quot;c&quot;, 2], max: [&quot;c&quot;, 2] } - { key: &quot;key&quot;, min: [&quot;b&quot;, 3], max: [&quot;b&quot;, 3] } - { key: &quot;key&quot;, min: [&quot;a&quot;, 4], max: [&quot;a&quot;, 4] } - verify: collection: example/reductions/min-max-key documents: # Min of equal keys [&quot;a&quot;, 1] and [&quot;a&quot;, 4] =&gt; [&quot;a&quot;, 5]. - { key: &quot;key&quot;, min: [&quot;a&quot;, 5], max: [&quot;c&quot;, 2] } Copy","keywords":""},{"title":"Working with logs and statistics","type":0,"sectionRef":"#","url":"reference/working-logs-stats/","content":"","keywords":""},{"title":"Accessing logs​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs","content":"You can access logs by materializing them to an external endpoint, or from the command line. "},{"title":"Accessing logs from the command line​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-from-the-command-line","content":"The flowctl logs subcommand allows you to print logs from the command line. This method allows more flexibility and is ideal for debugging. You can retrieve logs for any task that is part of a catalog that is currently deployed. Printing logs for a specific task​ You can print logs for a given deployed task using the flag --task followed by the task name. flowctl logs --task acmeCo/anvils/capture-one Copy Printing all logs for a tenant​ You can print all logs for currently deployed catalogs of a given tenant using the flag --tenant. flowctl logs --tenant acmeCo Copy This is the same as printing the entire contents of the collection ops/acmeCo/logs. Printing logs by task type​ Within a given tenant, you can print logs for all deployed tasks of a given type using the flag --task-type followed by one of capture, derivation, or materialization. flowctl logs --tenant acmeCo --task-type capture Copy "},{"title":"Accessing logs by materialization​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-by-materialization","content":"You can materialize your logs collection to an external system. This is typically the preferred method if you’d like to continuously work with or monitor logs. It's easiest to materialize the whole collection, but you can use a partition selector to only materialize specific tasks, as the logs collection is partitioned on tasks. caution Be sure to add a partition selector to exclude the logs of the materialization itself. Otherwise, you could trigger an infinite loop in which the connector materializes its own logs, logs that event, and so on. acmeCo/anvils/logs: endpoint: connector: image: ghcr.io/estuary/materialize-webhook:dev config: address: my.webhook.com bindings: - resource: relativePath: /log/wordcount source: ops/acmeCo/logs # Exclude the logs of this materialization to avoid an infinite loop. partitions: exclude: name: ['acmeCo/anvils/logs'] Copy "},{"title":"merge","type":0,"sectionRef":"#","url":"reference/reduction-strategies/merge/","content":"merge merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays. If both sides are objects, merge performs a deep merge of each property. If LHS and RHS are both arrays, items at each index of both sides are merged together, extending the shorter of the two sides by taking items off the longer: collections: - name: example/reductions/merge schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Merge only works with types &quot;array&quot; or &quot;object&quot;. # Others will throw an error at build time. type: [array, object] reduce: { strategy: merge } # Deeply merge sub-locations (items or properties) by summing them. items: type: number reduce: { strategy: sum } additionalProperties: type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can merge arrays by index&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [1, 1] } - { key: &quot;key&quot;, value: [2, 2, 2] } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [3, 3, 2] } &quot;Expect we can merge objects by property&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;b&quot;: 1 } } - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;c&quot;: 1 } } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 2, &quot;b&quot;: 1, &quot;c&quot;: 1 } } Copy Merge may also take a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. Arrays must be pre-sorted and de-duplicated by the key, and merge itself always maintains this invariant. Note that you can use a key of [“”] for natural item ordering, such as merging sorted arrays of scalars. collections: - name: example/reductions/merge-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: type: array reduce: strategy: merge key: [/k] items: { reduce: { strategy: firstWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can merge sorted arrays&quot;: - ingest: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }] } - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 2 }, { k: &quot;c&quot;, v: 2 }] } - verify: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }, { k: &quot;c&quot;, v: 2 }], } Copy As with append, the LHS of merge may be null, in which case the reduction is treated as a no-op and its result remains null.","keywords":""},{"title":"set","type":0,"sectionRef":"#","url":"reference/reduction-strategies/set/","content":"set set interprets the document location as an update to a set. The location must be an object having only “add&quot;, “intersect&quot;, and “remove” properties. Any single “add&quot;, “intersect&quot;, or “remove” is always allowed. A document with “intersect” and “add” is allowed, and is interpreted as applying the intersection to the left-hand side set, followed by a union with the additions. A document with “remove” and “add” is also allowed, and is interpreted as applying the removals to the base set, followed by a union with the additions. “remove” and “intersect” within the same document are prohibited. Set additions are deeply merged. This makes sets behave like associative maps, where the “value” of a set member can be updated by adding it to the set again, with a reducible update. Sets may be objects, in which case the object property serves as the set item key: collections: - name: example/reductions/set schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: { strategy: set } # Schema for &quot;add&quot;, &quot;intersect&quot;, and &quot;remove&quot; properties # (each a map of keys and their associated sums): additionalProperties: type: object additionalProperties: type: number reduce: { strategy: sum } # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } required: [key] key: [/key] tests: &quot;Expect we can apply set operations to incrementally build associative maps&quot;: - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 } } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: { &quot;b&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;d&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 2, &quot;c&quot;: 1, &quot;d&quot;: 1 } } } - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: { &quot;a&quot;: 0, &quot;d&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;e&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 3, &quot;d&quot;: 1, &quot;e&quot;: 1 } } } Copy Sets can also be sorted arrays, which are ordered using a provide key extractor. Keys are given as one or more JSON pointers, each relative to the item. As with merge, arrays must be pre-sorted and de-duplicated by the key, and set reductions always maintain this invariant. Use a key extractor of [“”] to apply the natural ordering of scalar values. Whether array or object types are used, the type must always be consistent across the “add” / “intersect” / “remove” terms of both sides of the reduction. collections: - name: example/reductions/set-array schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: strategy: set key: [/0] # Schema for &quot;add&quot;, &quot;intersect&quot;, &amp; &quot;remove&quot; properties # (each a sorted array of [key, sum] 2-tuples): additionalProperties: type: array # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } # Schema for contained [key, sum] 2-tuples: items: type: array items: - type: string - type: number reduce: { strategy: sum } reduce: { strategy: merge } required: [key] key: [/key] tests: ? &quot;Expect we can apply operations of sorted-array sets to incrementally build associative maps&quot; : - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;b&quot;, 1], [&quot;c&quot;, 1]] } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: [[&quot;b&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;d&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 2], [&quot;c&quot;, 1], [&quot;d&quot;, 1]] } } - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: [[&quot;a&quot;, 0], [&quot;d&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;e&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 3], [&quot;d&quot;, 1], [&quot;e&quot;, 1]] } } Copy","keywords":""}]