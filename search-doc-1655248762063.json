[{"title":"Journals","type":0,"sectionRef":"#","url":"concepts/advanced/journals/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#specification","content":"Flow collections can control some aspects of how their contents are mapped into journals through the journals stanza: collections: acmeCo/orders: schema: orders.schema.yaml key: [/id] journals: # Configuration for journal fragments. # Required, type: object. fragments: # Codec used to compress fragment files. # One of ZSTANDARD, SNAPPY, GZIP, or NONE. # Optional. Default is GZIP. compressionCodec: GZIP # Maximum flush delay before in-progress fragment buffers are closed # and persisted. Default uses no flush interval. # Optional. Given as a time duration. flushInterval: 15m # Desired content length of each fragment, in megabytes before compression. # Default is 512MB. # Optional, type: integer. length: 512 # Duration for which historical files of the collection should be kept. # Default is forever. # Optional. Given as a time duration. retention: 720h  Your storage mappings determine which of your cloud storage buckets is used for storage of collection fragment files. "},{"title":"Physical partitions​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#physical-partitions","content":"Every logical partition of a Flow collection is created with a single physical partition. Later and as required, new physical partitions are added in order to increase the write throughput of the collection. Each physical partition is responsible for all new writes covering a range of keys occurring in collection documents. Conceptually, if keys range from [A-Z] then one partition might cover [A-F] while another covers [G-Z]. The pivot of a partition reflects the first key in its covered range. One physical partition is turned into more partitions by subdividing its range of key ownership. For instance, a partition covering [A-F]is split into partitions [A-C] and [D-F]. Physical partitions are journals. The relationship between the journal and its specific collection and logical partition are encoded withinits journal specification. "},{"title":"Fragment files​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#fragment-files","content":"Journal fragment files each hold a slice of your collection's content, stored as a compressed file of newline-delimited JSON documents in your cloud storage bucket. Files are flushed to cloud storage periodically, typically after they reach a desired size threshold. They use a content-addressed naming scheme which allows Flow to understand how each file stitches into the overall journal. Consider a fragment file path like: s3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/0000000000000000-00000000201a3f27-1ec69e2de187b7720fb864a8cd6d50bb69cc7f26.gz This path has the following components: Component\tExampleStorage prefix of physical partition\ts3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/ Supplemental time pseudo-partitions\tutc_date=2022-01-07/utc_hour=19/ Beginning content offset\t0000000000000000 Ending content offset\t00000000201a3f27 SHA content checksum\t1ec69e2de187b7720fb864a8cd6d50bb69cc7f26 Compression codec\t.gz The supplemental time pseudo-partitions are not logical partitions, but are added to each fragment file path to facilitate integration with external tools that understand Hive layouts. "},{"title":"Hive layouts​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#hive-layouts","content":"As we've seen, collection fragment files are written to cloud storage with path components like/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/. If you've used tools within the Apache Hive ecosystem, this layout should feel familiar. Flow organizes files in this way to make them directly usable by tools that understand Hive partitioning, like Spark and Hive itself. Collections can also be integrated as Hive-compatible external tables in tools likeSnowflakeandBigQueryfor ad-hoc analysis. SQL queries against these tables can even utilize predicate push-down, taking query predicates over category, utc_date, and utc_hourand pushing them down into the selection of files that must be read to answer the query — often offering much faster and more efficient query execution because far less data must be read. "},{"title":"Logs and statistics","type":0,"sectionRef":"#","url":"concepts/advanced/logs-stats/","content":"","keywords":""},{"title":"Logs​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#logs","content":"Each organization that uses Flow has a logs collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/logs. These can be thought of as standard application logs: they store information about events that occur at runtime. They’re distinct from recovery logs, which track the state of various task shards. Regardless of how many Flow catalogs your organization has, all logs are stored in the same collection, which is read-only and logically partitioned on tasks. Logs are collected from events that occur within the Flow runtime, as well as the capture and materialization connectors your catalog is using. "},{"title":"Log level​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#log-level","content":"You can set the log level for each catalog task to control the level of detail at which logs are collected for that task. The available levels, listed from least to most detailed, are: error: Non-recoverable errors from the Flow runtime or connector that are critical to know aboutwarn: Errors that can be re-tried, but likely require investigationinfo: Task lifecycle events, or information you might want to collect on an ongoing basisdebug: Details that will help debug an issue with a tasktrace: Maximum level of detail that may yield gigabytes of logs The default log level is info. You can change a task’s log level by adding the shards keyword to its definition in the catalog spec: materializations: acmeCo/debugMaterialization: shards: logLevel: debug endpoint: {}  To learn more about working with logs and statistics, see their reference documentation. "},{"title":"Concepts","type":0,"sectionRef":"#","url":"concepts/","content":"","keywords":""},{"title":"Catalogs​","type":1,"pageTitle":"Concepts","url":"concepts/#catalogs","content":"A catalog comprises all the components that describe how your data flows function and behave: captures, collections, derivations, materializations, tests, and more. For example: How to capture data from source systems into collectionsThe schemas of those collections, which Flow enforcesHow to derive collections as transformations of other source collectionsMaterializations of collections into destination systemsYour tests of schema and derivation behaviors Together the captures, collections, derivations, and materializations of your catalog form a graph of your data flows: graph LR; capture/two--&gt;collection/D; capture/one--&gt;collection/C; capture/one--&gt;collection/A; collection/A--&gt;derivation/B; collection/D--&gt;derivation/E; collection/C--&gt;derivation/E; derivation/B--&gt;derivation/E; collection/D--&gt;materialization/one; derivation/E--&gt;materialization/two; "},{"title":"Namespace​","type":1,"pageTitle":"Concepts","url":"concepts/#namespace","content":"All catalog entities (captures, materializations, and collections) are identified by a namesuch as acmeCo/teams/manufacturing/anvils. Names have directory-like prefixes and every name within Flow is globally unique. If you've ever used database schemas to organize your tables and authorize access, you can think of name prefixes as being akin to database schemas with arbitrary nesting. All catalog entities exist together in a single namespace. As a Flow customer, you're provisioned one or more high-level prefixes for your organization. Further division of the namespace into prefixes is up to you. Prefixes of the namespace, like acmeCo/teams/manufacturing/, are the foundation for Flow's authorization model. "},{"title":"Builds​","type":1,"pageTitle":"Concepts","url":"concepts/#builds","content":"Catalog entities like collections are very long-lived and may evolve over time. A collection's schema might be extended with new fields, or a transformation might be updated with a bug fix. When one or more catalog entities are updated, a catalog build validates their definitions and prepares them for execution by Flow's runtime. Every build is assigned a unique identifier called a build ID, and the build ID is used to reconcile which version of a catalog entity is being executed by the runtime. A catalog build is activated into Flow's runtime to deploy its captures, collections, and so on, possibly replacing an older build under which they had been running. "},{"title":"Specifications​","type":1,"pageTitle":"Concepts","url":"concepts/#specifications","content":"A catalog build begins from a set of catalog specificationswhich define the behavior of your catalog: the entities it contains, like captures, collections, and materializations, and their specific behaviors and configuration. You define catalog specifications using either the Flow web application, or by directly creating and editing YAML or JSON files which are typically managed in a Git repository using familiar developer workflows (often called &quot;GitOps&quot;). These files use the extension *.flow.yaml or simply flow.yaml by convention. As a practical benefit, using this extension activates Flow's VS Code integration and auto-complete. Flow integrates with VS Code for development environment support, like auto-complete, tooltips, and inline documentation. Depending on your catalog, you may also have TypeScript modules, JSON schemas, or test fixtures which are also managed in your Git repository. Whether you use the web app or Git-managed specifications is up to you, and teams can switch back and forth depending on what's more familiar. note The Flow web application is currently in private beta.  "},{"title":"Collections​","type":1,"pageTitle":"Concepts","url":"concepts/#collections","content":"Collections are the fundamental representation for datasets within Flow, akin to a database table. More technically, they're a collection of documents having a common key and schema. Data in collections is not modelled as a table, however. Collections are best described as a real-time data lake: documents are stored as an organized layout of JSON files in your cloud storage bucket. If Flow needs to read historical data — say, as part of creating a new materialization — it does so by reading from your bucket. You can use regular bucket lifecycle policies to manage the deletion of data from a collection. However, capturing into a collection or materializing from a collection happens within milliseconds. Learn more about collections "},{"title":"Journals​","type":1,"pageTitle":"Concepts","url":"concepts/#journals","content":"Journals provide the low-level storage for Flow collections. Each logical and physical partition of a collection is backed by a journal. Task shards also use journals to provide for their durability and fault tolerance. Each shard has an associated recovery log, which is a journal into which internal checkpoint states are written. Journals and shards are advanced topics that may be beneficial for specialized engineering applications. Learn more about journals  "},{"title":"Captures​","type":1,"pageTitle":"Concepts","url":"concepts/#captures","content":"A capture is a Flow task that connects to a source endpoint system and binds one or more of its resources (tables, streams, etc) to Flow collections. Data continuously flows from each resource in the endpoint to its Flow collection; as new data become available at the source, Flow validates their schema and adds them to their bound collection. There are two categories of captures: Pull captures which pull documents from an endpoint using a connector.Push captures which expose an URL endpoint which can be directly written into, such as via a Webhook POST. caution Push captures are under development. Learn more about captures  "},{"title":"Materializations​","type":1,"pageTitle":"Concepts","url":"concepts/#materializations","content":"A materialization is a catalog task that connects to an destination endpoint system and binds one or more collections to corresponding resources (tables, etc) in that system. Data continuously flows from each Flow collection into its corresponding resource in the endpoint. Materializations are the conceptual inverse of captures. As new documents become available within bound collections, the materialization keeps endpoint resources up to date using precise, incremental updates. Like captures, materializations are powered by connectors. Learn more about materializations  "},{"title":"Derivations​","type":1,"pageTitle":"Concepts","url":"concepts/#derivations","content":"A derivation is a collection that continuously derives its documents from transformations that are applied to one or more source collections. You can use derivations to map, reshape, and filter documents. They can also be used to tackle complex stateful streaming workflows, including joins and aggregations, and are not subject to the windowing and scaling limitations that are common to other systems. Learn more about derivations  "},{"title":"Schemas​","type":1,"pageTitle":"Concepts","url":"concepts/#schemas","content":"All collections in Flow have an associatedJSON schemaagainst which documents are validated every time they're written or read. Schemas are key to how Flow ensures the integrity of your data. Flow validates your documents to ensure that bad data doesn't make it into your collections — or worse, into downstream data products! Flow pauses catalog tasks when documents don't match the collection schema, alerting you to the mismatch and allowing you to fix it before it creates a bigger problem. "},{"title":"Constraints​","type":1,"pageTitle":"Concepts","url":"concepts/#constraints","content":"JSON schema is a flexible standard for representing structure, invariants, and other constraints over your documents. Schemas can be very permissive, highly exacting, or somewhere in between. JSON schema goes far beyond checking basic document structure. It also supports conditionals and invariants like &quot;I expect all items in this array to be unique&quot;, or &quot;this string must be an email&quot;, or &quot;this integer must be between a multiple of 10 and in the range 0-100&quot;. "},{"title":"Projections​","type":1,"pageTitle":"Concepts","url":"concepts/#projections","content":"Flow leverages your JSON schemas to produce other types of schemas as needed, such as TypeScript types and SQL CREATE TABLE statements. In many cases these projections provide comprehensive end-to-end type safety of Flow catalogs and their TypeScript transformations, all statically verified when the catalog is built. "},{"title":"Reductions​","type":1,"pageTitle":"Concepts","url":"concepts/#reductions","content":"Flow collections have a defined key, which is akin to a database primary key declaration and determines how documents of the collection are grouped. When a collection is materialized into a database table, its key becomes the SQL primary key of the materialized table. This of course raises the question: what happens if multiple documents of a given key are added to a collection? You might expect that the last-written document is the effective document for that key. This &quot;last write wins&quot; treatment is how comparable systems behave, and is also Flow's default. Flow also offers schema extensionsthat give you substantially more control over how documents are combined and reduced.reduce annotations let you deeply merge documents, maintain running counts, and achieve other complex aggregation behaviors. "},{"title":"Key strategies​","type":1,"pageTitle":"Concepts","url":"concepts/#key-strategies","content":"Reduction annotations change the common patterns for how you think about collection keys. Suppose you are building a reporting fact table over events of your business. Today you would commonly consider a unique event ID to be its natural key. You would load all events into your warehouse and perform query-time aggregation. When that becomes too slow, you periodically refresh materialized views for fast-but-stale queries. With Flow, you instead use a collection key of your fact table dimensions, and use reduce annotations to define your metric aggregations. A materialization of the collection then maintains a database table which is keyed on your dimensions, so that queries are both fast and up to date. Learn more about schemas  "},{"title":"Tasks​","type":1,"pageTitle":"Concepts","url":"concepts/#tasks","content":"Captures, derivations, and materializations are collectively referred to as catalog tasks. They are the &quot;active&quot; components of a catalog, each running continuously and reacting to documents as they become available. Collections, by way of comparison, are inert. They reflect data at rest, and are acted upon by catalog tasks: A capture adds documents to a collection pulled from a source endpoint.A derivation updates a collection by applying transformations to other collections.A materialization reacts to changes of a collection to update a destination endpoint. "},{"title":"Task shards​","type":1,"pageTitle":"Concepts","url":"concepts/#task-shards","content":"Task shards are the unit of execution for a catalog task. A single task can have many shards, which allow the task to scale across many machines to achieve more throughput and parallelism. Shards are created and managed by the Flow runtime. Each shard represents a slice of the overall work of the catalog task, including its processing status and associated internal checkpoints. Catalog tasks are created with a single shard, which can be repeatedly subdivided at any time — with no downtime — to increase the processing capacity of the task. Learn more about shards  "},{"title":"Endpoints​","type":1,"pageTitle":"Concepts","url":"concepts/#endpoints","content":"Endpoints are the external systems that you connect using Flow. All kinds of systems can be endpoints: databases, key/value stores, streaming pub/sub systems, SaaS APIs, and cloud storage locations. Captures pull or ingest data from an endpoint, while materializations push data into an endpoint. There's an essentially unbounded number of different systems and APIs to which Flow might need to capture or materialize data. Rather than attempt to directly integrate them all, Flow's runtime communicates with endpoints through plugin connectors. "},{"title":"Resources​","type":1,"pageTitle":"Concepts","url":"concepts/#resources","content":"An endpoint resource is an addressable collection of data within an endpoint. The exact meaning of a resource is up to the endpoint and its connector. For example: Resources of a database endpoint might be its individual tables.Resources of a Kafka cluster might be its topics.Resources of a SaaS connector might be its various API feeds. "},{"title":"Connectors​","type":1,"pageTitle":"Concepts","url":"concepts/#connectors","content":"There are lots of potential endpoints where you want to work with data. Though Flow is a unified platform for data synchronization, it's impractical for any single company — Estuary included — to provide an integration for every possible endpoint in the growing landscape of data solutions. Connectors are plugin components that bridge the gap between Flow’s runtime and the various endpoints from which you capture or materialize data. They're packaged as Docker images, each encapsulating the details of working with a particular kind of endpoint. The connector then interacts with Flow's runtime through common and open protocols for configuration, introspection of endpoint resources, and to coordinate the movement of data into and out of the endpoint. Crucially, this means Flow doesn't need to know about new types of endpoint ahead of time: so long as a connector is available Flow can work with the endpoint, and it's relatively easy to build a connector yourself. "},{"title":"Discovery​","type":1,"pageTitle":"Concepts","url":"concepts/#discovery","content":"Connectors offer discovery APIs for understanding how a connector should be configured, and what resources are available within an endpoint. Flow works with connector APIs to provide a guided discovery workflow which makes it easy to configure the connector, and select from a menu of available endpoint resources you can capture. Learn more about endpoints and connectors  "},{"title":"Tests​","type":1,"pageTitle":"Concepts","url":"concepts/#tests","content":"You use tests to verify the end-to-end behavior of your collections and derivations. A test is a sequence of ingestion or verification steps. Ingestion steps ingest one or more document fixtures into a collection, and verification steps assert that the contents of another derived collection match a test expectation. Learn more about tests  "},{"title":"Storage mappings​","type":1,"pageTitle":"Concepts","url":"concepts/#storage-mappings","content":"Flow collections use cloud storage buckets for the durable storage of data. Storage mappings define how Flow maps your various collections into your storage buckets and prefixes. Learn more about storage mappings  "},{"title":"flowctl​","type":1,"pageTitle":"Concepts","url":"concepts/#flowctl","content":"flowctl is Flow's command-line interface. With flowctl, developers can work directly on active catalogs and drafts created in the Flow webapp. They can develop locally, test more flexibly, and collaboratively refine catalogs. Learn more about flowctl "},{"title":"Projections","type":0,"sectionRef":"#","url":"concepts/advanced/projections/","content":"","keywords":""},{"title":"Logical partitions​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#logical-partitions","content":"Projections can also be used to logically partition a collection, specified as a longer-form variant of a projection definition: collections: acmeCo/user-sessions: schema: session.schema.yaml key: [/user/id, /timestamp] projections: country: location: /country partition: true device: location: /agent/type partition: true network: location: /agent/network partition: true  Logical partitions isolate the storage of documents by their differing values for partitioned fields. Flow extracts partitioned fields from each document, and every unique combination of partitioned fields is a separate logical partition. Every logical partition has one or more physical partitionsinto which their documents are written, which in turn controls how files are arranged within cloud storage. For example, a document of &quot;acmeCo/user-sessions&quot; like: {&quot;country&quot;: &quot;CA&quot;, &quot;agent&quot;: {&quot;type&quot;: &quot;iPhone&quot;, &quot;network&quot;: &quot;LTE&quot;}, ...}  Might produce files in cloud storage like: s3://bucket/example/sessions/country=CA/device=iPhone/network=LTE/pivot=00/utc_date=2020-11-04/utc_hour=16/&lt;name&gt;.gz  info country, device, and network together identify a logical partition, while pivot identifies a physical partition.utc_date and utc_hour is the time at which the journal fragment was created. Learn more about physical partitions. "},{"title":"Partition selectors​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#partition-selectors","content":"When reading from a collection, Flow catalog entities like derivations, materializations, and tests can provide a partition selector, which identifies the subset of partitions that should be read from a source collection: # Partition selectors are included as part of a larger entity, # such as a derivation or materialization. partitions: # `include` selects partitioned fields and corresponding values that # must be matched in order for a partition to be processed. # All of the included fields must be matched. # Default: All partitions are included. type: object include: # Include partitions from North America. country: [US, CA] # AND where the device is a mobile phone. device: [iPhone, Android] # `exclude` selects partitioned fields and corresponding values which, # if matched, exclude the partition from being processed. # A match of any of the excluded fields will exclude the partition. # Default: No partitions are excluded. type: object exclude: # Skip sessions which were over a 3G network. network: [&quot;3G&quot;]  Partition selectors are efficient as they allow Flow to altogether avoid reading documents that aren’t needed. "},{"title":"Task shards","type":0,"sectionRef":"#","url":"concepts/advanced/shards/","content":"","keywords":""},{"title":"Shard splits​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#shard-splits","content":"When a task is first created, it is initialized with a single shard. Later and as required, a shard can be split into two shards. Once initiated, the split may require up to a few minutes to complete, but it doesn't require downtime and the selected shard continues to run until the split occurs. This process can be repeated as needed until your required throughput is achieved. TODO This section is incomplete. See flowctl shards split --help for further details. "},{"title":"Recovery logs​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#recovery-logs","content":"info Shard stores and associated states are transparent to you, the Flow user. This section is informational only, to provide a sense of how Flow works. All task shards have associated state, which is managed in the shard's store. Capture tasks must track incremental checkpoints of their endpoint connectors.Derivation tasks manage a potentially very large index of registers, as well as read checkpoints of sourced collection journals.Materialization tasks track incremental checkpoints of their endpoint connectors, as well as read checkpoints of sourced collection journals. Shard stores userecovery logsto replicate updates and implement transaction semantics. Recovery logs are regular journals, but hold binary data and are not intended for direct use. However, they can hold your user data. Recovery logs of derivations hold your derivation register values. Recovery logs are stored in your cloud storage bucket, and must have a configured storage mapping. "},{"title":"Captures","type":0,"sectionRef":"#","url":"concepts/captures/","content":"","keywords":""},{"title":"Pull captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#pull-captures","content":"Pull captures pull documents from an endpoint using a connector: # A set of captures to include in the catalog. # Optional, type: object captures: # The name of the capture. acmeCo/example/source-s3: # Endpoint defines how to connect to the source of the capture. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image which implements the capture connector. image: ghcr.io/estuary/source-s3:dev # File which provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how collections are populated from the data source. # A capture may bind multiple resources to different collections. # Required, type: array bindings: - # The target collection to capture into. # This may be defined in a separate, imported catalog source file. # Required, type: string target: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and capture a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: stream: a-bucket/and-prefix # syncMode should be set to incremental for all Estuary connectors syncMode: incremental - target: acmeCo/example/another-collection resource: stream: a-bucket/another-prefix syncMode: incremental  "},{"title":"Estuary sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#estuary-sources","content":"Estuary builds and maintains many real-time connectors for various technology systems, such as database change data capture (CDC) connectors. See the source connector reference documentation. "},{"title":"Airbyte sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#airbyte-sources","content":"Flow also natively supports Airbyte source connectors. These connectors tend to focus on SaaS APIs, and do not offer real-time streaming integrations. Flow runs the connector at regular intervals to capture updated documents. Airbyte source connectors are independently reviewed and sometime updated for compatibility with Flow. Estuary's source connectors documentation includes actively supported Airbyte connectors. A full list of Airbyte's connectors is available at Airbyte docker hub. If you see a connector you'd like to prioritize for access in the Flow web app, contact us. "},{"title":"Discovery​","type":1,"pageTitle":"Captures","url":"concepts/captures/#discovery","content":"Flow offers a CLI tool flowctl discover --image connector/image:tag which provides a guided workflow for creating a correctly configured capture. "},{"title":"Push captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#push-captures","content":"Push captures expose an endpoint to which documents may be pushed using a supported ingestion protocol: captures: # The name of the capture. acmeCo/example/webhook-ingest: endpoint: # This endpoint is an ingestion. ingest: {} bindings: - # The target collection to capture into. target: acmeCo/example/webhooks # The resource configures the specific behavior of the ingestion endpoint. resource: name: webhooks  caution Push captures are under development. Estuary intends to offer Webhook, Websocket, and Kafka-compatible APIs for capturing into collections. Specification details are likely to exist. "},{"title":"Collections","type":0,"sectionRef":"#","url":"concepts/collections/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Collections","url":"concepts/collections/#specification","content":"Collections are expressed within a Flow catalog specification: # A set of collections to include in the catalog. # Optional, type: object collections: # The unique name of the collection. acmeCo/products/anvils: # The schema of the collection, against which collection documents # are validated. This may be an inline definition or a relative URL # reference. # Required, type: string (relative URL form) or object (inline form) schema: anvils.schema.yaml # The key of the collection, specified as JSON pointers of one or more # locations within collection documents. If multiple fields are given, # they act as a composite key, equivalent to a SQL table PRIMARY KEY # with multiple table columns. # Required, type: array key: [/product/id] # Projections and logical partitions for this collection. # See the &quot;Projections&quot; concept page to learn more. # Optional, type: object projections: # Derivation that builds this collection from others through transformations. # See the &quot;Derivations&quot; concept page to learn more. # Optional, type: object derivation:  "},{"title":"Schemas​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schemas","content":"Every Flow collection must declare a schema, and will never accept documents that do not validate against the schema. This helps ensure the quality of your data products and the reliability of your derivations and materializations. Schema specifications are flexible: yours could be exactingly strict, extremely permissive, or somewhere in between. Schemas may either be declared inline, or provided as a reference to a file. References can also include JSON pointers as a URL fragment to name a specific schema of a larger schema document: InlineFile referenceReference with pointer collections: acmeCo/collection: schema: type: object required: [id] properties: id: string key: [/id]  Learn more about schemas "},{"title":"Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#keys","content":"Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. For example: flow.yamlschema.yaml collections: acmeCo/users: schema: schema.yaml key: [/userId]  Suppose the following JSON documents are captured into acmeCo/users: {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;William&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;}  As its key is [/userId], a materialization of the collection into a database table will reduce to a single row: userId | name 1 | Will  If its key were instead [/name], there would be two rows in the table: userId | name 1 | Will 1 | William  "},{"title":"Schema restrictions​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schema-restrictions","content":"Keyed document locations may be of a limited set of allowed types: booleanintegerstring Excluded types are: arraynullobjectFractional number Keyed fields also must always exist in collection documents. Flow performs static inference of the collection schema to verify the existence and types of all keyed document locations, and will report an error if the location could not exist, or could exist with the wrong type. Flow itself doesn't mind if a keyed location could have multiple types, so long as they're each of the allowed types: an integer or string for example. Some materialization connectors, however, may impose further type restrictions as required by the endpoint. For example, SQL databases do not support multiple types for a primary key. "},{"title":"Composite Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#composite-keys","content":"A collection may have multiple locations which collectively form a composite key. This can include locations within nested objects and arrays: flow.yamlschema.yaml collections: acmeCo/compound-key: schema: schema.yaml key: [/foo/a, /foo/b, /foo/c/0, /foo/c/1]  "},{"title":"Key behaviors​","type":1,"pageTitle":"Collections","url":"concepts/collections/#key-behaviors","content":"A collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. Flow also performs opportunistic local reductions over windows of documents (also called &quot;combines&quot;) to improve its performance and reduce the volumes of data at each processing stage. An important subtlety is that the underlying storage of a collection will potentially retain many documents of a given key. In the acmeCo/users example, each of the &quot;Will&quot; or &quot;William&quot; variants is likely represented in the collection's storage — so long as they didn't arrive so closely together that they were locally combined by Flow. If desired, a derivation could re-key the collection on [/userId, /name] to materialize the various /names seen for a /userId. This property makes keys less lossy than they might otherwise appear, and it is generally good practice to chose a key that reflects how you wish to query a collection, rather than an exhaustive key that's certain to be unique for every document. "},{"title":"Projections​","type":1,"pageTitle":"Collections","url":"concepts/collections/#projections","content":"Projections are named locations within a collection document that may be used for logical partitioning or directly exposed to databases into which collections are materialized. Many projections are automatically inferred from the collection schema. The projections stanza can be used to provide additional projections, and to declare logical partitions: collections: acmeCo/products/anvils: schema: anvils.schema.yaml key: [/product/id] # Projections and logical partitions for this collection. # Keys name the unique projection field, and values are its JSON Pointer # location within the document and configure logical partitioning. # Optional, type: object projections: # Short form: define a field &quot;product_id&quot; with document pointer /product/id. product_id: &quot;/product/id&quot; # Long form: define a field &quot;metal&quot; with document pointer /metal_type # which is a logical partition of the collection. metal: location: &quot;/metal_type&quot; partition: true  Learn more about projections. "},{"title":"Storage​","type":1,"pageTitle":"Collections","url":"concepts/collections/#storage","content":"Collections are real-time data lakes. Historical documents of the collection are stored as an organized layout of regular JSON files in your cloud storage bucket. Reads of that history are served by directly reading files from your bucket. Your storage mappingsdetermine how Flow collections are mapped into your cloud storage buckets. Unlike a traditional data lake, however, it's very efficient to read collection documents as they are written. Derivations and materializations that source from a collection are notified of its new documents within milliseconds of their being published. Learn more about journals, which provide storage for collections "},{"title":"Connectors","type":0,"sectionRef":"#","url":"concepts/connectors/","content":"","keywords":""},{"title":"Why an open connector architecture?​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#why-an-open-connector-architecture","content":"Historically, data platforms have directly implemented integrations to external systems with which they interact. Today, there are simply so many systems and APIs that companies use, that it’s not feasible for a company to provide all possible integrations. Users are forced to wait indefinitely while the platform works through their prioritized integration list. An open connector architecture removes Estuary — or any company — as a bottleneck in the development of integrations. Estuary contributes open-source connectors to the ecosystem, and in turn is able to leverage connectors implemented by others. Users are empowered to write their own connectors for esoteric systems not already covered by the ecosystem. Furthermore, implementing a Docker-based community specification brings other important qualities to Estuary connectors: Cross-platform interoperability between Flow, Airbyte, and any other platform that supports the protocolThe abilities to write connectors in any language and run them on any machineBuilt-in solutions for version management (through image tags) and distributionThe ability to integrate connectors from different sources at will, without the centralized control of a single company, thanks to container image registries "},{"title":"Using connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#using-connectors","content":"Most — if not all — of your data flows will use at least one connector. Connector configuration is an important aspect of catalog configuration, and when you deploy a catalog, you're also deploying all the connectors it uses. You can interact with connectors using either the Flow web application or the flowctl CLI. "},{"title":"Flow web application​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#flow-web-application","content":"The Flow web application is designed to assist you with connector configuration and deployment. It's a completely no-code experience, but it's compatible with Flow's command line tools, discussed below. When you add a capture or materialization in the Flow web app, choose the desired data system from the Connector drop-down menu. The required fields for the connector appear below the drop-down. When you fill in the fields and click Test Config, Flow automatically &quot;discovers&quot; the data streams or tables — known as resources — associated with the endpoint system. From there, you can refine the configuration, save, and publish the resulting catalog. "},{"title":"GitOps and flowctl​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#gitops-and-flowctl","content":"From a technical perspective, connectors are packaged as Open Container (Docker) images, and can be tagged, and pulled usingDocker Hub,GitHub Container registry, or any other public image registry provider. To interface with a connector, the Flow runtime needs to know: The specific image to use, through an image name such as ghcr.io/estuary/source-postgres:dev. Notice that the image name also conveys the specific image registry and version tag to use. Endpoint configuration such as a database address and account, with meaning that is specific to the connector. Resource configuration such as a specific database table to capture, which is also specific to the connector. To integrate a connector within your dataflow, You define all three components within your catalog specification. The web application is intended to generate the catalog specification YAML file. From there, you can use flowctl to refine it in your local environment. It's also possible to manually write your catalog YAML files, but this isn't the recommended workflow. materializations: acmeCo/postgres-views: endpoint: connector: # 1: Provide the image that implements your endpoint connector. # The `dev` tag uses the most recent version (the web app chooses this tag automatically) image: ghcr.io/estuary/materialize-postgres:dev # 2: Provide endpoint configuration that the connector requires. config: host: localhost password: password database: postgres user: postgres port: 5432 bindings: - source: acmeCo/products/anvils # 3: Provide resource configuration for the binding between the Flow # collection and the endpoint resource. This connector interfaces # with a SQL database and its resources are database tables. Here, # we provide a table to create and materialize which is bound to the # `acmeCo/products/anvils` source collection. resource: table: anvil_products # Multiple resources can be configured through a single connector. # Bind additional collections to tables as part of this connector instance: - source: acmeCo/products/TNT resource: table: tnt_products - source: acmeCo/customers resource: table: customers  Configuration​ Connectors interface with external systems and universally require endpoint configuration, such as a database hostname or account credentials, which must be provided to the connector for it to function. When directly working with catalog source files, you have the option of inlining the configuration into your connector or storing it in separate files: InlineReferenced file my.flow.yaml materializations: acmeCo/postgres-views: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: host: localhost password: password database: postgres user: postgres port: 5432 bindings: []  Storing configuration in separate files serves two important purposes: Re-use of configuration across multiple captures or materializationsThe ability to protect sensitive credentials "},{"title":"Protecting secrets​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#protecting-secrets","content":"Most endpoint systems require credentials of some kind, such as a username or password. Directly storing secrets in files that are versioned in Git is poor practice. Similarly, sensitive credentials should be protected while not in use within Flow's runtime as well. The only time a credential needs to be directly accessed is when it's required by Flow's runtime for the purposes of instantiating the connector. Flow integrates with Mozilla’s sops tool, which can encrypt and protect credentials within a GitOps-managed catalog. Flow's runtime similarly stores a sops-protected configuration in its encrypted form, and decrypts it only when invoking a connector on the user’s behalf. sops, short for “Secrets Operations,” is a tool that encrypts the values of a JSON or YAML document against a key management system (KMS) such as Google Cloud Platform KMS, Azure Key Vault, or Hashicorp Vault. Encryption or decryption of a credential with sops is an active process: it requires that the user (or the Flow runtime identity) have a current authorization to the required KMS, and creates a request trace which can be logged and audited. It's also possible to revoke access to the KMS, which immediately and permanently removes access to the protected credential. Example: Protect a configuration​ Suppose you're given a connector configuration: config.yaml host: my.hostname password: &quot;this is sensitive!&quot; user: my-user  You can protect it using a Google KMS key that you own: # Login to Google Cloud and initialize application default credentials used by `sops`. $ gcloud auth application-default login # Use `sops` to re-write the configuration document in place, protecting its values. $ sops --encrypt --in-place --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping each value in an encrypted envelope and adding a sops metadata section: config.yaml host: ENC[AES256_GCM,data:K/clly65pThTg2U=,iv:1bNmY8wjtjHFBcXLR1KFcsNMGVXRl5LGTdREUZIgcEU=,tag:5GKcguVPihXXDIM7HHuNnA==,type:str] password: ENC[AES256_GCM,data:IDDY+fl0/gAcsH+6tjRdww+G,iv:Ye8st7zJ9wsMRMs6BoAyWlaJeNc9qeNjkkjo6BPp/tE=,tag:EPS9Unkdg4eAFICGujlTfQ==,type:str] user: ENC[AES256_GCM,data:w+F7MMwQhw==,iv:amHhNCJWAJnJaGujZgjhzVzUZAeSchEpUpBau7RVeCg=,tag:62HguhnnSDqJdKdwYnj7mQ==,type:str] sops: # Some items omitted for brevity: gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T15:49:45Z&quot; enc: CiQAW8BC2GDYWrJTp3ikVGkTI2XaZc6F4p/d/PCBlczCz8BZiUISSQCnySJKIptagFkIl01uiBQp056c lastmodified: &quot;2022-01-05T15:49:45Z&quot; version: 3.7.1  You then use this config.yaml within your Flow catalog. The Flow runtime knows that this document is protected by sopswill continue to store it in its protected form, and will attempt a decryption only when invoking a connector on your behalf. If you need to make further changes to your configuration, edit it using sops config.yaml. It's not required to provide the KMS key to use again, as sops finds it within its metadata section. important When deploying catalogs onto the managed Flow runtime, you must grant access to decrypt your GCP KMS key to the Flow runtime service agent, which is: flow-258@helpful-kingdom-273219.iam.gserviceaccount.com  Example: Protect portions of a configuration​ Endpoint configurations are typically a mix of sensitive and non-sensitive values. It can be cumbersome when sops protects an entire configuration document as you lose visibility into non-sensitive values, which you might prefer to store as cleartext for ease of use. You can use the encrypted-suffix feature of sops to selectively protect credentials: config.yaml host: my.hostname password_sops: &quot;this is sensitive!&quot; user: my-user  Notice that password in this configuration has an added _sops suffix. Next, encrypt only values which have that suffix: $ sops --encrypt --in-place --encrypted-suffix &quot;_sops&quot; --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping only values having a &quot;_sops&quot; suffix and adding its sops metadata section: config.yaml host: my.hostname password_sops: ENC[AES256_GCM,data:dlfidMrHfDxN//nWQTPCsjoG,iv:DHQ5dXhyOOSKI6ZIzcUM67R6DD/2MSE4LENRgOt6GPY=,tag:FNs2pTlzYlagvz7vP/YcIQ==,type:str] user: my-user sops: # Some items omitted for brevity: encrypted_suffix: _sops gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T16:06:36Z&quot; enc: CiQAW8BC2Au779CGdMFUjWPhNleCTAj9rL949sBvPQ6eyAC3EdESSQCnySJKD3eWX8XrtrgHqx327 lastmodified: &quot;2022-01-05T16:06:37Z&quot; version: 3.7.1  You then use this config.yaml within your Flow catalog. Flow looks for and understands the encrypted_suffix, and will remove this suffix from configuration keys before passing them to the connector. "},{"title":"Connecting to endpoints on secure networks​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#connecting-to-endpoints-on-secure-networks","content":"In some cases, your source or destination endpoint may be within a secure network, and you may not be able to allow direct access to its port due to your organization's security policy. SHH tunneling, or port forwarding, provides a means for Flow to access the port indirectly through an SSH server. SSH tunneling is universally supported by Estuary's connectors. To set up and configure the SSH server, see the guide. Then, add the appropriate properties when you define the capture or materialization in the Flow web app, or add the networkTunnel stanza directly to the YAML, as shown below. Sample​ materialize-postgres-ssh-tunnel.flow.yaml materializations: acmeCo/postgres-materialize-ssh: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: # When using a proxy like SSH tunneling, set to localhost host: localhost # Specify an open port on your local machine to connect to the proxy. port: 15432 database: flow user: flow_user password: secret networkTunnel: sshForwarding: # Port on the local machine from which you'll connect to the SSH server. # If a port is specified elsewhere in the connector configuration, it must match. localPort: 15432 # Host or IP address of the final endpoint to which you’ll # connect via tunneling from the SSH server forwardHost: 127.0.0.1 # Port of the final endpoint to which you’ll connect via # tunneling from the SSH server. forwardPort: 5432 # Location of the remote SSH server that supports tunneling. # Formatted as ssh://hostname[:port]. sshEndpoint: ssh://198.21.98.1 # Username to connect to the SSH server. user: sshUser # Private key to connect to the SSH server, formatted as multiline plaintext. # Use the YAML literal block style with the indentation indicator. # See https://yaml-multiline.info/ for details. privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: []  "},{"title":"Available connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#available-connectors","content":"Learn about available connectors in the reference section "},{"title":"flowctl","type":0,"sectionRef":"#","url":"concepts/flowctl/","content":"","keywords":""},{"title":"Installation​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#installation","content":"Beta Simplified installation is coming soon. For now, you can contact Estuary support for access. "},{"title":"flowctl subcommands​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#flowctl-subcommands","content":"flowctl includes several top-level subcommands representing different functional areas. Each of these include multiple nested subcommands. Important top-level flowctl subcommands are described below. auth allows you to authenticate your development session in your local development environment. It's also how you provision Flow roles and users. Learn more about authentication. catalog allows you to work with your organization's current active catalog. You can investigate the current deployment, or add its specification to a draft, where you can develop it further. draft allows you to work with draft catalog specifications. You can create, test, develop locally, and then publish, or deploy, them. You can access full documentation of all flowctl subcommands from the command line by passing the --help or -h flag, for example: flowctl --help lists top-level flowctl subcommands flowctl catalog --help lists subcommands of catalog "},{"title":"Build directory​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#build-directory","content":"When building Flow catalogs, flowctl uses a build directorywhich is typically the root directory of your project, and is controlled by flag --directory. Within this directory, flowctl creates a number of files and sub-directories. Except where noted, it's recommended that these outputs be committed within your GitOps project. flow_generated/: ♻ Directory of generated files, including TypeScript classes and interfaces. See TypeScript code generation. dist/: ♻ Holds JavaScript and source map files produced during TypeScript compilation.dist/ should be added to your .gitignore. node_modules/: ♻ Location where npm will download your TypeScript dependencies.node_modules/ should be added to your .gitignore. package.json and package-lock.json: ♻ Files used by npm to manage dependencies and your catalog's associated JavaScript project. You may customize package.json, but its dependencies stanza will be overwritten by thenpmDependenciesof your catalog source files. .eslintrc.js: ⚓ Configures the TypeScript linter that's run as part of the catalog build process. Linting supplements TypeScript compilation to catch additional common mistakes and errors at build time. .prettierrc.js: ⚓ Configures the formatter that's used to format your TypeScript files. Legend ⚓: Generated only if it does not exist. Never modified or deleted by flowctl. ♻: flowctl re-generates and overwrites contents. "},{"title":"TypeScript code generation​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#typescript-code-generation","content":"TypeScript files are used in the Flow catalog both as part of the automatic build process, and to define lambdas functions for derivations, which requires your input. As part of the catalog build process, Flow translates yourschemasinto equivalent TypeScript types on your behalf. These definitions live within flow_generated/ in your catalog build directory, and are frequently over-written by invocations of flowctl. Files in this subdirectory are human-readable and stable. You may want to commit them as part of a GitOps-managed project, but this isn't required. Whenever you define a derivation that uses a lambda, you must define the lambda in an accompanying TypeScript module, and reference that module in the derivation's definition. To facilitate this, you can generate a stub of the module using flowctl typescript generateand simply write the function bodies.Learn more about this workflow. If a TypeScript module exists, flowctl will never overwrite it, even if you update or expand your catalog sources such that the required interfaces have changed. "},{"title":"Imports","type":0,"sectionRef":"#","url":"concepts/import/","content":"","keywords":""},{"title":"Fetch behavior​","type":1,"pageTitle":"Imports","url":"concepts/import/#fetch-behavior","content":"Flow resolves, fetches, and validates all imports during the catalog build process, and then includes their fetched contents within the built catalog. The built catalog is thus a self-contained snapshot of all resourcesas they were at the time the catalog was built. This means it's both safe and recommended to directly reference an authoritative source of a resource, such as a third-party JSON schema. It will be fetched and verified only at catalog build time, and thereafter that fetched version will be used for execution, regardless of whether the authority URL itself later changes or errors. "},{"title":"Import types​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-types","content":"Almost always, the import stanza is used to import other Flow catalog source files. This is the default when given a string path: import: - path/to/source/catalog.flow.yaml  A long-form variant also accepts a content type of the imported resource: import: - url: path/to/source/catalog.flow.yaml contentType: CATALOG  Other permitted content types include JSON_SCHEMA, but these are not typically used and are needed only for advanced use cases. "},{"title":"JSON Schema $ref​","type":1,"pageTitle":"Imports","url":"concepts/import/#json-schema-ref","content":"Certain catalog entities, like collections, commonly reference JSON schemas. It's not necessary to explicitly add these to the import section; they are automatically resolved and treated as an import. You can think of this as an analog to the JSON Schema $ref keyword, which is used to reference a schema that may be contained in another file. The one exception is schemas that use the $id keyword at their root to define an alternative canonical URL. In this case, the schema must be referenced through its canonical URL, and then explicitly added to the import section with JSON_SCHEMA content type. "},{"title":"Importing derivation resources​","type":1,"pageTitle":"Imports","url":"concepts/import/#importing-derivation-resources","content":"In many cases, derivations in your catalog will need to import resources. Usually, these are Typescript modules that define the lambda functions of a transformation, and, in certain cases, the NPM dependencies of that Typescript module. These imports are specified in the derivation specification, not in the import section of the catalog spec. For more information, see Derivation specification and creating TypeScript modules. "},{"title":"Import paths​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-paths","content":"If a catalog source file foo.flow.yaml references a collection in bar.flow.yaml, for example as a target of a capture, there must be an import path where either foo.flow.yamlimports bar.flow.yaml or vice versa. Import paths can be direct: graph LR; foo.flow.yaml--&gt;bar.flow.yaml; Or they can be indirect: graph LR; bar.flow.yaml--&gt;other.flow.yaml; other.flow.yaml--&gt;foo.flow.yaml; The sources must still have an import path even if referenced from a common parent. The following would not work: graph LR; parent.flow.yaml--&gt;foo.flow.yaml; parent.flow.yaml--&gt;bar.flow.yaml; These rules make your catalog sources more self-contained and less brittle to refactoring and reorganization. Consider what might otherwise happen if foo.flow.yamlwere imported in another project without bar.flow.yaml. "},{"title":"Materializations","type":0,"sectionRef":"#","url":"concepts/materialization/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#specification","content":"Materializations are expressed within a Flow catalog specification: # A set of materializations to include in the catalog. # Optional, type: object materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image which implements the materialization connector. image: ghcr.io/estuary/materialize-postgres:dev # File which provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how one or more collections map to materialized endpoint resources. # A single materialization may include many collections and endpoint resources, # each defined as a separate binding. # Required, type: object bindings: - # The source collection to materialize. # This may be defined in a separate, imported catalog source file. # Required, type: string source: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and materialize a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: # The materialize-postgres connector expects a `table` key # which names a table to materialize into. table: example_table  "},{"title":"Continuous materialized views​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#continuous-materialized-views","content":"Flow materializations are continuous materialized views. They maintain a representation of the collection within the endpoint system as a resource that is updated in near real-time. It's indexed on thecollection key. As the materialization runs, it ensures that all collection documents and their accumulated reductions are reflected in this managed endpoint resource. For example, consider a collection and its materialization:  collections: acmeCo/colors: key: [/color] schema: type: object required: [color, total] reduce: {strategy: merge} properties: color: {enum: [red, blue, purple]} total: type: integer reduce: {strategy: sum} materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/colors resource: { table: colors }  Suppose documents are periodically added to the collection: {&quot;color&quot;: &quot;red&quot;, &quot;total&quot;: 1} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 2} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 3}  Its materialization into a database table will have a single row for each unique color. As documents arrive in the collection, the row total is updated within the materialized table so that it reflects the overall count:  When you first declare a materialization, Flow back-fills the endpoint resource with the historical documents of the collection. Once caught up, Flow applies new collection documents using incremental and low-latency updates. As collection documents arrive, Flow: Reads previously materialized documents from the endpoint for the relevant keysReduces new documents into these read documentsWrites updated documents back into the endpoint resource, indexed by their keys Flow does not keep separate internal copies of collection or reduction states, as some other systems do. The endpoint resource is the one and only place where state &quot;lives&quot; within a materialization. This makes materializations very efficient and scalable to operate. They are able to maintain very large tables stored in highly scaled storage systems like OLAP warehouses, BigTable, or DynamoDB. "},{"title":"Projected fields​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#projected-fields","content":"Many systems are document-oriented and can directly work with collections of JSON documents. Others systems are table-oriented and require an up-front declaration of columns and types to be most useful, such as a SQL CREATE TABLE definition. Flow uses collection projections to relate locations within a hierarchical JSON document to equivalent named fields. A materialization can in turn select a subset of available projected fields where, for example, each field becomes a column in a SQL table created by the connector. It would be tedious to explicitly list projections for every materialization, though you certainly can if desired. Instead, Flow and the materialization connector negotiate a recommended field selection on your behalf, which can be fine-tuned. For example, a SQL database connector will typically require that fields comprising the primary key be included, and will recommend that scalar values be included, but will by default exclude document locations that don't have native SQL representations, such as locations which can have multiple JSON types or are arrays or maps. materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/example/collection resource: { table: example_table } # Select (or exclude) projections of the collection for materialization as fields. # If not provided, the recommend fields of the endpoint connector are used. # Optional, type: object fields: # Whether to include fields that are recommended by the endpoint connector. # If false, then fields can still be added using `include`. # Required, type: boolean recommended: true # Fields to exclude. This is useful for deselecting a subset of recommended fields. # Default: [], type: array exclude: [myField, otherField] # Fields to include. This can supplement recommended fields, or can # designate explicit fields to use if recommended fields are disabled. # # Values of this map are used to customize connector behavior on a per-field basis. # They are passed directly to the connector and are not interpreted by Flow. # Consult your connector's documentation for details of what customizations are available. # This is an advanced feature and is not commonly used. # # default: {}, type: object include: {goodField: {}, greatField: {}}  "},{"title":"Partition selectors​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#partition-selectors","content":"Partition selectors let you materialize only a subset of a collection that haslogical partitions. For example, you might have a large collection that is logically partitioned on each of your customers: collections: acmeCo/anvil/orders: key: [/id] schema: orders.schema.yaml projections: customer: location: /order/customer partition: true  A large customer asks if you can provide an up-to-date accounting of their orders. This can be accomplished with a partition selector: materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/anvil/orders resource: { table: coyote_orders } # Process partitions where &quot;Coyote&quot; is the customer. partitions: include: customer: [Coyote]  Learn more about partition selectors. "},{"title":"SQLite endpoint​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#sqlite-endpoint","content":"In addition to materialization connectors, Flow offers a built-in SQLite endpoint for local testing and development. SQLite is not suitable for materializations running within a managed data plane. materializations: acmeCo/example/database-views: endpoint: # A SQLite endpoint is specified using `sqlite` instead of `connector`. sqlite: # The SQLite endpoint requires the `path` of the SQLite database to use, # specified as a file path. It may include URI query parameters; # See: https://www.sqlite.org/uri.html and https://github.com/mattn/go-sqlite3#connection-string path: example/database.sqlite?_journal_mode=WAL  "},{"title":"Backpressure​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#backpressure","content":"Flow processes updates in transactions, as quickly as the endpoint can handle them. This might be milliseconds in the case of a fast key/value store, or many minutes in the case of an OLAP warehouse. If the endpoint is also transactional, Flow integrates its internal transactions with those of the endpoint for integrated end-to-end “exactly once” semantics. The materialization is sensitive to back pressure from the endpoint. As a database gets busy, Flow adaptively batches and combines documents to consolidate updates: In a given transaction, Flow reduces all incoming documents on the collection key. Multiple documents combine and result in a single endpoint read and write during the transaction.As a target database becomes busier or slower, transactions become larger. Flow does more reduction work within each transaction, and each endpoint read or write accounts for an increasing volume of collection documents. This allows you to safely materialize a collection with a high rate of changes into a small database, so long as the cardinality of the materialization is of reasonable size. "},{"title":"Delta updates​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#delta-updates","content":"As described above, Flow's standard materialization mechanism involves querying the target system for data state before reducing new documents directly into it. For these standard updates to work, the endpoint must be a stateful system, like a relational database. However, other systems — like Webhooks and Pub/Sub — may also be endpoints. None of these typically provide a state representation that Flow can query. They are write-only in nature, so Flow cannot use their endpoint state to help it fully reduce collection documents on their keys. Even some stateful systems are incompatible with Flow's standard updates due to their unique design and architecture. For all of these endpoints, Flow offers a delta-updates mode. When using delta updates, Flow does not attempt to maintain full reductions of each unique collection key. Instead, Flow locally reduces documents within each transaction (this is often called a &quot;combine&quot;), and then materializes onedelta document per key to the endpoint. In other words, when delta updates are used, Flow sends information about data changes by key, and further reduction is left up to the endpoint system. Some systems may reduce documents similar to Flow; others use a different mechanism; still others may not perform reductions at all. A given endpoint may support standard updates, delta updates, or both. This depends on the materialization connector. Expect that a connector will use standard updates only unless otherwise noted in its documentation. "},{"title":"Schemas","type":0,"sectionRef":"#","url":"concepts/schemas/","content":"","keywords":""},{"title":"JSON Schema​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#json-schema","content":"JSON Schemais an expressive open standard for defining the schema and structure of documents. Flow uses it for all schemas defined within a Flow catalog. JSON Schema goes well beyond basic type information and can modeltagged unions, recursion, and other complex, real-world composite types. Schemas can also define rich data validations like minimum and maximum values, regular expressions, dates, timestamps, email addresses, and other formats. Together, these features let schemas represent structure as well asexpectations and constraints that are evaluated and must hold true for every collection document before it’s added to the collection. They’re a powerful tool for ensuring end-to-end data quality: for catching data errors and mistakes early, before they can impact your production data products. "},{"title":"Generation​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#generation","content":"When capturing data from an external system, Flow can usually generate suitable JSON schemas on your behalf. Learn more about using connectors "},{"title":"Translations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#translations","content":"You must only provide Flow a model of a given dataset one time, as a JSON schema. Having done that, Flow leverages static inference over your schemas to perform many build-time validations of your catalog entities, helping you catch potential problems early. Schema inference is also used to provide translations into other schema flavors: Most projections of a collection are automatically inferred from its schema. Materializations use your projections to create appropriate representations in your endpoint system. A SQL connector will create table definitions with appropriate columns, types, and constraints.Flow generates TypeScript definitions from schemas to provide compile-time type checks of user lambda functions. These checks are immensely helpful for surfacing mismatched expectations around, for example, whether a field could ever be null or is misspelt — which, if not caught, might otherwise fail at runtime. "},{"title":"Annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#annotations","content":"The JSON Schema standard introduces the concept ofannotations, which are keywords that attach metadata to a location within a validated JSON document. For example, title and description can be used to annotate a schema with its meaning: properties: myField: title: My Field description: A description of myField  Flow extends JSON Schema with additional annotation keywords, which provide Flow with further instruction of how documents should be processed. What’s especially powerful about annotations is that they respond toconditionals within the schema. Consider a schema validating a positive or negative number: type: number oneOf: - exclusiveMinimum: 0 description: A positive number. - exclusiveMaximum: 0 description: A negative number. - const: 0 description: Zero.  Here, the activated description of this schema location depends on whether the integer is positive, negative, or zero. "},{"title":"Writing schemas​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#writing-schemas","content":"Your schema can be quite permissive or as strict as you wish. There are a few things to know, however. The top-level type must be object. Flow adds a bit of metadata to each of your documents under the _meta property, which can only be done with a top-level object. Any fields that are part of the collection's key must provably exist in any document that validates against the schema. Put another way, every document within a collection must include all of the fields of the collection's key, and the schema must guarantee that. For example, the following collection schema would be invalid because the id field, which is used as its key, is not required, so it might not actually exist in all documents: collections: acmeCo/whoops: schema: type: object required: [value] properties: id: {type: integer} value: {type: string} key: [/id]  To fix the above schema, change required to [id, value]. Learn more of how schemas can be expressed within collections. "},{"title":"Organization​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#organization","content":"JSON schema has a $ref keyword which is used to reference a schema stored elsewhere. Flow resolves $ref as a relative URL of the current file, and also supportsJSON fragment pointersfor referencing a specific schema within a larger schema document, such as ../my/widget.schema.yaml#/path/to/schema. It's recommended to use references in order to organize your schemas for reuse. $ref can also be used in combination with other schema keywords to further refine a base schema. Here's an example that uses references to organize and further tighten the constraints of a reused base schema: flow.yamlschemas.yaml collections: acmeCo/coordinates: key: [/id] schema: schemas.yaml#/definitions/coordinate acmeCo/integer-coordinates: key: [/id] schema: schemas.yaml#/definitions/integer-coordinate acmeCo/positive-coordinates: key: [/id] schema: # Compose a restriction that `x` &amp; `y` must be positive. $ref: schemas.yaml#/definitions/coordinate properties: x: {exclusiveMinimum: 0} y: {exclusiveMinimum: 0}  tip You can write your JSON schemas as either YAML or JSON across any number of files, all referenced from Flow catalog files or other schemas. Schema references are always resolved as URLs relative to the current file, but you can also use absolute URLs to a third-party schema likeschemastore.org. "},{"title":"Reductions​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reductions","content":"Flow collections have keys, and multiple documents may be added to collections that share a common key. When this happens, Flow will opportunistically merge all such documents into a single representative document for that key through a process known as reduction. Flow's default is simply to retain the most recent document of a given key, which is often the behavior that you're after. Schema reduce annotations allow for far more powerful behaviors. The Flow runtime performs reductions frequently and continuously to reduce the overall movement and cost of data transfer and storage. A torrent of input collection documents can often become a trickle of reduced updates that must be stored or materialized into your endpoints. info Flow never delays processing in order to batch or combine more documents, as some systems do (commonly known as micro-batches, or time-based polling). Every document is processed as quickly as possible, from end to end. Instead Flow uses optimistic transaction pipelining to do as much useful work as possible, while it awaits the commit of a previous transaction. This natural back-pressure affords plenty of opportunity for data reductions while minimizing latency. "},{"title":"reduce annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reduce-annotations","content":"Reduction behaviors are defined by reduceJSON schema annotationswithin your document schemas. These annotations provide Flow with the specific reduction strategies to use at your various document locations. If you're familiar with the map and reduce primitives present in Python, Javascript, and many other languages, this should feel familiar. When multiple documents map into a collection with a common key, Flow reduces them on your behalf by using your reduce annotations. Here's an example that sums an integer: type: integer reduce: { strategy: sum } # 1, 2, -1 =&gt; 2  Or deeply merges a map: type: object reduce: { strategy: merge } # {&quot;a&quot;: &quot;b&quot;}, {&quot;c&quot;: &quot;d&quot;} =&gt; {&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: &quot;d&quot;}  Learn more in thereductions strategiesreference documentation. Composition with conditionals​ Like any other JSON Schema annotation,reduce annotations respond to schema conditionals. Here we compose append and lastWriteWins strategies to reduce an appended array which can also be cleared: type: array oneOf: # If the array is non-empty, reduce by appending its items. - minItems: 1 reduce: { strategy: append } # Otherwise, if the array is empty, reset the reduced array to be empty. - maxItems: 0 reduce: { strategy: lastWriteWins } # [1, 2], [3, 4, 5] =&gt; [1, 2, 3, 4, 5] # [1, 2], [], [3, 4, 5] =&gt; [3, 4, 5] # [1, 2], [3, 4, 5], [] =&gt; []  Combining schema conditionals with annotations can be used to buildrich behaviors. "},{"title":"Storage mappings","type":0,"sectionRef":"#","url":"concepts/storage-mappings/","content":"","keywords":""},{"title":"Recovery logs​","type":1,"pageTitle":"Storage mappings","url":"concepts/storage-mappings/#recovery-logs","content":"Flow tasks — captures, derivations, and materializations — use recovery logs to durably store their processing context. Recovery logs are an opaque binary log, but may contain user data and are stored within the user’s buckets. They must have a defined storage mapping. The recovery logs of a task are always prefixed by recovery/, and a task named acmeCo/produce-TNT would require a storage mapping like: storageMappings: recovery/acmeCo/: stores: - provider: S3 bucket: acmeco-recovery  You may wish to use a separate bucket for recovery logs, distinct from the bucket where collection data is stored. Buckets holding collection data are free to use a bucket lifecycle policy to manage data retention; for example, to remove data after six months. This is not true of buckets holding recovery logs. Flow prunes data from recovery logs once it is no longer required. warning Deleting data from recovery logs while it is still in use can cause Flow processing tasks to fail permanently. "},{"title":"Tests","type":0,"sectionRef":"#","url":"concepts/tests/","content":"","keywords":""},{"title":"Ingest​","type":1,"pageTitle":"Tests","url":"concepts/tests/#ingest","content":"ingest steps add documents to a named collection. All documents must validate against the collection'sschema, or a catalog build error will be reported. All documents from a single ingest step are added in one transaction. This means that multiple documents with a common key will be combined priorto their being appended to the collection. Suppose acmeCo/people had key [/id]: tests: acmeCo/tests/greetings: - ingest: description: Zeldas are combined to one added document. collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda One&quot; } - { userId: 1, name: &quot;Zelda Two&quot; } - verify: description: Only one Zelda is greeted. collection: acmeCo/greetings documents: - { userId: 1, greeting: &quot;Hello Zelda Two&quot; }  "},{"title":"Verify​","type":1,"pageTitle":"Tests","url":"concepts/tests/#verify","content":"verify steps assert that the current contents of a collection match the provided document fixtures. Verified documents are fully reduced, with one document for each unique key, ordered under the key's natural order. You can verify the contents of both derivations and captured collections. Documents given in verify steps do not need to be comprehensive. It is not an error if the actual document has additional locations not present in the document to verify, so long as all matched document locations are equal. Verified documents also do not need to validate against the collection's schema. They do, however, need to include all fields that are part of the collection's key. tests: acmeCo/tests/greetings: - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda&quot; } - { userId: 2, name: &quot;Link&quot; } - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda Again&quot; } - { userId: 3, name: &quot;Pikachu&quot; } - verify: collection: acmeCo/greetings documents: # greetings are keyed on /userId, and the second greeting is kept. - { userId: 1, greeting: &quot;Hello Zelda Again&quot; } # `greeting` is &quot;Hello Link&quot;, but is not asserted here. - { userId: 2 } - { userId: 3, greeting: &quot;Hello Pikachu&quot; }  "},{"title":"Partition selectors​","type":1,"pageTitle":"Tests","url":"concepts/tests/#partition-selectors","content":"Verify steps may include a partition selector to verify only documents of a specific partition: tests: acmeCo/tests/greetings: - verify: collection: acmeCo/greetings description: Verify only documents which greet Nintendo characters. documents: - { userId: 1, greeting: &quot;Hello Zelda&quot; } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } partitions: include: platform: [Nintendo]  Learn more about partition selectors. "},{"title":"Tips​","type":1,"pageTitle":"Tests","url":"concepts/tests/#tips","content":"The following tips can aid in testing large or complex derivations. "},{"title":"Testing reductions​","type":1,"pageTitle":"Tests","url":"concepts/tests/#testing-reductions","content":"Reduction annotations are expressive and powerful, and their use should thus be tested thoroughly. An easy way to test reduction annotations on captured collections is to write a two-step test that ingests multiple documents with the same key and then verifies the result. For example, the following test might be used to verify the behavior of a simple sum reduction: tests: acmeCo/tests/sum-reductions: - ingest: description: Ingest documents to be summed. collection: acmeCo/collection documents: - {id: 1, value: 5} - {id: 1, value: 4} - {id: 1, value: -3} - verify: description: Verify value was correctly summed. collection: acmeCo/collection documents: - {id: 1, value: 6}  "},{"title":"Reusing common fixtures​","type":1,"pageTitle":"Tests","url":"concepts/tests/#reusing-common-fixtures","content":"When you write a lot of tests, it can be tedious to repeat documents that are used multiple times. YAML supports anchors and references, which you can implement to re-use common documents throughout your tests. One nice pattern is to define anchors for common ingest steps in the first test, which can be re-used by subsequent tests. For example: tests: acmeCo/tests/one: - ingest: &amp;mySetup collection: acmeCo/collection documents: - {id: 1, ...} - {id: 2, ...} ... - verify: ... acmeCo/tests/two: - ingest: *mySetup - verify: ...  This allows all the subsequent tests to re-use the documents from the first ingest step without having to duplicate them. "},{"title":"Flow tutorials","type":0,"sectionRef":"#","url":"getting-started/flow-tutorials/","content":"Flow tutorials info Flow is being developed rapidly and is in private beta, so our tutorials are currently limited. More will be available at the time of the GA release and the addition of Flow's UI. For more information, you can reach our team via email or sign up for our beta waitlist. Flow tutorials are designed for brand new users to get up and running quickly, while demonstrating important Flow concepts and capabilities. All you need to do first is set up your development environment. We assume you have some engineering background, but anyone with basic technical skills should be able to follow along.","keywords":""},{"title":"Derivations","type":0,"sectionRef":"#","url":"concepts/derivations/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#specification","content":"A derivation is specified as a regular collection with an additional derivation stanza: collections: # The unique name of the derivation. acmeCo/my/derivation: schema: my-schema.yaml key: [/key] # Presence of a `derivation` stanza makes this collection a derivation. # Type: object derivation: # Register definition of the derivation. # If not provided, registers have an unconstrained schema # and initialize to the `null` value. # Optional, type: object register: # JSON Schema of register documents. As with collection schemas, # this is either an inline definition or a relative URL reference. # Required, type: string (relative URL form) or object (inline form) schema: type: integer # Initial value taken by a register which has never been updated before. # Optional, default: null initial: 0 # TypeScript module that implements any lambda functions invoked by this derivation. # Optional, type: object typescript: # TypeScript module implementing this derivation. # Module is either a relative URL of a TypeScript module file (recommended), # or an inline representation of a TypeScript module. module: acmeModule.ts # NPM package dependencies of the module # Version strings can take any form understood by NPM. # See https://docs.npmjs.com/files/package.json#dependencies npmDependencies: {} # Transformations of the derivation, # specified as a map of named transformations. transform: # Unique name of the transformation, containing only Unicode # Letters and Numbers (no spaces or punctuation). myTransformName: # Source collection read by this transformation. # Required, type: object source: # Name of the collection to be read. # Required. name: acmeCo/my/source/collection # JSON Schema to validate against the source collection. # If not set, the schema of the source collection is used. # Optional, type: string (relative URL form) or object (inline form) schema: {} # Partition selector of the source collection. # Optional. Default is to read all partitions. partitions: {} # Delay applied to sourced documents before being processed # by this transformation. # Default: No delay, pattern: ^\\\\d+(s|m|h)$ readDelay: &quot;48h&quot; # Shuffle determines the key by which source documents are # shuffled (mapped) to a register. # Optional, type: object. # If not provided, documents are shuffled on the source collection key. shuffle: # Key is a composite key which is extracted from documents # of the source. key: [/shuffle/key/one, /shuffle/key/two] # Update lambda of the transformation. # Optional, type: object update: {lambda: typescript} # Publish lambda of the transformation. # Optional, type: object publish: {lambda: typescript} # Priority applied to processing documents of this transformation # relative to other transformations of the derivation. # Default: 0, integer &gt;= 0 priority: 0  "},{"title":"Background​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#background","content":"The following sections will refer to the following common example to illustrate concepts. Suppose you have an application through which users send one another some amount of currency, like in-game tokens or dollars or digital kittens: transfers.flow.yamltransfers.schema.yaml collections: # Collection of 💲 transfers between accounts: # {id: 123, sender: alice, recipient: bob, amount: 32.50} acmeBank/transfers: schema: transfers.schema.yaml key: [/id]  There are many views over this data that you might require, such as summaries of sender or receiver activity, or current account balances within your application. "},{"title":"Transformations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#transformations","content":"A transformation binds a source collection to a derivation. As documents of the source collection arrive, the transformation processes the document to publish new documents,update aregister, or both. Read source documents are shuffled on a shuffle key to co-locate the processing of documents that have equal shuffle keys. The transformation then processes documents by invoking lambdas: user-defined functions that accept documents as arguments and return documents in response. A derivation may have many transformations, and each transformation has a long-lived and stable name. Each transformation independently reads documents from its source collection and tracks its own read progress. More than one transformation can read from the same source collection, and transformations may also source from their own derivation, enabling cyclic data-flows and graph algorithms. Transformations may be added to or removed from a derivation at any time. This makes it possible to, for example, add a new collection into an existing multi-way join, or gracefully migrate to a new source collection without incurring downtime. However, renaming a running transformation is not possible. If attempted, the old transformation is dropped and a new transformation under the new name is created, which begins reading its source collection all over again. graph LR; d[Derivation]; t[Transformation]; r[Registers]; p[Publish λ]; u[Update λ]; c[Sourced Collection]; d-- has many --&gt;t; t-- reads from --&gt;c; t-- invokes --&gt;u; t-- invokes --&gt;p; u-- updates --&gt;r; r-- reads --&gt;p; d-- indexes --&gt;r; "},{"title":"Sources​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#sources","content":"The source of a transformation is a collection. As documents are published into the source collection, they are continuously read and processed by the transformation. A partition selector may be provided to process only a subset of the source collection's logical partitions. Selectors are efficient: only partitions that match the selector are read, and Flow can cheaply skip over partitions that don't. Derivations re-validate their source documents against the source collection's schema as they are read. This is because collection schemas may evolve over time, and could have inadvertently become incompatible with historical documents of the source collection. Upon a schema error, the derivation will pause and give you an opportunity to correct the problem. You may also provide an alternative source schema. Source schemas aide in processing third-party sources of data that you don't control, which can have unexpected schema changes without notice. You may want to capture this data with a minimal and very permissive schema. Then, a derivation can apply a significantly stricter source schema, which verifies your current expectations of what the data should be. If those expectations turn out to be wrong, little harm is done: your derivation is paused but the capture continues to run. You must simply update your transformations to account for the upstream changes and then continue without any data loss. "},{"title":"Shuffles​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#shuffles","content":"As each source document is read, it's shuffled — or equivalently, mapped — on an extracted key. If you're familiar with data shuffles in tools like MapReduce, Apache Spark, or Flink, the concept is very similar. Flow catalog tasks scale to run across many machines at the same time, where each machine processes a subset of source documents. Shuffles let Flow know how to group documents so that they're co-located, which can increase processing efficiency and reduce data volumes. They are also used to map source documents to registers. graph LR; subgraph s1 [Source Partitions] p1&gt;acmeBank/transfers/part-1]; p2&gt;acmeBank/transfers/part-2]; end subgraph s2 [Derivation Task Shards] t1([task/shard-1]); t2([task/shard-2]); end p1-- sender: alice --&gt;t1; p1-- sender: bob --&gt;t2; p2-- sender: alice --&gt;t1; p2-- sender: bob --&gt;t2; If you don't provide a shuffle key, Flow will shuffle on the source collection key, which is typically what you want. If a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string or integer) as well as the number of components in a composite key. For example, one transformation couldn't shuffle transfers on [/id]while another shuffles on [/sender], because sender is a string andid an integer. Similarly mixing a shuffle of [/sender] alongside [/sender, /recipient]is prohibited because the keys have different numbers of components. However, one transformation can shuffle on [/sender]while another shuffles on [/recipient], as in the examples below. "},{"title":"Registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#registers","content":"Registers are the internal memory of a derivation. They are a building block that enable derivations to tackle advanced stateful streaming computations like multi-way joins, windowing, and transaction processing. As we've already seen, not all derivations require registers, but they are essential for a variety of important use cases. Each register is a document with a user-definedschema. Registers are keyed, and every derivation maintains an index of keys and their corresponding register documents. Every source document is mapped to a specific register document through its extracted shuffle key. For example, when shuffling acmeBank/transfers on [/sender] or [/recipient], each account (&quot;alice&quot;, &quot;bob&quot;, or &quot;carol&quot;) is allocated its own register. You might use that register to track a current account balance given the received inflows and sent outflows of each account. If you instead shuffle on [/sender, /recipient], each pair of accounts (&quot;alice -&gt; bob&quot;, &quot;alice -&gt; carol&quot;, &quot;bob -&gt; carol&quot;) is allocated a register. Transformations of a derivation may have different shuffle keys, but the number of key components and their JSON types must agree. Two transformations could map on [/sender] and [/recipient], but not [/sender] and [/recipient, /sender]. Registers are best suited for relatively small, fast-changing documents that are shared within and across the transformations of a derivation. The number of registers indexed within a derivation may be very large, and if a register has never before been used, it starts with a user-defined initial value. From there, registers may be modified through an update lambda. info Under the hood, registers are backed by replicated, embedded RocksDB instances, which co-locate with the lambda execution contexts that Flow manages. As contexts are assigned and re-assigned, their register databases travel with them. If any single RocksDB instance becomes too large, Flow is able to perform an online split, which subdivides its contents into two new databases — and paired execution contexts — which are re-assigned to other machines. "},{"title":"Lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#lambdas","content":"Lambdas are user-defined functions that are invoked by derivations. They accept documents as arguments and return transformed documents in response. Lambdas can be used to update registers, publish documents into a derived collection, or compute a non-trivial shuffle key of a document. Beta The ability for lambdas to compute a document's shuffle key is coming soon. Flow supports TypeScript lambdas, which you define in an accompanying TypeScript module and reference in a derivation's typescript stanzas. See the derivation specification and Creating TypeScript modules for more details on how to get started. TypeScript lambdas are &quot;serverless&quot;; Flow manages the execution and scaling of your Lambda on your behalf. Alternatively, Flow also supports remote lambdas, which invoke an HTTP endpoint you provide, such as an AWS Lambda or Google Cloud Run function. In terms of the MapReduce functional programming paradigm, Flow lambdas are mappers, which map documents into new user-defined shapes. Reductions are implemented by Flow using the reduction annotations of your collection schemas. "},{"title":"Publish lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#publish-lambdas","content":"A publish lambda publishes documents into the derived collection. To illustrate first with an example, suppose you must know the last transfer from each sender that was over $100: last-large-send.flow.yamllast-large-send.tslast-large-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/last-large-send: schema: transfers.schema.yaml key: [/sender] derivation: typescript: module: last-large-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  This transformation defines a TypeScript publish lambda, which is implemented in an accompanying TypeScript module. The lambda is invoked as each source transfer document arrives. It is given the source document, and also includes the a _register and _previous register, which are not used here. The lambda outputs zero or more documents, each of which must conform to the derivation's schema. As this derivation's collection is keyed on /sender, the last published document (the last large transfer) of each sender is retained. If it were instead keyed on /id, then all transfers with large amounts would be retained. In SQL terms, the collection key acts as a GROUP BY.  Derivation collection schemas may havereduction annotations, and publish lambdas can be combined with reductions in interesting ways. You may be familiar with map and reduce functions built into languages likePython,JavaScript; and many others, or have used tools like MapReduce or Spark. In functional terms, lambdas you write within Flow are &quot;mappers,&quot; and reductions are always done by the Flow runtime using your schema annotations. Suppose you need to know the runningaccount balancesof your users given all of their transfers thus far. Tackle this by reducing the final account balance for each user from all of the credit and debit amounts of their transfers: balances.flow.yamlbalances.tsbalances-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/balances: schema: type: object required: [user] reduce: { strategy: merge } properties: user: { type: string } balance: type: number reduce: { strategy: sum } key: [/user] derivation: typescript: module: balances.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  "},{"title":"Update lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#update-lambdas","content":"An update lambda transforms a source document into an update of the source document's register. To again illustrate through an example, suppose your compliance department wants you to flag the first transfer a sender sends to a new recipient. You achieve this by shuffling on pairs of[/sender, /recipient] and using a register to track whether this account pair has been seen before: first-send.flow.yamlfirst-send.tsfirst-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/first-send: schema: transfers.schema.yaml key: [/id] derivation: # We'll store a `true/false` boolean in our register documents, # which is initially `false` and becomes `true` after the first transfer. register: schema: { type: boolean } initial: false typescript: module: first-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers # Shuffle so that each account pair # is allocated its own register. shuffle: key: [/sender, /recipient] update: lambda: typescript publish: lambda: typescript  This transformation uses both a publish and an update lambda, implemented in an accompanying TypeScript module. The update lambda is invoked first for each source document, and it returns zero or more documents, which each must conform to the derivation's register schema (in this case, a simple boolean). The publish lambda is invoked next, and is given the sourcedocument as well as the before (previous) and after (_register) values of the updated register. In this case, we don't need the after value: our update lambda implementation implies that it's always true. The before value, however, tells us whether this was the very first update of this register, and by implication was the first transfer for this pair of accounts. sequenceDiagram autonumber Derivation-&gt;&gt;Update λ: update({sender: alice, recipient: bob})? Update λ--&gt;&gt;Derivation: return &quot;true&quot; Derivation-&gt;&gt;Registers: lookup(key = [alice, bob])? Registers--&gt;&gt;Derivation: not found, initialize as &quot;false&quot; Derivation--&gt;&gt;Derivation: Register: &quot;false&quot; =&gt; &quot;true&quot; Derivation-)Registers: store(key = [alice, bob], value = &quot;true&quot;) Derivation-&gt;&gt;Publish λ: publish({sender: alice, recipient: bob}, register = &quot;true&quot;, previous = &quot;false&quot;)? Publish λ--&gt;&gt;Derivation: return {sender: alice, recipient: bob} FAQ Why not have one lambda that can return a register update and derived documents? Performance.Update and publish are designed to be parallelized and pipelined over many source documents simultaneously, while still giving the appearance and correctness of lambdas are invoked in strict serial order. Notice that (1) above doesn't depend on actually knowing the register value, which doesn't happen until (4). Many calls like (1) can also happen in parallel, so long as their applications to the register value (5) happen in the correct order. In comparison, a single-lambda design would require Flow to await each invocation before it can begin the next.  Register schemas may also havereduction annotations, and documents returned by update lambdas are reduced into the current register value. The compliance department reached out again, and this time they need you to identify transfers where the sender's account had insufficient funds. You manage this by tracking the running credits and debits of each account in a register. Then, you enrich each transfer with the account's current balance and whether the account was overdrawn: flagged-transfers.flow.yamlflagged-transfers.tsflagged-transfers-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/flagged-transfers: schema: # Extend transfer schema with `balance` and `overdrawn` fields. $ref: transfers.schema.yaml required: [balance, overdrawn] properties: balance: { type: number } overdrawn: { type: boolean } key: [/id] projections: # Logically partition on transfers which are flagged as overdrawn. overdrawn: location: /overdrawn partition: true derivation: # Registers track the current balance of each account. register: schema: type: number reduce: { strategy: sum } initial: 0 typescript: module: flagged-transfers.ts transform: fromTransferSender: source: { name: examples/acmeBank/transfers } shuffle: { key: [/sender] } # Debit the sender's register balance. update: { lambda: typescript } # Publish transfer enriched with current sender balance. publish: { lambda: typescript } fromTransferRecipient: source: { name: examples/acmeBank/transfers } shuffle: { key: [/recipient] } # Credit the recipient's register balance. update: { lambda: typescript }  Source transfers are read twice. The first read shuffles on /recipientto track account credits, and the second shuffles on /senderto track account debits and to publish enriched transfer events. Update lambdas return the amount of credit or debit, and these amounts are summed into a derivation register keyed on the account. sequenceDiagram autonumber Derivation-&gt;&gt;Registers: lookup(key = alice)? Registers--&gt;&gt;Derivation: not found, initialize as 0 Derivation-&gt;&gt;Update λ: update({recipient: alice, amount: 50, ...})? Update λ--&gt;&gt;Derivation: return +50 Derivation-&gt;&gt;Update λ: update({sender: alice, amount: 75, ...})? Update λ--&gt;&gt;Derivation: return -75 Derivation--&gt;&gt;Derivation: Register: 0 + 50 =&gt; 50 Derivation--&gt;&gt;Derivation: Register: 50 - 75 =&gt; -25 Derivation-&gt;&gt;Publish λ: publish({sender: alice, amount: 75, ...}, register = -25, previous = 50)? Publish λ--&gt;&gt;Derivation: return {sender: alice, amount: 75, balance: -25, overdrawn: true} "},{"title":"Creating TypeScript modules​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#creating-typescript-modules","content":"To create a new TypeScript module for the lambdas of your derivation, you can use flowctl typescript generate to generate it. In the derivation specification, choose the name for the new module and run flowctl typescript generate. Flow creates a module with the name you specified, stubs of the required interfaces, and TypeScript types that match your schemas. Update the module with your lambda function bodies, and proceed to test and deploy your catalog. Using the example below, flowctl typescript generate --source=acmeBank.flow.yaml will generate the stubbed-out acmeBank.ts. acmeBank.flow.yamlacmeBank.ts (generated stub) collections: acmeBank/balances: schema: balances.schema.yaml key: [/account] derivation: typescript: module: acmeBank.ts transform: fromTransfers: source: { name: acmeBank/transfers } publish: { lambda: typescript }  Learn more about TypeScript generation "},{"title":"NPM dependencies​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#npm-dependencies","content":"Your TypeScript modules may depend on otherNPM packages, which can be be imported through the npmDependenciesstanza of the derivation spec. For example, moment is a common library for working with times: catalog.flow.yamlfirst-send.ts derivation: typescript: module: first-send.ts npmDependencies: moment: &quot;^2.24&quot; transform: { ... }  Use any version string understood by package.json, which can include local packages, GitHub repository commits, and more. See package.json documentation. During the catalog build process, Flow gathers NPM dependencies across all catalog source files and patches them into the catalog's managed package.json. Flow organizes its generated TypeScript project structure for a seamless editing experience out of the box with VS Code and other common editors. "},{"title":"Remote lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#remote-lambdas","content":"A remote Lambda is one that you implement and host yourself as a web-accessible endpoint, typically via a service like AWS Lambda or Google Cloud Run. Flow will invoke your remote Lambda as needed, POST-ing JSON documents to process and expecting JSON documents in the response. "},{"title":"Processing order​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#processing-order","content":"Derivations may have multiple transformations that simultaneously read from different source collections, or even multiple transformations that read from the same source collection. Roughly speaking, the derivation will globally process transformations and their source documents in the time-based order in which the source documents were originally written to their source collections. This means that a derivation started a month ago and a new copy of the derivation started today, will process documents in the same order and arrive at the same result. Derivations are repeatable. More precisely, processing order is stable for each individual shuffle key, though different shuffle keys may process in different orders if more than one task shard is used. Processing order can be attenuated through a read delayor differing transformation priority. "},{"title":"Read delay​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-delay","content":"A transformation can define a read delay, which will hold back the processing of its source documents until the time delay condition is met. For example, a read delay of 15 minutes would mean that a source document cannot be processed until it was published at least 15 minutes ago. If the derivation is working through a historical backlog of source documents, than a delayed transformation will respect its ordering delay relative to the publishing times of other historical documents also being read. Event-driven workflows are a great fit for reacting to events as they occur, but aren’t terribly good at taking action when something hasn’t happened: A user adds a product to their cart, but then doesn’t complete a purchase.A temperature sensor stops producing its expected, periodic measurements. A common pattern for tackling these workflows in Flow is to read a source collection without a delay and update a register. Then, read a collection with a read delay and determine whether the desired action has happened or not. For example, source from a collection of sensor readings and index the last timestamp of each sensor in a register. Then, source the same collection again with a read delay: if the register timestamp isn't more recent than the delayed source reading, the sensor failed to produce a measurement. Flow read delays are very efficient and scale better than managing very large numbers of fine-grain timers. Learn more from the Citi Bike idle bikes example "},{"title":"Read priority​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-priority","content":"Sometimes it's necessary for all documents of a source collection to be processed by a transformation before any documents of some other source collection are processed, regardless of their relative publishing time. For example, a collection may have corrections that should be applied before the historical data of another collection is re-processed. Transformation priorities allow you to express the relative processing priority of a derivation's various transformations. When priorities are not equal, all available source documents of a higher-priority transformation are processed before any source documents of a lower-priority transformation. "},{"title":"Where to accumulate?​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#where-to-accumulate","content":"When you build a derived collection, you must choose where accumulation will happen: whether Flow will reduce into documents held within your materialization endpoint, or within the derivation's registers. These two approaches can produce equivalent results, but they do so in very different ways. "},{"title":"Accumulate in your database​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-your-database","content":"To accumulate in your materialization endpoint, such as a database, you define a derivation with a reducible schema and use only publish lambdas and no registers. The Flow runtime uses your reduction annotations to combine published documents, which are written to the collection. It then fully reduces collection documents into the values stored in the database. This keeps the materialized table up to date. A key insight is that the database is the only stateful system in this scenario, and Flow uses reductions in two steps: To combine many published documents into intermediate delta documents, which are the documents written to collection storage.To reduce delta states into the final database-stored document. For example, consider a collection that’s summing a value: Time\tDB\tLambdas\tDerived DocumentT0\t0\tpublish(2, 1, 2)\t5 T1\t5\tpublish(-2, 1)\t-1 T2\t4\tpublish(3, -2, 1)\t2 T3\t6\tpublish()\t This works especially well when materializing into a transactional database. Flow couples its processing transactions with corresponding database transactions, ensuring end-to-end “exactly once” semantics. When materializing into a non-transactional store, Flow is only able to provide weaker “at least once” semantics; it’s possible that a document may be combined into a database value more than once. Whether that’s a concern depends a bit on the task at hand. Some reductions can be applied repeatedly without changing the result (they're &quot;idempotent&quot;), while in other use cases approximations are acceptable. For the summing example above, &quot;at-least-once&quot; semantics could give an incorrect result. "},{"title":"Accumulate in registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-registers","content":"To accumulate in registers, you use a derivation that defines a reducible register schema that's updated through update lambdas. The Flow runtime allocates, manages, and scales durable storage for registers; you don’t have to. Then you use publish lambdas to publish a snapshot of your register value into your collection. Returning to our summing example: Time\tRegister\tLambdas\tDerived DocumentT0\t0\tupdate(2, 1, 2), publish(register)\t5 T1\t5\tupdate(-2, 1), publish(register)\t4 T2\t4\tupdate(3, -2, 1), publish(register)\t6 T3\t6\tupdate()\t Register derivations are a great solution for materializations into non-transactional stores because the documents they produce can be applied multiple times without breaking correctness. They’re also well-suited for materializations into endpoints that aren't stateful, such as pub/sub systems or Webhooks, because they can produce fully reduced values as stand-alone updates. Learn more in the derivation pattern examples of Flow's repository "},{"title":"Your first data flow","type":0,"sectionRef":"#","url":"getting-started/flow-tutorials/hello-flow/","content":"","keywords":""},{"title":"Word count in a continuous PostgreSQL materialized view​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#word-count-in-a-continuous-postgresql-materialized-view","content":"PostgreSQL is a great open-source database that supports materialized views, but it doesn't offer continuous materialized views. In this tutorial, you'll build one with Flow. How many times have you managed text documents in PostgreSQL and thought to yourself: &quot;Gee-whiz, self, I wish I had a table of word counts that was always up-to-date!&quot; ... basically never, right? Well, it's a simple way to get familiar with a powerful concept, so let's do it anyway! Our data flow will: Capture data from a documents table in the PostgreSQL database.Use a derivation to compute word and document frequency updates.Materialize the results back into the table in a a word_counts table. These three processes — captures, derivations, and materializations — comprise the possible tasks in any data flow. They're configured in YAML files known as a catalog specification. For this example, they've been configured for you. If you'd like, you can check out flow.yaml and word-counts.flow.yaml to get oriented. "},{"title":"Set up​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#set-up","content":"These instructions assume you've set up a development environment either using Codespaces or on your local machine. Go back and do that first, if necessaary. "},{"title":"Verify tests​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#verify-tests","content":"All cutting-edge word count projects should have tests. Let's make sure the words are, um, counted. Run the following: $ flowctl test --source word-counts.flow.yaml  Wait until you see: Running 1 tests... ✔️ word-counts.flow.yaml :: acmeCo/tests/word-counts-from-documents Ran 1 tests, 1 passed, 0 failed  Your test performed as expected; now you can deploy the catalog. "},{"title":"Run it locally​","type":1,"pageTitle":"Your first data flow","url":"getting-started/flow-tutorials/hello-flow/#run-it-locally","content":"Start a local, temporary Flow data plane: $ flowctl temp-data-plane  A data plane is a long-lived, multi-tenant, scale-out component that usually runs in a data center. Fortunately it also shrinks down to your laptop. This returns a couple exported addresses. Copy these; you'll need them in a moment: export BROKER_ADDRESS=http://localhost:8080 export CONSUMER_ADDRESS=http://localhost:9000  Now, deploy the catalog to your data plane by running: $ export BROKER_ADDRESS=http://localhost:8080 $ export CONSUMER_ADDRESS=http://localhost:9000 $ flowctl deploy --wait-and-cleanup --source flow.yaml  After a moment, you'll see: Deployment done. Waiting for Ctrl-C to clean up and exit.  Flow is now watching the documents table, and materializing to word_counts. Start a new terminal window to begin working with the database. $ psql --host localhost psql (13.5 (Debian 13.5-0+deb11u1), server 13.2 (Debian 13.2-1.pgdg100+1)) Type &quot;help&quot; for help.  The documents table is still empty, so you'll populate it with a few phrases: flow=# insert into documents (body) values ('The cat in the hat.'), ('hat Cat CAT!'); INSERT 0 2  Now, you'll take a look at word_counts to see the results: flow=# select word, count, doc_count from word_counts; word | count | doc_count ------+-------+----------- cat | 3 | 2 hat | 2 | 2 in | 1 | 1 the | 2 | 1 (4 rows)  Say you made a typo in that second value. You can immediately update it: flow=# update documents set body = 'cat Cat CAT!' where id = 2; UPDATE 1 flow=# select word, count, doc_count from word_counts order by word; word | count | doc_count ------+-------+----------- cat | 4 | 2 hat | 1 | 1 in | 1 | 1 the | 2 | 1 (4 rows)  Now, let's clean up the table: flow=# delete from documents ; DELETE 2 flow=# select word, count, doc_count from word_counts order by word; word | count | doc_count ------+-------+----------- cat | 0 | 0 hat | 0 | 0 in | 0 | 0 the | 0 | 0 (4 rows)  The updates you push to documents are materialized to word_counts with millisecond latency. In effect, you've added a new, powerful capability to PostgreSQL that has a multitude of real-world and business applications (far beyond counting cats and hats). When you're done with your testing, exit the data flow by returning to your first console window and pressing Ctrl-C. "},{"title":"Setting up a development environment","type":0,"sectionRef":"#","url":"getting-started/installation/","content":"","keywords":""},{"title":"Using GitHub Codespaces​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#using-github-codespaces","content":"GitHub codespaces provides VM-backed, portable development environments that are ideal for getting started with Flow in minutes. Currently, Codespaces is available to GitHub Teams and Enterprise customers, as well as individuals enrolled in the beta. If you have access, this is the preferred method — setting up a devcontainer in Codespaces is much quicker than doing so locally. Visit the Flow Template repository, click Code, and choose New Codespace. The VM spins up within a minute or two, and you can immediately begin developing and testing. The template includes a PostgreSQL database for this purpose. "},{"title":"Using Visual Studio Code locally​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#using-visual-studio-code-locally","content":"If you don't have access to Codespaces, or prefer local development, use this method to create a local environment. Download and install the following prerequisites: DockerVS CodeVS Code Remote Containers extension Create a Git repository from the Flow Template ​ Visit the Flow Template repository on GitHub, click on Use this template, and proceed to create your repository. Open in VS Code ​ Clone your repository locally and open it in VS Code. You'll see a popup in the lower right corner asking if you'd like to re-open the repository in a container. Click Re-open in container. It may take several minutes to download components and build the container. "},{"title":"Test your environment​","type":1,"pageTitle":"Setting up a development environment","url":"getting-started/installation/#test-your-environment","content":"Regardless of the method you used, first test everything is working as expected. The repository contains a sample project, which includes a test. (It also serves as a quick tutorial, which we recommend as a next step). In a terminal window, run: flowctl test --source word-counts.flow.yaml  Verify that it returns: Ran 1 tests, 1 passed, 0 failed  You're now ready to start using Flow! Proceed to the Flow introductory tutorial. "},{"title":"Configure connections with SSH tunneling","type":0,"sectionRef":"#","url":"guides/connect-network/","content":"","keywords":""},{"title":"General setup​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#general-setup","content":"Activate an SSH implementation on a server, if you don't have one already. Consult the documentation for your server's operating system and/or cloud service provider, as the steps will vary. Configure the server to your organization's standards, or reference the SSH documentation for basic configuration options. Referencing the config files and shell output, collect the following information: The SSH endpoint for the SSH server, formatted as ssh://hostname[:port]. This may look like the any of following: ssh://ec2-198-21-98-1.compute-1.amazonaws.comssh://198.21.98.1ssh://198.21.98.1:22 The SSH user, which will be used to log into the SSH server, for example, sshuser. You may choose to create a new user for this workflow. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Taken together, these configuration details would allow you to log into the SSH server from your local machine. They'll allow the connector to do the same. Configure your internal network to allow the SSH server to access your capture or materialization endpoint. Note the internal host and port; these are necessary to open the connection. Configure your network to expose the SSH server endpoint to external traffic. The method you use depends on your organization's IT policies. Currently, Estuary doesn't provide a list of static IPs for whitelisting purposes, but if you require one, contact Estuary support. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for AWS​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-aws","content":"To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Amazon RDS, you can find these in the RDS console as Endpoint and Port. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for Google Cloud​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-google-cloud","content":"To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM). Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Google Cloud SQL, you can find the host in the Cloud Console as Public IP Address. Use 5432 as the port. Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Setup for Azure​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-azure","content":"To allow SSH tunneling to a database instance hosted on Azure, you'll need to create a virtual machine (VM) in the same virtual network as your endpoint database. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Create and connect to a VM in a virtual network, and add the endpoint database to the network. Create a new virtual network and subnet. Create a Linux or Windows VM within the virtual network, directing the SSH public key source to the public key you generated previously. Note the VM's public IP; you'll need this later. Create a service endpoint for your database in the same virtual network as your VM. Instructions for Azure Database For PostgreSQL can be found here; note that instructions for other database engines may be different. Find and note the host and port for your capture or materialization endpoint. tip For database instances hosted in Azure, you can find the host as Server Name, and the port under Connection Strings (usually 5432). Choose an open port on your localhost from which you'll connect to the SSH server. "},{"title":"Configuration​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#configuration","content":"After you've completed the prerequisites, you should have the following parameters: SSH Endpoint / sshEndpoint: the SSH server's hostname, or public IP address, formatted as ssh://hostname[:port]Private Key / privateKey: the contents of the PEM fileUser / user: the username used to connect to the SSH server.Forward Host / forwardHost: the capture or materialization endpoint's hostForward Port / forwardPort: the capture or materialization endpoint's portLocal Port / localPort: the port on the localhost used to connect to the SSH server Use these to add SSH tunneling to your capture or materialization definition, either by filling in the corresponding fields in a web app, or by working with the YAML directly. Reference the Connectors page for a YAML sample. Proxies like SSH are always run on an open port on your localhost, so you'll need to re-configure other fields in your capture or materialization definition. Set the connector's host property to match localhost in the SSH configuration. If the connector has a port property, set it to the same value as localPort in the SSH configuration. "},{"title":"Create a simple data flow","type":0,"sectionRef":"#","url":"guides/create-dataflow/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#prerequisites","content":"This guide assumes a basic understanding of Flow and its key concepts. Before you begin, it's recommended that you read the high level concepts documentation. "},{"title":"Introduction​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#introduction","content":"In Estuary Flow, you create data flows to connect data source and destination systems. The set of specifications that defines a data flow is known as a catalog, and is made of several important entities. The simplest Flow catalog comprises three types of entities: A data capture, which ingests data from an external sourceOne or more collections, which store that data in a cloud-backed data lakeA materialization, to push the data to an external destination Almost always, the capture and materialization each rely on a connector. A connector is a plug-in component that interfaces between Flow and whatever data system you need to connect to. Here, we'll walk through how to leverage various connectors, configure them, and deploy your catalog to create an active data flow. "},{"title":"Create a capture​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#create-a-capture","content":"You'll first create a capture to connect to your data source system. This process will create one or more collections in Flow, which you can then materialize to another system. Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Captures tab and choose New capture. On the Create Captures page, choose a name for your capture. Your capture name must begin with a prefix to which you have access. Click inside the Name field to generate a drop-down menu of available prefixes, and select your prefix. Append a unique capture name after the / to create the full name, for example acmeCo/myFirstCapture. Use the Connector drop down to choose your desired data source. A form appears with the properties required for that connector. More details are on each connector are provided in the connectors reference. Fill out the required properties and click Test Config. Flow uses the provided information to initiate a connection to the source system. It identifies one or more data resources — these may be tables, data streams, or something else, depending on the connector. Each resource is mapped to a collection through a binding. If there's an error, you'll be prompted to fix your configuration and test again. Look over the generated capture definition and the schema of the resulting Flow collection(s). Flow generates catalog specifications as YAML files. You can modify it by filling in new values in the form and clicking Regenerate Catalog, or by editing the YAML files directly in the web application. (Those who prefer a command-line interface can manage and edit YAML in their preferred development environment). It's not always necessary to review and edit the YAML — Flow will prevent the publication of invalid catalogs. Once you're satisfied with the configuration, click Save and publish. You'll see a notification when the capture publishes successfully. Click Materialize collections to continue. "},{"title":"Create a materialization​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#create-a-materialization","content":"Now that you've captured data into one or more collections, you can materialize it to a destination. The New Materializations page is pre-populated with the capture and collection you just created. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/myFirstMaterialization. Use the Connector drop down to choose your desired data destination. The rest of the page populates with the properties required for that connector. More details are on each connector are provided in the connectors reference. Fill out the required properties and click Regenerate catalog. Flow initiates a connection with the destination system, and creates a binding to map each collection in your catalog to a resource in the destination. Again, these may be tables, data streams, or something else. When you publish the complete catalog, Flow will create these new resources in the destination. Look over the generated materialization definition and edit it, if you'd like. Click Save and publish. You'll see a notification when the full data flow publishes successfully. "},{"title":"What's next?​","type":1,"pageTitle":"Create a simple data flow","url":"guides/create-dataflow/#whats-next","content":"Now that you've deployed your first data flow, you can explore more possibilities. Read the high level concepts to better understand how Flow works and what's possible. Create more complex data flows by mixing and matching collections in your captures and materializations. For example: Materialize the same collection to multiple destinations. If a capture produces multiple collections, materialize each one to a different destination. Materialize collections that came from different sources to the same destination. Advanced users can modify collection schemas, apply data reductions, or transform data with a derivation(derivations are currently available using the CLI, but support in the web application is coming soon.) "},{"title":"Comparisons","type":0,"sectionRef":"#","url":"overview/comparisons/","content":"","keywords":""},{"title":"Apache Beam and Google Cloud Dataflow​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#apache-beam-and-google-cloud-dataflow","content":"Flow’s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there’s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences. Like Beam, Flow’s primary primitive is a collection. You build a processing graph (called a pipeline in Beam and a catalog in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow’s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas. Unlike Beam, Flow doesn’t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types. Also, while Beam allows you the option to define combine operators, Flow’s runtime always applies combine operators. These are built using the declared semantics of the document’s schema, which makes it much more efficient and cost-effective to work with streaming data. Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow’s modeling of state – via its per-key register concept – is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer. "},{"title":"Kafka​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#kafka","content":"Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem. Flow is built on Gazette, a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette’s consumer framework, which has similarities to Kafka consumers. Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics. Journals in Gazette and Flow are roughly analogous to Kafka partitions. Each journal is a single append-only log. Gazette has no native notion of a topic, but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called fragments, which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently. Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called collections. Flow collections are somewhat similar to Kafka streams, but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning. Flow tasks are most similar to Kafka stream processors, but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls shards, to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling. "},{"title":"Spark​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#spark","content":"Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems. You can think of a Flow collection as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can materialize to push the data into some external system like cloud storage or a database. You can also create a collection that is derived by applying stateful transformations to one or more source collections. Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data. Spark's processing primitives, applications, jobs, and tasks, don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the catalog. A Flow catalog is a composition of Flow tasks, which are quite different from tasks in Spark. In Flow, a task is a logical unit of work that does one of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow shard. In Flow, a task is a logical unit of work, and shards represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling. Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations. "},{"title":"Hadoop, HDFS, and Hive​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#hadoop-hdfs-and-hive","content":"There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow. To gain an understanding of Flow's processing model for derivations, see this blog post about MapReduce in Flow. HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive. For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing collections, rather than creating a new one each time a job is run. In fact, Flow has no notion of a job like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. This blog post explores those differences in greater depth. To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries. In Flow, you instead define a capture of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow derivations, which again apply the transformations incrementally and in real time. While you could actually use tools like Hive to directly query data from Flow collections — the layout of collection data in cloud storage is intentionally compatible with this — you could also materialize a view of your transformation results to any database, which is also kept up to date in real time. "},{"title":"Fivetran, Airbyte, and other ELT solutions​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#fivetran-airbyte-and-other-elt-solutions","content":"Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the world in terms of independent scheduled jobs that copy data from source to destination, Flow catalogs model a directed graph ofcaptures (reads from sources),derivations (transforms), andmaterializations (writes to destinations). Collectively, these are called tasks. Tasks in Flow are only indirectly linked. Captures read data from a source and output to collections. Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing*. Like Airbyte, Flow uses connectors for interacting with external systems in captures and materializations. For captures, Flow integrates the Airbyte specification, so all Airbyte source connectors can be used with Flow. For materializations, Flow uses its own protocol which is not compatible with the Airbyte spec. In either case, the usage of connectors is pretty similar. In terms of technical capabilities, Flow can do everything that these tools can and more. Both Fivetran and Airbyte both currently have graphical interfaces that make them much easier for non-technical users to configure. Flow, too, is focused on empowering non-technical users through its web application. At the same time, it Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow. * Some materialization endpoints can only make at-least-once guarantees. "},{"title":"dbt​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#dbt","content":"dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively. In addition to – and perhaps more important than – its transform capability, dbt brought an entirely new workflow for working with data: one that prioritizes version control, testing, local development, documentation, composition, and re-use. Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse. Flow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else. These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse. Within the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to productionize important insights as materialized views or by pushing to another destination. Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in a separate, dedicated tool like dbt. While Flow and dbt don’t interact directly, both offer easy integration through your data warehouse. "},{"title":"Materialize, Rockset, ksqlDB, and other real-time databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#materialize-rockset-ksqldb-and-other-real-time-databases","content":"Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers, and can keep SQL views up to date in real time. These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not. However, this doesn't mean that Flow should replace these systems in your stack. In fact, it can be optimal to use Flow to feed data into them. Flow adds real-time data capture and materialization options that many real-time databases don't support. Once data has arrived in the database, you have access to real-time SQL analysis and other analytical tools not native to Flow. For further explanation, read the section below on OLAP databases. "},{"title":"Snowflake, BigQuery, and other OLAP databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#snowflake-bigquery-and-other-olap-databases","content":"Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by materializing views into it. Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow catalogs define collections. These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key. Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run. Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date. A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow’s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant. "},{"title":"Who should use Flow?","type":0,"sectionRef":"#","url":"overview/who-should-use-flow/","content":"","keywords":""},{"title":"Benefits​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#benefits","content":"These characteristics set Flow apart from other data integration workflows and address the pain points listed above. "},{"title":"Fully integrated pipelines​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#fully-integrated-pipelines","content":"With Flow, you can build, test, and evolve pipelines that continuously capture, transform, and materialize data across all of your systems. With one tool, you can power workflows that have historically required you to first piece together services, then integrate and operate them in-house to meet your needs. To achieve comparable capabilities to Flow you would need: A low-latency streaming system, such as AWS KinesisData lake build-out, such as Kinesis Firehose to S3Custom ETL application development, such as Spark, Flink, or AWS λSupplemental data stores for intermediate transformation statesETL job management and execution, such as a self-hosting or Google Cloud DataflowCustom reconciliation of historical vs streaming datasets, including onerous backfills of new streaming applications from historical data Flow's declarative GitOps workflow is a dramatic simplification from this inherent complexity. It saves you time and costs, catches mistakes before they hit production, and keeps your data fresh across all the places you use it. The UI-forward web application takes usability to a new level, allowing more types of professionals to contribute to what would otherwise require a highly specialized set of technical skills. "},{"title":"Efficient architecture​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#efficient-architecture","content":"Flow mixes a variety of architectural techniques to deliver great throughput, avoid latency, and minimize operating costs. These include: Leveraging reductions to reduce the amount of data that must be ingested, stored, and processed, often dramaticallyExecuting transformations predominantly in-memoryOptimistic pipelining and vectorization of internal remote procedure calls (RPCs) and operationsA cloud-native design that optimizes for public cloud pricing models Flow also makes it easy to materialize focused data rollups as views directly into your warehouse, so you don't need to repeatedly query the much larger source datasets. This can dramatically lower warehouse costs. "},{"title":"Powerful transformations​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#powerful-transformations","content":"With Flow, you can build pipelines that join a current event with an event that happened days, weeks, even years in the past. Flow can model arbitrary stream-to-stream joins without the windowing constraints imposed by other systems, which limit how far back in time you can join. Flow transforms data in durable micro-transactions, meaning that an outcome, once committed, won't be silently re-ordered or changed due to a crash or machine failure. This makes Flow uniquely suited for operational workflows, like assigning a dynamic amount of available inventory to a stream of requests — decisions that, once made, should not be forgotten. You can also evolve transformations as business requirements change, enriching them with new datasets or behaviors without needing to re-compute from scratch. "},{"title":"Data integrity​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#data-integrity","content":"Flow supports strong schematization, durable transactions with exactly-once semantics, and easy end-to-end testing to ensure that your data is accurate and that changes don't break pipelines. JSON schemas are verified with every document read or written. If a document violates its schema, Flow pauses the pipeline, giving you a chance to fix the error.Schemas can encode constraints, like that a latitude value must be between +90 and -90 degrees, or that a field must be a valid email address.Flow projects JSON schema into other flavors, like TypeScript types or SQL tables. Strong type checking catches bugs before they're applied to production.Flow's declarative tests verify the integrated, end-to-end behavior of processing pipelines. "},{"title":"Dynamic scaling​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#dynamic-scaling","content":"The Flow runtime scales from a single process for local development up to a large Kubernetes cluster for high-volume production deployments. Processing tasks are quickly reassigned upon any machine failure for high availability. Each process can also be scaled independently, at any time, and without downtime. This is unique to Flow. Comparable systems require that an arbitrary data partitioning be decided upfront, a crucial performance knob that's awkward and expensive to change. Instead, Flow can repeatedly split a running task into two new tasks, each half the size, without stopping it or impacting its downstream uses. "},{"title":"Authorizing users and authenticating with Flow","type":0,"sectionRef":"#","url":"reference/authentication/","content":"","keywords":""},{"title":"Subjects, objects, and inherited capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#subjects-objects-and-inherited-capabilities","content":"The entity to which you grant a capability is called the subject, and the entity over which access is granted is called the object. The subject can be either a user or a prefix, and the object is always a prefix. This allows subjects to inherit nested capabilities, so long as they are granted admin. For example, user X of Acme Co has admin access to the acmeCo/ prefix, and user Y has write access. A third party has granted acmeCo/ read access to shared data at outside-org/acmeCo-share/. User X automatically inherits read access to outside-org/acmeCo-share/, but user Y does not. "},{"title":"Default authorization settings​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#default-authorization-settings","content":"When you first sign up to use Flow, your organization is provisioned a prefix, and your username is granted admin access to the prefix. Your prefix is granted write access to itself and read access to its logs, which are stored under a unique sub-prefix of the global ops/ prefix. Using the same example, say user X signs up on behalf of their company, AcmeCo. User X is automatically granted admin access to the acmeCo/ prefix.acmeCo/, in turn, has write access to acmeCo/ and read access to ops/acmeCo/. As more users and prefixes are added, admins can provision capabilities using the CLI. "},{"title":"Authenticating Flow in the web app​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-in-the-web-app","content":"You must sign in to begin a new session using the Flow web application. For the duration of the session, you'll be able to perform actions depending on the capabilities granted to the user profile. You can view the capabilities currently provisioned in your organization on the Admin tab. "},{"title":"Authenticating Flow using the CLI​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-using-the-cli","content":"You can use the flowctl CLI to work with your organization's catalogs and drafts in your local development environment. To authenticate a local development session using the CLI, do the following: Sign into the Flow web application. Click the Admin tab, scroll to the Access Token box, and copy the token. In the terminal of your local development environment, run: flowctl auth token --token=&lt;copied-token&gt;  The token will expire after a predetermined duration. Generate a new token using the web application and re-authenticate. "},{"title":"Provisioning capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#provisioning-capabilities","content":"As an admin, you can provision capabilities using the CLI with the subcommands of flowctl auth roles. For example: flowctl auth roles list returns a list of all currently provisioned capabilities flowctl auth roles grant --object-role=acmeCo/ --capability=admin --subject-user-id=userZ grants user Z admin access to acmeCo flowctl auth roles revoke --object-role=outside-org/acmeCo-share/ --capability=read --subject-role=acmeCo/ would be used by an admin of outside-orgto revoke acmeCo/'s read access to outside-org/acmeCo-share/. You can find detailed help for all subcommands using the --help or -h flag. "},{"title":"Connectors","type":0,"sectionRef":"#","url":"reference/Connectors/","content":"Connectors A current list and configuration details for Estuary's connectors can be found on the following pages: Capture connectorsMaterialization connectors You can learn more about how connectors work and how to use them in their conceptual documentation.","keywords":""},{"title":"Capture connectors","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/","content":"","keywords":""},{"title":"Available capture connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#available-capture-connectors","content":"Amazon Kinesis ConfigurationPackage — ghcr.io/estuary/source-kinesis:dev Amazon S3 ConfigurationPackage — ghcr.io/estuary/source-s3:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/source-kafka:dev Exchange Rates API ConfigurationPackage - ghcr.io/estuary/airbyte-source-exchange-rates:dev Facebook Marketing ConfigurationPackage - ghcr.io/estuary/airbyte-source-facebook-marketing:dev Google Analytics ConfigurationPackage - ghcr.io/estuary/airbyte-source-google-analytics-v4:dev Google Cloud Storage ConfigurationPackage — ghcr.io/estuary/source-gcs:dev Google Sheets ConfigurationPackage - ghcr.io/estuary/airbyte-source-google-sheets:dev Hubspot ConfigurationPackage - ghcr.io/estuary/airbyte-source-hubspot:dev Mailchimp ConfigurationPackage - ghcr.io/estuary/airbyte-source-mailchimp:dev MySQL ConfigurationPackage - ghcr.io/estuary/source-mysql:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/source-postgres:dev "},{"title":"Amazon Kinesis","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-kinesis/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#prerequisites","content":"To use this connector, you'll need: One or more Amazon Kinesis streams. For a given capture, all streams must: Contain JSON data onlyBe in the same AWS region An IAM user with the following permissions: ListShards on all resourcesGetRecords on all streams usedGetShardIterator on all streams usedDescribeStream on all streams usedDescribeStreamSummary on all streams used These permissions should be specified with the kinesis: prefix in an IAM policy document. For more details and examples, see Controlling Access to Amazon Kinesis Data in the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Amazon Kinesis source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-secret-access-key&quot; /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a kinesis-compatible API that isn't provided by AWS.\tstring /region\tAWS region\tThe name of the AWS region where the Kinesis stream is located.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tStream name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kinesis:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: ${STREAM_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kinesis stream. Learn more about capture definitions.. "},{"title":"Amazon S3","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-s3/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#prerequisites","content":"To use this connector, either your S3 bucket must be public, or you must have access via a root or IAM user. For public buckets, verify that the access policy allows anonymous reads.For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the S3 source connector. tip You might organize your S3 bucket using prefixes to emulate a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring\t&quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring\t&quot;example-aws-secret-access-key&quot; /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a S3-compatible API that isn't provided by AWS.\tstring /matchKeys\tMatch keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-s3:dev config: bucket: &quot;my-bucket&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different S3 prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/apache-kafka/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#prerequisites","content":"A Kafka cluster with: bootstrap.servers configured so that clients may connect via the desired host and portAn authentication mechanism of choice set up (highly recommended for production environments)Connection security enabled with TLS (highly recommended for production environments) "},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#authentication-and-connection-security","content":"Neither authentication nor connection security are enabled by default in your Kafka cluster, but both are important considerations. Similarly, Flow's Kafka connectors do not strictly require authentication or connection security mechanisms. You may choose to omit them for local development and testing; however, both are strongly encouraged for production environments. A wide variety of authentication methods is available in Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. Behavior using other authentication methods is not guaranteed. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol. If you don't already have authentication enabled on your cluster, Estuary recommends either of listed SASL/SCRAM methods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications. For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentation for details. Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Apache Kafka source connector. "},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to. The Kafka client will be informed of the rest of the cluster nodes by connecting to one of these nodes.\tarray\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /authentication\tAuthentication\tConnection details used to authenticate a client connection to Kafka via SASL.\tnull, object /authentication/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /authentication/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /authentication/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tKafka topic name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: [localhost:9093] tls: system_certificates authentication: mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman bindings: - resource: stream: ${TOPIC_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kafka topic. Learn more about capture definitions.. "},{"title":"Exchange Rates API","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/exchange-rates/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#prerequisites","content":"An API key generated through an Exchange Rate API account. After you sign up, your API key can be found on your account page. You may use the free account, but note that you'll be limited to the default base currency, EUR. "},{"title":"Configuration​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Exchange Rates source connector. "},{"title":"Properties​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/access_key\tAccess key\tYour API access key. The key is case sensitive.\tstring\tRequired /base\tBase currency\tISO reference currency. See the documentation. Free plan doesn't support Source Currency Switching, default base currency is EUR\tstring\tEUR /ignore_weekends\tIgnore weekends\tIgnore weekends? (Exchanges don't run on weekends)\tboolean\ttrue /start_date\tStart date\tThe date in the format YYYY-MM-DD. Data will begin from this date.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData stream from which Flow captures data. Always set to exchange_rates.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-exchange-rates:dev config: base: EUR access_key: &lt;secret&gt; start_date: 2022-01-01 ignore_weekends: true bindings: - resource: stream: exchange_rates syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  This capture definition should only have one binding, as exchange_rates is the only available data stream. Learn more about capture definitions. "},{"title":"Facebook Marketing","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/facebook-marketing/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#supported-data-resources","content":"The following data resources are supported: AdsAd activitiesAd creativesAd insightsBusiness ad accountsCampaignsImagesVideos By default, each resource associated with your Facebook Business account is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#prerequisites","content":"A Facebook Business account, and its Ad Account ID.A Facebook app with: The Marketing API enabled.A Marketing API access token generated.Access upgrade from Standard Access (the default) to Advanced Access. This allows a sufficient rate limit to support the connector. Follow the steps below to meet these requirements. "},{"title":"Setup​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#setup","content":"Find your Facebook Business Ad Account ID. You'll use this for the Business ID property. In Meta for Developers, create a new app of the type Business. On your new app's dashboard, click the button to set up the Marketing API. On the Marketing API Tools tab, generate a Marketing API access token with all available permissions (ads_management, ads_read, read_insights, and business_management). Request Advanced Access for your app. Specifically request the Advanced Access to the following: The feature Ads Management Standard Access The permission ads_read The permission ads_management Once your request is approved, you'll have a high enough rate limit to proceed with running the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Facebook Marketing source connector. "},{"title":"Properties​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#properties","content":"Endpoint​ By default, this connector captures all data associated with your Business Ad Account. You can refine the data you capture from Facebook Marketing using the optional Custom Insights configuration. You're able to specify certain fields to capture and apply data breakdowns.Breakdowns are a feature of the Facebook Marketing Insights API that allows you to group API output by common metrics.Action breakdownsare a subset of breakdowns that must be specified separately. Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe value of the access token generated.\tstring\tRequired /business_id\tBusiness ID\tYour Facebook Business Ad Account ID. The connector will fetch all the Ad Accounts that this business has access to.\tstring\tRequired /custom_insights\tCustom Insights\tA list which contains insights entries. Each entry must have a name and can contains fields, breakdowns or action_breakdowns\tarray /custom_insights/-/action_breakdowns\tAction Breakdowns\tA list of chosen action_breakdowns to apply\tarray\t[] /custom_insights/-/action_breakdowns/-\tValidActionBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/breakdowns\tBreakdowns\tA list of chosen breakdowns to apply\tarray\t[] /custom_insights/-/breakdowns/-\tValidBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/end_date\tEnd Date\tThe date until which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z. All data generated between the start date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /custom_insights/-/fields\tFields\tA list of chosen fields to capture\tarray\t[] /custom_insights/-/fields/-\tValidEnums\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/name\tName\tThe name of the insight\tstring /custom_insights/-/start_date\tStart Date\tThe date from which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z.\tstring /custom_insights/-/time_increment\tTime Increment\tTime window in days by which to aggregate statistics. The sync will be chunked into N day intervals, where N is the number of days you specified. For example, if you set this value to 7, then all statistics will be reported as 7-day aggregates by starting from the start_date. If the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).\tinteger\t1 /end_date\tEnd Date\tThe date until which you'd like to capture data, in the format YYYY-MM-DDT00:00:00Z. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /fetch_thumbnail_images\tFetch Thumbnail Images\tIn each Ad Creative, fetch the thumbnail_url and store the result in thumbnail_data_url\tboolean\tfalse /include_deleted\tInclude Deleted\tInclude data from deleted Campaigns, Ads, and AdSets\tboolean\tfalse /page_size\tPage Size of Requests\tPage size used when sending requests to Facebook API to specify number of records per page when response has pagination. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t25 /start_date\tStart Date\tThe date from which you'd like to begin capturing data, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Facebook Marketing account from which collections are captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-facebook-marketing:dev config: access_token: &lt;secret&gt; business_id: 000000000000000 start_date: 2022-03-01T00:00:00Z custom_insights: - name: my-custom-insight fields: [ad_id, account_currency] breakdowns: [device_platform] action_breakdowns: [action_type] start_date: 2022-03-01T00:00:00Z bindings: - resource: stream: ad_account syncMode: incremental target: ${PREFIX}/ad_account - resource: stream: ad_sets syncMode: incremental target: ${PREFIX}/ad_sets - resource: stream: ads_insights syncMode: incremental target: ${PREFIX}/ads_insights - resource: stream: ads_insights_age_and_gender syncMode: incremental target: ${PREFIX}/ads_insights_age_and_gender - resource: stream: ads_insights_country syncMode: incremental target: ${PREFIX}/ads_insights_country - resource: stream: ads_insights_region syncMode: incremental target: ${PREFIX}/ads_insights_region - resource: stream: ads_insights_dma syncMode: incremental target: ${PREFIX}/ads_insights_dma - resource: stream: ads_insights_platform_and_device syncMode: incremental target: ${PREFIX}/ads_insights_platform_and_device - resource: stream: ads_insights_action_type syncMode: incremental target: ${PREFIX}/ads_insights_action_type - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns - resource: stream: activities syncMode: incremental target: ${PREFIX}/activities - resource: stream: ads syncMode: incremental target: ${PREFIX}/ads - resource: stream: ad_creatives syncMode: full_refresh target: ${PREFIX}/ad_creatives  Learn more about capture definitions. "},{"title":"Google Cloud Storage","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/gcs/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#prerequisites","content":"To use this connector, either your GCS bucket must be public, or you must have access via a Google service account. For public buckets, verify that objects in the bucket are publicly readable.For buckets accessed by a Google Service Account: Ensure that the user has been assigned a role with read access.Create a JSON service account key. Google's Application Default Credentials will use this file for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the GCS source connector. tip You might use prefixes to organize your GCS bucket in a way that emulates a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/ascendingKeys\tAscending keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering.\tboolean\tfalse /bucket\tBucket\tName of the Google Cloud Storage bucket\tstring\tRequired /googleCredentials\tGoogle service account\tContents of the service account JSON file. Required unless the bucket is public.\tstring /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose key (relative to the prefix) matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gcs:dev config: bucket: my-bucket googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different GCS prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Advanced: Configure Google service account impersonation​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#advanced-configure-google-service-account-impersonation","content":"info This is an advanced workflow and is typically not necessary to successfully configure this connector. As part of your Google IAM management, you may have configured one service account to impersonate another service account. You may find this useful when you want to easily control access to multiple service accounts with only one set of keys. If necessary, you can configure this authorization model for a GCS capture in Flow using the GitOps workflow. To do so, you'll enable sops encryption and impersonate the target account with JSON credentials. Before you begin, make sure you're familiar with how to encrypt credentials in Flow using sops. Use the following sample as a guide to add the credentials JSON to the capture's endpoint configuration. The sample uses the encrypted suffix feature of sops to encrypt only the sensitive credentials, but you may choose to encrypt the entire configuration. config: bucket: &lt;bucket-name&gt; googleCredentials_sops: # URL containing the account to impersonate and the associated project service_account_impersonation_url: https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/&lt;target-account&gt;@&lt;project&gt;.iam.gserviceaccount.com:generateAccessToken # Credentials for the account that has been configured to impersonate the target. source_credentials: # In addition to the listed fields, copy and paste the rest of your JSON key file as your normally would # for the `googleCredentials` field client_email: &lt;origin-account&gt;@&lt;anotherproject&gt;.iam.gserviceaccount.com token_uri: https://oauth2.googleapis.com/token type: service_account type: impersonated_service_account  "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Sheet: using OAuth, and by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Prerequisites for OAuth​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#prerequisites-for-oauth","content":"Beta OAuth implementation is under active development and is coming soon. Use the service account key method for now. A link to a Google spreadsheet. Simply copy the link from your browser. Your Google account username and password. "},{"title":"Prerequisites using a service account key​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#prerequisites-using-a-service-account-key","content":"A link to a Google spreadsheet. Simply copy the link from your browser. Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Share your Google spreadsheet with the service account. You may either share the sheet so that anyone with the link can view it, or share explicitly with the service account's email address. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Google Sheets source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tGoogle API Credentials for connecting to Google Sheets and Google Drive APIs\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/service_account_info\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /spreadsheet_id\tSpreadsheet Link\tThe link to your spreadsheet.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tSheet\tEach sheet in your Google Sheets document.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to full_refresh.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-google-sheets:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; spreadsheet_id: https://docs.google.com/spreadsheets/... bindings: - resource: stream: Sheet1 syncMode: full_refresh target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Google Analytics","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-analytics/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#supported-data-resources","content":"The following data resources are captured to Flow collections by default: Website overviewTraffic sourcesPagesLocationsMonthly active usersFour weekly active usersTwo weekly active usersWeekly active usersDaily active usersDevices Each resource is mapped to a Flow collection through a separate binding. You can also configure custom reports. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Google Analytics view: using OAuth, and by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Prerequisites for OAuth​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#prerequisites-for-oauth","content":"Beta OAuth implementation is under active development and is coming soon. Use the service account key method for now. The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Your Google account username and password. "},{"title":"Prerequisites using a service account key​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#prerequisites-using-a-service-account-key","content":"The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Google Analytics and Google Analytics Reporting APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source Google Analytics view. Follow the steps below to meet these prerequisites: Enable the Google Analytics and Google Analytics Reporting APIs for the Google project with which your Analytics view is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON keyDuring setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Add the service account to the Google Analytics view. Grant the account Viewer permissions (formerly known as Read &amp; Analyze permissions). "},{"title":"Configuration​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Google Analytics source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/credentials_json\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from GA.\tstring /start_date\tStart Date\tThe date in the format YYYY-MM-DD. Any data before this date will not be replicated.\tstring\tRequired /view_id\tView ID\tThe ID for the Google Analytics View you want to fetch data from. This can be found from the Google Analytics Account Explorer: https://ga-dev-tools.appspot.com/account-explorer/\tstring\tRequired /window_in_days\tWindow in days (Optional)\tThe amount of days each stream slice would consist of beginning from start_date. Bigger the value - faster the fetch. (Min=1, as for a Day; Max=364, as for a Year).\tinteger\t1 Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData resource from the Google Analytics view.\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Custom reports​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#custom-reports","content":"You can include data beyond the default data resources with Custom Reports. These replicate the functionality of Custom Reports in the Google Analytics Web console. To do so, fill out the Custom Reports property witha JSON array as a string with the following schema: [{&quot;name&quot;: string, &quot;dimensions&quot;: [string], &quot;metrics&quot;: [string]}]  You may specify default Google Analytics dimensions and metrics from the table below, or custom dimensions and metrics you've previously defined. Each custom report may contain up to 7 unique dimensions and 10 unique metrics. You must include the ga:date dimension for proper data flow. Supported GA dimensions\tSupported GA metricsga:browser\tga:14dayUsers ga:city\tga:1dayUsers ga:continent\tga:28dayUsers ga:country\tga:30dayUsers ga:date\tga:7dayUsers ga:deviceCategory\tga:avgSessionDuration ga:hostname\tga:avgTimeOnPage ga:medium\tga:bounceRate ga:metro\tga:entranceRate ga:operatingSystem\tga:entrances ga:pagePath\tga:exits ga:region\tga:newUsers ga:socialNetwork\tga:pageviews ga:source\tga:pageviewsPerSession ga:subContinent\tga:sessions ga:sessionsPerUser ga:uniquePageviews ga:users "},{"title":"Sample​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-google-analytics-v4:dev config: view_id: 000000000 start_date: 2022-03-01 credentials: auth_type: service credentials_json: &lt;secret&gt; window_in_days: 1 bindings: - resource: stream: daily_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: devices syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: four_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: locations syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: monthly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: pages syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: traffic_sources syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: two_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: website_overview syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Hubspot","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/hubspot/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#supported-data-resources","content":"By default, each resource associated with your Hubspot account is mapped to a Flow collection through a separate binding. The following data resources are supported for all subscription levels: CampaignsCompaniesContact ListsContactsContacts List MembershipsDeal PipelinesDealsEmail EventsEngagementsEngagements CallsEngagements EmailsEngagements MeetingsEngagements NotesEngagements TasksFormsForm SubmissionsLine ItemsOwnersProductsProperty HistoryQuotesSubscription ChangesTicketsTicket Pipelines The following data resources are supported for pro accounts (set Subscription type to pro in the configuration): Feedback SubmissionsMarketing EmailsWorkflows "},{"title":"Prerequisites​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#prerequisites","content":"There are two ways to authenticate with Hubspot when capturing data: using OAuth, and with an API key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the API key method is the only supported method using the command line. "},{"title":"Prerequisites for OAuth​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#prerequisites-for-oauth","content":"Beta OAuth implementation is under active development and is coming soon. Use the API key method for now. A Hubspot account "},{"title":"Prerequisites using an API key​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#prerequisites-using-an-api-key","content":"A Hubspot account A Hubspot API key "},{"title":"Configuration​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Hubspot source connector. "},{"title":"Properties​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#properties","content":"Endpoint​ The following properties reflect the API Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication mechanism\tChoose how to authenticate to HubSpot.\tobject\tRequired /credentials/credentials_title\tCredentials set\tType of credentials. Set to API Key Credentials\tstring\tRequired /credentials/api_key\tAPI Key\tYour Hubspot API key\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /subscription_type\tYour HubSpot account subscription type\tSome streams are only available to certain subscription packages, we use this information to select which streams to pull data from.\tstring\t&quot;starter&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tData resource\tName of the data resource.\tstring\tRequired /syncMode\tSync Mode\tConnection method\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-hubspot:dev config: credentials: credentials_title: API Key Credentials api_key: &lt;secret&gt; bindings: - resource: stream: companies syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your configuration will have many more bindings representing all supported resourcesin your Hubspot account. Learn more about capture definitions. "},{"title":"Mailchimp","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/mailchimp/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#prerequisites","content":"There are two ways to authenticate with MailChimp when capturing data: using OAuth, and with an API key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the API key method is the only supported method using the command line. "},{"title":"Prerequisites for OAuth​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#prerequisites-for-oauth","content":"Beta OAuth implementation is under active development and is coming soon. Use the API key method for now. A Mailchimp account "},{"title":"Prerequisites using an API key​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#prerequisites-using-an-api-key","content":"A Mailchimp account A Mailchimp API key "},{"title":"Configuration​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the Mailchimp source connector. "},{"title":"Properties​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#properties","content":"Endpoint​ The following properties reflect the API Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tAuthentication Type and Details\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication type. Set to apikey.\tstring\tRequired /credentials/apikey\tAPI Key\tYour Mailchimp API key\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tResource\tMailchimp lists, campaigns, or email_activity\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/airbyte-source-mailchimp:dev config: credentials: auth_type: apikey apikey: &lt;secret&gt; bindings: - resource: stream: lists syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: email_activity syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"MySQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/MySQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#prerequisites","content":"To use this connector, you'll need a MySQL database setup with the following: binlog_row_metadatasystem variable set to FULL. Note that this can be done on a dedicated replica even if the primary database has it set to MINIMAL. Binary log expiration period of at at least seven days. If possible, it's recommended to keep the default setting of 2592000 seconds (30 days).A watermarks table. The watermarks table is a small &quot;scratch space&quot; to which the connector occasionally writes a small amount of data (a UUID, specifically) to ensure accuracy when backfilling preexisting table contents. The default name is &quot;flow.watermarks&quot;, but this can be overridden in config.json. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to insert, update, and delete on the watermarks table.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. "},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#setup","content":"To meet these requirements, do the following: Create the watermarks table. This table can have any name and be in any database, so long as config.json is modified accordingly. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Configure the binary log to record complete table metadata. SET PERSIST binlog_row_metadata = 'FULL';  Configure the binary log to retain data for at least seven days, if previously set lower. If possible, it's recommended to use the default MySQL setting of 2592000 seconds (30 days). SET PERSIST binlog_expire_logs_seconds = 2592000;  "},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the MySQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address and Port\tThe host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /advanced/watermarks_table\tWatermarks Table Name\tThe name of the table used for watermark writes. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;flow.watermarks&quot; /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually. "},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"MySQL on managed cloud platforms​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#mysql-on-managed-cloud-platforms","content":"In addition to standard MySQL, this connector supports cloud-based MySQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#amazon-rds","content":"You can use this connector for MySQL instances on Amazon RDS using the following setup instructions. Estuary recommends creating a read replicain RDS for use with Flow; however, it's not required. You're able to apply the connector directly to the primary instance if you'd like. Setup​ You'll need to configure secure access to the database to enable the Flow capture. Estuary recommends SSH tunneling to allow this. Follow the guide to configure an SSH server for tunneling. beta SSH tunneling on the MySQL source connector is actively being worked on and will be fully supported soon. If you encounter issues with this feature, contact Estuary support. Create a RDS parameter group to enable replication in MySQL. Create a parameter group. Create a unique name and description and set the following properties: Family: mysql 8.0Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 If using the primary instance (not recommended), associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Create a read replica with the new parameter group applied (recommended). Create a read replicaof your MySQL database. Modify the replicaand set the following: DB parameter group: choose the parameter group you created previouslyBackup retention period: 7 days Reboot the replica to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Run the following command to set the binary log retention to seven days: CALL mysql.rds_set_configuration('binlog retention hours', 168);  "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#google-cloud-sql","content":"Google Cloud SQL doesn't currently support the setting binlog_row_metadata: FULL, which this connector requires. As a result, this connector can't be used directly for MySQL instance on Google Cloud. As an alternative, you can create a read replica outside of Google cloud. The replica can be treated as a standard MySQL instance. Set up an external replica. Follow the standard setup instructions for this connector. "},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#azure-database-for-mysql","content":"Azure Database for MySQL doesn't currently support the setting binlog_row_metadata: FULL, which this connector requires. As a result, this connector can't be used for MySQL instance on Azure. Contact your account manager or Estuary support for help using a third-party connector. Note that third party connectors will require you to create a read replica. "},{"title":"Materialization connectors","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/","content":"","keywords":""},{"title":"Available materialization connectors​","type":1,"pageTitle":"Materialization connectors","url":"reference/Connectors/materialization-connectors/#available-materialization-connectors","content":"Apache Parquet in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-parquet:dev Elasticsearch ConfigurationPackage — ghcr.io/estuary/materialize-elasticsearch:dev Firebolt ConfigurationPackage - ghcr.io/estuary/materialize-firebolt:dev Google BigQuery ConfigurationPackage — ghcr.io/estuary/materialize-bigquery:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/materialize-postgres:dev Rockset ConfigurationPackage — ghcr.io/estuary/materialize-rockset:dev Snowflake ConfigurationPackage — ghcr.io/estuary/materialize-snowflake:dev "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#prerequisites","content":"This connector supports PostgreSQL versions 10.0 and later. You'll need a PostgreSQL database setup with the following: Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#setup","content":"info These setup instructions are PostgreSQL instances you manage yourself. If you use a cloud-based managed service for your database, different setup steps may be required. Instructions for setup on Amazon RDS can be found here. If you use a different managed service and the standard steps don't work as expected, contact Estuary support. The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role. For a more restricted setup, create a new user with just the required permissions as detailed in the following steps: Create a new user and password: CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;  Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication: CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES;  Set WAL level to logical: ALTER SYSTEM SET wal_level = logical;  Restart PostgreSQL to allow the WAL level change to take effect. "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog spec YAML. See connectors to learn more about using connectors. The values and YAML sample below provide configuration details specific to the PostgreSQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced/publicationName\tPublication name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/slotName\tSlot name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in &lt;schema&gt;.&lt;table&gt; form.\tstring\t&quot;public.flow_watermarks&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#amazon-rds","content":"You can use this connector for PostgreSQL instances on Amazon RDS using the following setup instructions. Setup​ You'll need to configure secure access to the database to enable the Flow capture. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. Enable logical replication on your RDS PostgreSQL instance. Create a parameter group. Create a unique name and description and set the following properties: Family: postgres13Type: DB Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the database. Reboot the database to allow the new parameter group to take effect. In the PostgreSQL client, run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the forwardHost and forwardPort in the RDS console as the Endpoint and Port, respectively. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#google-cloud-sql","content":"You can use this connector for PostgreSQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow the connector to access your PostgreSQL instance using one of the following methods: Configure secure access. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. You'll need to set up a Google Cloud Virtual Machine to act as a proxy; be sure to follow the prerequisites outlined in the Google Cloud sectionsection of the guide. Configure the instance to allow unsecured connections. In your Cloud SQL settings, disable the requirement for SSL/TLSand enable public IP access, if necessary. Set the cloudsql.logical_decoding flag to on to enable logical replication on your loud SQL PostgreSQL instance. In your PostgreSQL client, issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH REPLICATION IN ROLE cloudsqlsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the forwardHost under Public IP Address. The forwardPort is always 5432. "},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#azure-database-for-postgresql","content":"You can use this connector for instances on Azure Database for PostgreSQL using the following setup instructions. Setup​ You'll need to configure secure access to the database to enable the Flow capture. This is currently supported through SSH tunneling. Follow the guide to configure an SSH server for tunneling. In your Azure PostgreSQL instance's server parameters, set wal_level to logical to enable logical replication. In the PostgreSQL client, run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION; GRANT pg_read_all_data TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON information_schema.columns, information_schema.tables, pg_catalog.pg_attribute, pg_catalog.pg_class, pg_catalog.pg_index, pg_catalog.pg_namespace TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES;  Configure your connector as described in the configuration section above, with the additional of the proxy stanza to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. You can find the host as Server Name, and the port under Connection Strings (usually 5432). "},{"title":"TOASTed values​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#toasted-values","content":"PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately. TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the a value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made. The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update. However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically: When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values "},{"title":"Troubleshooting​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#troubleshooting","content":"If you encounter an issue that you suspect is due to TOASTed values, try the following: Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance. "},{"title":"Google BigQuery","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/BigQuery/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#prerequisites","content":"To use this connector, you'll need: A new Google Cloud Storage bucket in the same region as the BigQuery destination dataset. A Google Cloud service account with a key file generated and the following roles: roles/bigquery.dataEditor on the destination dataset roles/bigquery.jobUser on the project with which the BigQuery destination dataset is associated roles/storage.objectAdminon the GCS bucket created above See Setup for detailed steps to set up your service account. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#setup","content":"To configure your service account, complete the following steps. Log into the Google Cloud console and create a service account. During account creation: Grant the user access to the project.Grant the user roles roles/bigquery.dataEditor, roles/bigquery.jobUser, and roles/storage.objectAdmin.Click Done. Select the new service account from the list of service accounts. On the Keys tab, click Add key and create a new JSON key. The key is automatically downloaded to your machine. In your terminal, base64-encode the JSON key: base64 /path/to/key.json You'll use the resulting string when you configure the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a BigQuery materialization, which will direct one or more of your Flow collections to your desired tables within a BigQuery dataset. A BigQuery dataset is the top-level container within a project, and comprises multiple tables. You can think of a dataset as somewhat analogous to a schema in a relational database. For a complete introduction to resource organization in Bigquery, see the BigQuery docs. "},{"title":"Properties​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe project ID for the Google Cloud Storage bucket and BigQuery dataset.\tString\tRequired /billing_project_id\tBilling project ID\tThe project ID to which these operations are billed in BigQuery. Typically, you want this to be the same as project_id (the default).\tString\tSame as project_id /dataset\tDataset\tName of the target BigQuery dataset.\tString\tRequired /region\tRegion\tThe GCS region.\tString\tRequired /bucket\tBucket\tName of the GCS bucket.\tString\tRequired /bucket_path\tBucket path\tBase path within the GCS bucket. Also called &quot;Folder&quot; in the GCS console.\tString\tRequired /credentials_json\tCredentials JSON\tBase64-encoded string of the full service account file.\tByte\tRequired To learn more about project billing, see the BigQuery docs. Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name.\tstring\tRequired /delta_updates\tDelta updates.\tWhether to use standard or delta updates\tboolean\tfalse "},{"title":"Sample​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: project_id: our-bigquery-project dataset: materialized-data region: US bucket: our-gcs-bucket bucket_path: bucket-path/ credentials_json: SSBqdXN0IHdhbm5hIHRlbGwgeW91IGhvdyBJJ20gZmVlbGluZwpHb3R0YSBtYWtlIHlvdSB1bmRlcnN0YW5kCk5ldmVyIGdvbm5hIGdpdmUgeW91IHVwCk5ldmVyIGdvbm5hIGxldCB5b3UgZG93bgpOZXZlciBnb25uYSBydW4gYXJvdW5kIGFuZCBkZXNlcnQgeW91Ck5ldmVyIGdvbm5hIG1ha2UgeW91IGNyeQpOZXZlciBnb25uYSBzYXkgZ29vZGJ5ZQpOZXZlciBnb25uYSB0ZWxsIGEgbGllIGFuZCBodXJ0IHlvdQ== image: ghcr.io/estuary/materialize-bigquery:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your BigQuery table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in BigQuery won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"Elasticsearch","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Elasticsearch/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#prerequisites","content":"To use this connector, you'll need: An Elastic cluster with a known endpoint If the cluster is on the Elastic Cloud, you'll need an Elastic user with a role that grants all privileges on indices you plan to materialize to within the cluster. See Elastic's documentation on defining roles andsecurity privileges. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an Elasticsearch materialization, which will direct the contents of these Flow collections into Elasticsearch indices. By default, the connector attempts to map each field in the Flow collection to the most appropriate Elasticsearch field type. However, because each JSON field type can map to multiple Elasticsearch field types, you may want to override the defaults. You can configure this by adding field_overrides to the collection's binding in the materialization specification. To do so, provide a JSON pointer to the field in the collection schema, choose the output field type, and specify additional properties, if necessary. "},{"title":"Properties​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/endpoint\tEndpoint\tEndpoint host or URL. If using Elastic Cloud, this follows the format https://CLUSTER_ID.REGION.CLOUD_PLATFORM.DOMAIN:PORT.\tstring\tRequired /password\tPassword\tPassword to connect to the endpoint.\tstring /username\tUsername\tUser to connect to the endpoint.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\tRequired /field_overrides\tField overrides\tAssign Elastic field type to each collection field.\tarray\tRequired /field_overrides/-/es_type\tElasticsearch type\tThe overriding Elasticsearch data type of the field.\tobject /field_overrides/-/es_type/date_spec\tDate specifications\tConfiguration for the date field, effective if field_type is 'date'. See Elasticsearch docs.\tobject /field_overrides/-/es_type/date_spec/format\tDate format\tFormat of the date. See Elasticsearch docs.\tstring /field_overrides/-/es_type/field_type\tField type\tThe Elasticsearch field data types. Supported types include: boolean, date, double, geo_point, geo_shape, keyword, long, null, text.\tstring /field_overrides/-/es_type/keyword_spec\tKeyword specifications\tConfiguration for the keyword field, effective if field_type is 'keyword'. See Elasticsearch docs\tobject /field_overrides/-/es_type/keyword_spec/ignore_above\tIgnore above\tStrings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs\tinteger /field_overrides/-/es_type/text_spec\tText specifications\tConfiguration for the text field, effective if field_type is 'text'.\tobject /field_overrides/-/es_type/text_spec/dual_keyword\tDual keyword\tWhether or not to specify the field as text/keyword dual field.\tboolean /field_overrides/-/es_type/text_spec/keyword_ignore_above\tIgnore above\tEffective only if Dual Keyword is enabled. Strings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs.\tinteger /field_overrides/-/pointer\tPointer\tA '/'-delimited json pointer to the location of the overridden field.\tstring /index\tindex\tName of the ElasticSearch index to store the materialization results.\tstring\tRequired /number_of_replicas\tNumber of replicas\tThe number of replicas in ElasticSearch index. If not set, default to be 0. For single-node clusters, make sure this field is 0, because the Elastic search needs to allocate replicas on different nodes.\tinteger\t0 /number_of_shards\tNumber of shards\tThe number of shards in ElasticSearch index. Must set to be greater than 0.\tinteger\t1 "},{"title":"Sample​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-elasticsearch:dev config: endpoint: https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 username: flow_user password: secret # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: index: last-updated delta_updates: false field_overrides: - pointer: /updated-date es_type: field_type: date date_spec: format: yyyy-MM-dd source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#delta-updates","content":"This connector supports both standard and delta updates. You must choose an option for each binding. Learn more about delta updates and the implications of using each update type. "},{"title":"Firebolt","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Firebolt/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#prerequisites","content":"To use this connector, you'll need: A Firebolt database with at least one engine The engine must be started before creating the materialization.It's important that the engine stays up throughout the lifetime of the materialization. To ensure this is the case, select Edit Engine on your engine. In the engine settings, set Auto-stop engine after to Always On. An S3 bucket where JSON documents will be stored prior to loading The bucket must be in a supported AWS region matching your Firebolt database.The bucket may be public, or may be accessible by an IAM user. To configure your IAM user, see the steps below. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#setup","content":"For non-public buckets, you'll need to configure access in AWS IAM. Follow the Firebolt documentation to set up an IAM policy and role, and add it to the external table definition. Create a new IAM user. During setup: Choose Programmatic (access key) access. This ensures that an access key ID and secret access key are generated. You'll use these to configure the connector. On the Permissions page, choose Attach existing policies directly and attach the policy you created in step 1. After creating the user, download the IAM credentials file. Take note of the access key ID and secret access key and use them to configure the connector. See the Amazon docs if you lose your credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Firebolt materialization, which will direct Flow data to your desired Firebolt tables via an external table. "},{"title":"Properties​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/aws_key_id\tAWS key ID\tAWS access key ID for accessing the S3 bucket.\tstring /aws_region\tAWS region\tAWS region the bucket is in.\tstring /aws_secret_key\tAWS secret access key\tAWS secret key for accessing the S3 bucket.\tstring /database\tDatabase\tName of the Firebolt database.\tstring\tRequired /engine_url\tEngine URL\tEngine URL of the Firebolt database, in the format: &lt;engine-name&gt;.&lt;organization&gt;.&lt;region&gt;.app.firebolt.io.\tstring\tRequired /password\tPassword\tFirebolt password.\tstring\tRequired /s3_bucket\tS3 bucket\tName of S3 bucket where the intermediate files for external table will be stored.\tstring\tRequired /s3_prefix\tS3 prefix\tA prefix for files stored in the bucket.\tstring /username\tUsername\tFirebolt username.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the Firebolt table to store materialized results in. The external table will be named after this table with an _external suffix.\tstring\tRequired /table_type\tTable type\tType of the Firebolt table to store materialized results in. See the Firebolt docs for more details.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: my-db engine_url: my-db-my-engine-name.my-organization.us-east-1.app.firebolt.io password: secret # For public S3 buckets, only the bucket name is required s3_bucket: my-bucket username: firebolt-user # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-firebolt:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: table-name table_type: fact source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#delta-updates","content":"The Firebolt connector operates only in delta updates mode. Firebolt stores all deltas — the unmerged collection documents — directly. In some cases, this will affect how materialized views look in Firebolt compared to other systems that use standard updates. "},{"title":"Reserved words​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#reserved-words","content":"Firebolt has a list of reserved words, which my not be used in identifiers. Collections with field names that include a reserved word may not be materialized to Firebolt. Reserved words all\tfalse\tor alter\tfetch\torder and\tfirst\touter array\tfloat\tover between\tfrom\tpartition bigint\tfull\tprecision bool\tgenerate\tprepare boolean\tgroup\tprimary both\thaving\tquarter case\tif\tright cast\tilike\trow char\tin\trows concat\tinner\tsample copy\tinsert\tselect create\tint\tset cross\tinteger\tshow current_date\tintersect\ttext current_timestamp\tinterval\ttime database\tis\ttimestamp date\tisnull\ttop datetime\tjoin\ttrailing decimal\tjoin_type\ttrim delete\tleading\ttrue describe\tleft\ttruncate distinct\tlike\tunion double\tlimit\tunknown_char doublecolon\tlimit_distinct\tunnest dow\tlocaltimestamp\tunterminated_string doy\tlong\tupdate drop\tnatural\tusing empty_identifier\tnext\tvarchar epoch\tnot\tweek except\tnull\twhen execute\tnumeric\twhere exists\toffset\twith explain\ton extract\tonly\t "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#prerequisites","content":"To use this connector, you'll need: At least one Flow collection. If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. The spreadsheet ID for your Google spreadsheet. This is a string of characters that can be found as a segment of the spreadsheet URL in your browser. The example below shows this structure: https://docs.google.com/spreadsheets/d/SPREADSHEETID/edit#gid=0 Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Google Service Account parameter when you configure the connector. Share your Google spreadsheet with the service account, granting edit access. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Sheets materialization. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/googleCredentials\tGoogle Service Account\tService account JSON key to use as Application Default Credentials\tstring\tRequired /spreadsheetId\tSpreadsheet ID\tID of the spreadsheet to materialize, which is shared with the service account.\tstring\tRequired Bindings​ Configure a separate binding for each collection you want to materialize to a sheet. Note that the connector will add an addition column to the beginning of each sheet; this is to track the internal state of the data. Property\tTitle\tDescription\tType\tRequired/Default/sheet\tSheet Name\tName of the spreadsheet sheet to materialize into\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: googleCredentials: &lt;secret&gt; spreadsheetID: &lt;string&gt; image: ghcr.io/estuary/materialize-google-sheets:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: sheet: my_sheet source: ${PREFIX}/${source_collection}  "},{"title":"Apache Parquet in S3","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Parquet/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#prerequisites","content":"To use this connector, you'll need: An AWS root or IAM user with access to the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a materialization, which will direct the contents of these Flow collections to Parquet files in S3. "},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS Access Key ID\tAWS credential used to connect to S3.\tstring\tRequired /awsSecretAccessKey\tAWS Secret Access Key\tAWS credential used to connect to S3.\tstring\tRequired /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a S3-compatible API that isn't provided by AWS\tstring. /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring /uploadIntervalInSeconds\tUpload Interval in Seconds\tTime interval, in seconds, at which to upload data from Flow to S3.\tinteger\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/compressionType\tCompression type\tThe method used to compress data in Parquet.\tstring /pathPrefix\tPath prefix\tThe desired Parquet file path within the bucket as determined by an S3 prefix.\tstring\tRequired The following compression types are supported: snappygziplz4zstd "},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: config: awsAccessKeyId: AKIAIOSFODNN7EXAMPLE awsSecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYSECRET bucket: my-bucket uploadIntervalInSeconds: 300 # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-s3-parquet:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: pathPrefix: /my-prefix source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#delta-updates","content":"This connector uses only delta updates mode. Collection documents are converted to Parquet format and stored in their unmerged state. "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#prerequisites","content":"To use this connector, you'll need: A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /host\tHost\tHost name of the database.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /port\tPort\tPort on which to connect to the database.\tinteger /user\tUser\tDatabase user to connect as.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow host: localhost password: flow port: 5432 user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances. To connect securely, you must use an SSH tunnel. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#setup","content":"Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the additional of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. tip You can find the values for forwardHost and forwardPort in the following locations in each platform's console: Amazon RDS: forwardHost as Endpoint; forwardPort as Port.Google Cloud SQL: forwardHost as Private IP Address; forwardPort is always 5432. You may need to configure private IP on your database.Azure Database: forwardHost as Server Name; forwardPort under Connection Strings (usually 5432). "},{"title":"Reserved words​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#reserved-words","content":"PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation. These reserve words are listed in the table below. Flow automatically quotes fields that are in this list. Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear "},{"title":"Rockset","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Rockset/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites","content":"To use this connector, you'll need: A Rockset account with an API key generated from the web UIA Rockset workspace Optional; if none exist, one will be created by the connector. A Rockset collection Optional; if none exist, one will be created by the connector. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections. "},{"title":"Properties​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI key\tThe key used to authenticate to the Rockset API.\tString\tRequired Bindings​ The binding configuration for this connector includes two optional sections.Backfill from S3 allows you to backfill historical data from an S3 bucket, as detailed below.Advanced collection settings includes settings that may help optimize your resulting Rockset collections: Clustering fields: You can specify clustering fields for your Rockset collection's columnar index to help optimize specific query patterns. See the Rockset docs for more information.Event time field: All Rockset documents have an associated _event_time field, which is created for each collection. You can specify an existing integer or timestamp field in your data to be used for _event_time. See the Rockset docs for more information.Insert only: Disallows updates and deletes. The materialization will fail if there are documents with duplicate keys, but indexing in Rockset will be more efficient.Retention period: Amount of time before data is purged, in seconds. A low value will keep the amount of data indexed in Rockset smaller. Property\tTitle\tDescription\tType\tRequired/Default/advancedCollectionSettings\tAdvanced Collection Settings object /advancedCollectionSettings/clustering_key\tClustering Key\tList of clustering fields\tarray /advancedCollectionSettings/clustering_key/-/field_name\tField Name\tThe name of a field\tstring /advancedCollectionSettings/event_time_info\tEvent Time Info object /advancedCollectionSettings/event_time_info/field\tField Name\tName of the field containing the event time\tstring /advancedCollectionSettings/event_time_info/format\tFormat\tFormat of the time field\tstring /advancedCollectionSettings/event_time_info/time_zone\tTimezone\tDefault timezone\tstring /advancedCollectionSettings/insert_only\tInsert Only\tIf true disallows updates and deletes. The materialization will fail if there are documents with duplicate keys.\tboolean /advancedCollectionSettings/retention_secs\tRetention Period\tNumber of seconds after which data is purged based on event time\tinteger /collection\tRockset Collection\tThe name of the Rockset collection (will be created if it does not exist)\tstring /initializeFromS3\tBackfill from S3 object /initializeFromS3/bucket\tBucket\tThe name of the S3 bucket to load data from.\tstring /initializeFromS3/integration\tIntegration Name\tThe name of the integration that was previously created in the Rockset UI\tstring /initializeFromS3/pattern\tPattern\tA regex that is used to match objects to be ingested\tstring /initializeFromS3/prefix\tPrefix\tPrefix of the data within the S3 bucket. All files under this prefix will be loaded. Optional. Must not be set if 'pattern' is defined.\tstring /initializeFromS3/region\tRegion\tThe AWS region in which the bucket resides. Optional.\tstring /workspace\tWorkspace\tThe name of the Rockset workspace (will be created if it does not exist)\tstring\t "},{"title":"Sample​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: api_key: supersecret # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-rockset:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: workspace: ${namespace_name} collection: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates and reduction strategies​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#delta-updates-and-reduction-strategies","content":"The Rockset connector operates only in delta updates mode. This means that Rockset, rather than Flow, performs the document merge. In some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates. Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of RFC 7396 - JSON merge. This differs from how Flow would reduce documents, most notably in that Rockset will not honor any reduction strategies defined in your Flow schema. For consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations in Flow mirror Rockset's semantics. To accomplish this, ensure that your collection schema has the following data reductions defined in its schema: A top-level reduction strategy of mergeA strategy of lastWriteWins for all nested values (this is the default) "},{"title":"Bulk ingestion for large backfills of historical data​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#bulk-ingestion-for-large-backfills-of-historical-data","content":"You can backfill large amounts of historical data into Rockset using a bulk ingestion. Bulk ingestion must originate in S3 and requires additional steps in your dataflow. This workflow is supported using the flowctl CLI. "},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites-1","content":"Before completing this workflow, make sure you have: A working catalog spec including at least one Flow collection.A production or development environment tip The following is an intermediate workflow. As needed, refer to this guide for the basic steps to create and deploy a catalog spec using the GitOps workflow. "},{"title":"How to perform a bulk ingestion​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#how-to-perform-a-bulk-ingestion","content":"A bulk ingestion from a Flow collection into Rockset is essentially a two-step process. First, Flow writes your historical data into an S3 bucket using Estuary's S3-Parquet materialization connector. Once the data is caught up, it uses the Rockset connector to backfill the data from S3 into Rockset and then switches to the Rockset Write API for the continuous materialization of new data. graph TD A[Create an S3 integration in Rockset] --&gt; B B[Create Flow materialization into S3 bucket] --&gt; C C[Wait for S3 materialization to catch up with historical data] --&gt;|When ready to bulk ingest into Rockset| D D[Disable S3 materialization shards] --&gt; E E[Update same materialization to use the Rockset connector with the integration created in first step] --&gt; F F[Rockset connector automatically continues materializing after the bulk ingestion completes] To set this up, use the following procedure as a guide, substituting example/flow/collection for your collection: You'll need an S3 integration in Rockset. To create one, follow the instructions here, but do not create the Rockset collection yet.Create and activate a materialization of example/flow/collection into a unique prefix within an S3 bucket of your choosing. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-s3-parquet:dev config: bucket: example-s3-bucket region: us-east-1 awsAccessKeyId: &lt;your key&gt; awsSecretAccessKey: &lt;your secret&gt; uploadIntervalInSeconds: 300 bindings: - resource: pathPrefix: example/s3-prefix/ source: example/flow/collection Once the S3 materialization is caught up with your historical data, you'll switch to the Rockset write API for your future data. To make the switch, first disable the S3 materialization by setting shards to disabled in the definition, and re-deploy the catalog. This is necessary to ensure correct ordering of documents written to Rockset. materializations: example/toRockset: shards: disable: true # ...the remainder of the materialization yaml remains the same as above Update the materialization to use the materialize-rockset connector, and re-enable the shards. Here you'll provide the name of the Rockset S3 integration you created above, as well as the bucket and prefix that you previously materialized into. It's critical that the name of the materialization remains the same as it was for materializing into S3. materializations: example/toRockset: endpoint: connector: image: ghcr.io/estuary/materialize-rockset:dev config: api_key: &lt;your rockset API key here&gt; bindings: - resource: workspace: &lt;your rockset workspace name&gt; collection: &lt;your rockset collection name&gt; initializeFromS3: integration: &lt;rockset integration name&gt; bucket: example-s3-bucket region: us-east-1 prefix: example/s3-prefix/ source: example/flow/collection When you activate the new materialization, the connector will create the Rockset collection using the given integration, and wait for it to ingest all of the historical data from S3 before it continues. Once this completes, the Rockset connector will automatically switch over to the incoming stream of new data. "},{"title":"Snowflake","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#prerequisites","content":"To use this connector, you'll need: A Snowflake account that includes: A target database, to which you'll materialize dataA schema — a logical grouping of database objects — within the target databaseA user with a role assigned that grants the MODIFY privilege on the target database At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Snowflake materialization, which will direct one or more of your Flow collections to new Snowflake tables. "},{"title":"Properties​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account\tAccount\tThe Snowflake account identifier\tstring\tRequired /database\tDatabase\tName of the Snowflake database to which to materialize\tstring\tRequired /password\tPassword\tSnowflake user password\tstring\tRequired /cloud_provider\tCloud Provider\tCloud Provider where the account is located\tstring\tRequired /region\tRegion\tRegion where the account is located\tstring\tRequired /role\tRole\tRole assigned to the user\tstring /schema\tSchema\tSnowflake schema within the database to which to materialize\tstring\tRequired /user\tUser\tSnowflake username\tstring\tRequired /warehouse\tWarehouse\tName of the data warehouse that contains the database\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean /table\tTable\tTable name\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret cloud_provider: aws region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your Snowflake table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Snowflake won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"Optimizing performance for standard updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#optimizing-performance-for-standard-updates","content":"When using standard updates for a large dataset, the collection key you choose can have a significant impact on materialization performance and efficiency. Snowflake uses micro partitions to physically arrange data within tables. Each micro partition includes metadata, such as the minimum and maximum values for each column. If you choose a collection key that takes advantage of this metadata to help Snowflake prune irrelevant micro partitions, you'll see dramatically better performance. For example, if you materialize a collection with a key of /user_id, it will tend to perform far worse than a materialization of /date, /user_id. This is because most materializations tend to be roughly chronological over time, and that means that data is written to Snowflake in roughly /date order. This means that updates of keys /date, /user_id will need to physically read far fewer rows as compared to a key like /user_id, because those rows will tend to live in the same micro-partitions, and Snowflake is able to cheaply prune micro-partitions that aren't relevant to the transaction. "},{"title":"Reserved words​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#reserved-words","content":"Snowflake has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Snowflake's documentation here and in the table below. caution In Snowflake, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Snowflake docs. Reserved words account\tfrom\tqualify all\tfull\tregexp alter\tgrant\trevoke and\tgroup\tright any\tgscluster\trlike as\thaving\trow between\tilike\trows by\tin\tsample case\tincrement\tschema cast\tinner\tselect check\tinsert\tset column\tintersect\tsome connect\tinto\tstart connection\tis\ttable constraint\tissue\ttablesample create\tjoin\tthen cross\tlateral\tto current\tleft\ttrigger current_date\tlike\ttrue current_time\tlocaltime\ttry_cast current_timestamp\tlocaltimestamp\tunion current_user\tminus\tunique database\tnatural\tupdate delete\tnot\tusing distinct\tnull\tvalues drop\tof\tview else\ton\twhen exists\tor\twhenever false\torder\twhere following\torganization\twith for  "},{"title":"Organizing a Flow catalog","type":0,"sectionRef":"#","url":"reference/organizing-catalogs/","content":"","keywords":""},{"title":"import​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#import","content":"Flow's import directive can help you easily handle all of these scenarios while keeping your catalogs well organized. Each catalog spec file may import any number of other files, and each import may refer to either relative or an absolute URL. When you use import in a catalog spec, you're conceptually bringing the entirety of another catalog — as well as the schemas and typescript files it uses — into your catalog. Imports are also transitive, so when you import another catalog, you're also importing everything that other catalog has imported. This allows you to keep your catalogs organized, and is flexible enough to support collaboration between separate teams and organizations. Perhaps the best way of explaining this is with some examples. Example: Organizing collections​ Let's look at a relatively simple case in which you want to organize your collections into multiple catalog files. Say you work for Acme Corp on the team that's introducing Flow. You might start with the collections and directory structure below: acme/customers/customerInfo acme/products/info/manufacturers acme/products/info/skus acme/products/inventory acme/sales/pending acme/sales/complete  acme ├── flow.yaml ├── customers │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml ├── products │ ├── flow.yaml │ ├── info │ │ ├── flow.ts │ │ ├── flow.yaml │ │ └── schemas.yaml │ └── inventory │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml schemas.yaml └── sales ├── flow.ts ├── flow.yaml └── schemas.yaml  It's immediately clear where each of the given collections is defined, since the directory names match the path segments in the collection names. This is not required by theflowctl CLI, but is strongly recommended, since it makes your catalogs more readable and maintainable. Each directory contains a catalog spec (flow.yaml), which will import all of the catalogs from child directories. So, the top-level catalog spec, acme/flow.yaml, might look something like this: import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml  This type of layout has a number of other advantages. During development, you can easily work with a subset of collections using, for example, flowctl test --source acme/products/flow.yaml to run only the tests for product-related collections. It also allows other imports to be more granular. For example, you might want a derivation under sales to read from acme/products/info. Since info has a separate catalog spec, acme/sales/flow.yaml can import acme/products/info/flow.yaml without creating a dependency on the inventory collection. Example: Separate environments​ It's common to use separate environments for tiers like development, staging, and production. Flow catalog specs often necessarily include endpoint configuration for external systems that will hold materialized views. Let's say you want your production environment to materialize views to Snowflake, but you want to develop locally on SQLite. We might modify the Acme example slightly to account for this. acme ├── dev.flow.yaml ├── prod.flow.yaml ... the remainder is the same as above  Each of the top-level catalog specs might import all of the collections and define an endpoint called ourMaterializationEndpoint that points to the desired system. The import block might be the same for each system, but each file may use a different configuration for the endpoint, which is used by any materializations that reference it. Our configuration for our development environment will look like: dev.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml ourMaterializationEndpoint: # dev.flow.yaml sqlite: path: dev-materializations.db  While production will look like: prod.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml endpoints: snowflake: account: acme_production role: admin schema: snowflake.com/acmeProd user: importantAdmin password: abc123 warehouse: acme_production  When we test the draft locally, we'll work with dev.flow.yaml, but we'll publish prod.flow.yaml. Everything will continue to work because in our development environment we'll be binding collections to our local SQLite DB and in production we'll use Snowflake. Example: Cross-team collaboration​ When working across teams, it's common for one team to provide a data product for another to reference and use. Flow is designed for cross-team collaboration, allowing teams and users to reference each other's full catalog or schema.  Again using the Acme example, let's imagine we have two teams. Team Web is responsible for Acme's website, and Team User is responsible for providing a view of Acme customers that's always up to date. Since Acme wants a responsive site that provides a good customer experience, Team Web needs to pull the most up-to-date information from Team User at any point. Let's look at Team User's collections: teamUser.flow.yaml import: - userProfile.flow.yaml  Which references: userProfile.flow.yaml collection: userProfile: schema: -&quot;/userProfile/schema&quot; key: [/id]  Team User references files in their directory, which they actively manage in both their import and schema sections. If Team Web wants to access user data (and they have access), they can use a relative path or a URL-based path given that Team User publishes their data to a URL for access: teamWeb.flow.yaml import: -http://www.acme.com/teamUser#userProfile.flow.yaml -webStuff.flow.yaml  Now Team Web has direct access to collections (referenced by their name) to build derived collections on top of. They can also directly import schemas: webStuff.flow.yaml collection: webStuff: schema: -http://acme.com/teamUser#userProfile/#schema key: [/id]  "},{"title":"Global namespace​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#global-namespace","content":"Every Flow collection has a name, and that name must be unique within a running Flow system. Flow collections should be thought of as existing within a global namespace. Keeping names globally unique makes it easy to import catalogs from other teams, or even other organizations, without having naming conflicts or ambiguities. For example, imagine your catalog for the inside sales team has a collection just named customers. If you later try to import a catalog from the outside sales team that also contains a customers collection, 💥 there's a collision. A better collection name would be acme/inside-sales/customers. This allows a catalog to include customer data from separate teams, and also separate organizations. Learn more about the Flow namespace. "},{"title":"Reduction strategies","type":0,"sectionRef":"#","url":"reference/reduction-strategies/","content":"","keywords":""},{"title":"Reduction guarantees​","type":1,"pageTitle":"Reduction strategies","url":"reference/reduction-strategies/#reduction-guarantees","content":"In Flow, documents that share the same collection key and are written to the same logical partition have a total order, meaning that one document is universally understood to have been written before the other. This isn't true of documents of the same key written to different logical partitions. These documents can be considered “mostly” ordered: Flow uses timestamps to understand the relative ordering of these documents, and while this largely produces the desired outcome, small amounts of re-ordering are possible and even likely. Flow guarantees exactly-once semantics within derived collections and materializations (so long as the target system supports transactions), and a document reduction will be applied exactly one time. Flow does not guarantee that documents are reduced in sequential order, directly into a base document. For example, documents of a single Flow capture transaction are combined together into one document per collection key at capture time – and that document may be again combined with still others, and so on until a final reduction into the base document occurs. Taken together, these total-order and exactly-once guarantees mean that reduction strategies must be associative [as in (2 + 3) + 4 = 2 + (3 + 4) ], but need not be commutative [ 2 + 3 = 3 + 2 ] or idempotent [ S u S = S ]. They expand the palette of strategies that can be implemented, and allow for more efficient implementations as compared to, for example CRDTs. In this documentation, we’ll refer to the “left-hand side” (LHS) as the preceding document and the “right-hand side” (RHS) as the following one. Keep in mind that both the LHS and RHS may themselves represent a combination of still more ordered documents because, for example, reductions are applied associatively. "},{"title":"append","type":0,"sectionRef":"#","url":"reference/reduction-strategies/append/","content":"append append works with arrays, and extends the left-hand array with items from the right-hand side. collections: - name: example/reductions/append schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Append only works with type &quot;array&quot;. # Others will throw an error at build time. type: array reduce: { strategy: append } required: [key] key: [/key] tests: &quot;Expect we can append arrays&quot;: - ingest: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2] } - { key: &quot;key&quot;, value: [3, null, &quot;abc&quot;] } - verify: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2, 3, null, &quot;abc&quot;] } The right-hand side must always be an array. The left-hand side may be null, in which case the reduction is treated as a no-op and its result remains null. This can be combined with schema conditionals to toggle whether reduction-reduction should be done or not.","keywords":""},{"title":"Composing with conditionals","type":0,"sectionRef":"#","url":"reference/reduction-strategies/composing-with-conditionals/","content":"Composing with conditionals Reduction strategies are JSON Schema annotations. As such, their applicability at a given document location can be controlled through the use of conditional keywords within the schema, like oneOf or if/then/else. This means Flow’s built-in strategies can be combined with schema conditionals to construct a wider variety of custom reduction behaviors. For example, here’s a reset-able counter: collections: - name: example/reductions/sum-reset schema: type: object properties: key: { type: string } value: { type: number } required: [key] # Use oneOf to express a tagged union over &quot;action&quot;. oneOf: # When action = reset, reduce by taking this document. - properties: { action: { const: reset } } reduce: { strategy: lastWriteWins } # When action = sum, reduce by summing &quot;value&quot;. Keep the LHS &quot;action&quot;, # preserving a LHS &quot;reset&quot;, so that resets are properly associative. - properties: action: const: sum reduce: { strategy: firstWriteWins } value: { reduce: { strategy: sum } } reduce: { strategy: merge } key: [/key] tests: &quot;Expect we can sum or reset numbers&quot;: - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: sum, value: 5 } - { key: &quot;key&quot;, action: sum, value: -1.2 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 3.8 } - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: reset, value: 0 } - { key: &quot;key&quot;, action: sum, value: 1.3 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 1.3 } ","keywords":""},{"title":"firstWriteWins and lastWriteWins","type":0,"sectionRef":"#","url":"reference/reduction-strategies/firstwritewins-and-lastwritewins/","content":"firstWriteWins and lastWriteWins firstWriteWins always takes the first value seen at the annotated location. Likewise, lastWriteWins always takes the last. Schemas that don’t have an explicit reduce annotation default to lastWriteWins behavior. collections: - name: example/reductions/fww-lww schema: type: object reduce: { strategy: merge } properties: key: { type: string } fww: { reduce: { strategy: firstWriteWins } } lww: { reduce: { strategy: lastWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can track first- and list-written values&quot;: - ingest: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;one&quot; } - { key: &quot;key&quot;, fww: &quot;two&quot;, lww: &quot;two&quot; } - verify: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;two&quot; } ","keywords":""},{"title":"merge","type":0,"sectionRef":"#","url":"reference/reduction-strategies/merge/","content":"merge merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays. If both sides are objects, merge performs a deep merge of each property. If LHS and RHS are both arrays, items at each index of both sides are merged together, extending the shorter of the two sides by taking items off the longer: collections: - name: example/reductions/merge schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Merge only works with types &quot;array&quot; or &quot;object&quot;. # Others will throw an error at build time. type: [array, object] reduce: { strategy: merge } # Deeply merge sub-locations (items or properties) by summing them. items: type: number reduce: { strategy: sum } additionalProperties: type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can merge arrays by index&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [1, 1] } - { key: &quot;key&quot;, value: [2, 2, 2] } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [3, 3, 2] } &quot;Expect we can merge objects by property&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;b&quot;: 1 } } - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;c&quot;: 1 } } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 2, &quot;b&quot;: 1, &quot;c&quot;: 1 } } Merge may also take a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. Arrays must be pre-sorted and de-duplicated by the key, and merge itself always maintains this invariant. Note that you can use a key of [“”] for natural item ordering, such as merging sorted arrays of scalars. collections: - name: example/reductions/merge-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: type: array reduce: strategy: merge key: [/k] items: { reduce: { strategy: firstWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can merge sorted arrays&quot;: - ingest: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }] } - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 2 }, { k: &quot;c&quot;, v: 2 }] } - verify: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }, { k: &quot;c&quot;, v: 2 }], } As with append, the LHS of merge may be null, in which case the reduction is treated as a no-op and its result remains null.","keywords":""},{"title":"minimize and maximize","type":0,"sectionRef":"#","url":"reference/reduction-strategies/minimize-and-maximize/","content":"minimize and maximize minimize and maximize reduce by taking the smallest or largest seen value, respectively. collections: - name: example/reductions/min-max schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: { reduce: { strategy: minimize } } max: { reduce: { strategy: maximize } } required: [key] key: [/key] tests: &quot;Expect we can min/max values&quot;: - ingest: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;abc&quot; } - { key: &quot;key&quot;, min: 42, max: &quot;def&quot; } - verify: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;def&quot; } minimize and maximize can also take a key, which is one or more JSON pointers that are relative to the reduced location. Keys make it possible to minimize and maximize over complex types by ordering over an extracted composite key. In the event that a right-hand side document key equals the current left-hand side minimum or maximum, the documents are deeply merged. This can be used to, for example, track not just the minimum value but also the number of times it’s been seen: collections: - name: example/reductions/min-max-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: $anchor: min-max-value type: array items: - type: string - type: number reduce: { strategy: sum } reduce: strategy: minimize key: [/0] max: $ref: &quot;#min-max-value&quot; reduce: strategy: maximize key: [/0] required: [key] key: [/key] tests: &quot;Expect we can min/max values using a key extractor&quot;: - ingest: collection: example/reductions/min-max-key documents: - { key: &quot;key&quot;, min: [&quot;a&quot;, 1], max: [&quot;a&quot;, 1] } - { key: &quot;key&quot;, min: [&quot;c&quot;, 2], max: [&quot;c&quot;, 2] } - { key: &quot;key&quot;, min: [&quot;b&quot;, 3], max: [&quot;b&quot;, 3] } - { key: &quot;key&quot;, min: [&quot;a&quot;, 4], max: [&quot;a&quot;, 4] } - verify: collection: example/reductions/min-max-key documents: # Min of equal keys [&quot;a&quot;, 1] and [&quot;a&quot;, 4] =&gt; [&quot;a&quot;, 5]. - { key: &quot;key&quot;, min: [&quot;a&quot;, 5], max: [&quot;c&quot;, 2] } ","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"reference/reduction-strategies/sum/","content":"sum sum reduces two numbers or integers by adding their values. collections: - name: example/reductions/sum schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sum only works with types &quot;number&quot; or &quot;integer&quot;. # Others will throw an error at build time. type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can sum two numbers&quot;: - ingest: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 5 } - { key: &quot;key&quot;, value: -1.2 } - verify: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 3.8 } ","keywords":""},{"title":"set","type":0,"sectionRef":"#","url":"reference/reduction-strategies/set/","content":"set set interprets the document location as an update to a set. The location must be an object having only “add&quot;, “intersect&quot;, and “remove” properties. Any single “add&quot;, “intersect&quot;, or “remove” is always allowed. A document with “intersect” and “add” is allowed, and is interpreted as applying the intersection to the left-hand side set, followed by a union with the additions. A document with “remove” and “add” is also allowed, and is interpreted as applying the removals to the base set, followed by a union with the additions. “remove” and “intersect” within the same document are prohibited. Set additions are deeply merged. This makes sets behave like associative maps, where the “value” of a set member can be updated by adding it to the set again, with a reducible update. Sets may be objects, in which case the object property serves as the set item key: collections: - name: example/reductions/set schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: { strategy: set } # Schema for &quot;add&quot;, &quot;intersect&quot;, and &quot;remove&quot; properties # (each a map of keys and their associated sums): additionalProperties: type: object additionalProperties: type: number reduce: { strategy: sum } # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } required: [key] key: [/key] tests: &quot;Expect we can apply set operations to incrementally build associative maps&quot;: - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 } } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: { &quot;b&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;d&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 2, &quot;c&quot;: 1, &quot;d&quot;: 1 } } } - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: { &quot;a&quot;: 0, &quot;d&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;e&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 3, &quot;d&quot;: 1, &quot;e&quot;: 1 } } } Sets can also be sorted arrays, which are ordered using a provide key extractor. Keys are given as one or more JSON pointers, each relative to the item. As with merge, arrays must be pre-sorted and de-duplicated by the key, and set reductions always maintain this invariant. Use a key extractor of [“”] to apply the natural ordering of scalar values. Whether array or object types are used, the type must always be consistent across the “add” / “intersect” / “remove” terms of both sides of the reduction. collections: - name: example/reductions/set-array schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: strategy: set key: [/0] # Schema for &quot;add&quot;, &quot;intersect&quot;, &amp; &quot;remove&quot; properties # (each a sorted array of [key, sum] 2-tuples): additionalProperties: type: array # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } # Schema for contained [key, sum] 2-tuples: items: type: array items: - type: string - type: number reduce: { strategy: sum } reduce: { strategy: merge } required: [key] key: [/key] tests: ? &quot;Expect we can apply operations of sorted-array sets to incrementally build associative maps&quot; : - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;b&quot;, 1], [&quot;c&quot;, 1]] } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: [[&quot;b&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;d&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 2], [&quot;c&quot;, 1], [&quot;d&quot;, 1]] } } - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: [[&quot;a&quot;, 0], [&quot;d&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;e&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 3], [&quot;d&quot;, 1], [&quot;e&quot;, 1]] } } ","keywords":""},{"title":"Working with logs and statistics","type":0,"sectionRef":"#","url":"reference/working-logs-stats/","content":"","keywords":""},{"title":"Accessing logs​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs","content":"You can access logs by materializing them to an external endpoint, or from the command line. "},{"title":"Accessing logs from the command line​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-from-the-command-line","content":"Beta The flowctl logs subcommand is not currently available due to an ongoing overhaul of the flowctl binary. Command line support will be added back soon. Contact Estuary Support for more information. For now, use a materialization to view logs. The flowctl logs subcommand allows you to print logs from the command line. This method allows more flexibility and is ideal for debugging. You can retrieve logs for any task that is part of a catalog that is currently deployed. Printing logs for a specific task​ You can print logs for a given deployed task using the flag --task followed by the task name. flowctl logs --task acmeCo/anvils/capture-one  Printing all logs for a prefix​ You can print all logs for currently deployed catalogs of a given prefix using the flag --tenant. flowctl logs --tenant acmeCo  This is the same as printing the entire contents of the collection ops/acmeCo/logs. Printing logs by task type​ Within a given prefix, you can print logs for all deployed tasks of a given type using the flag --task-type followed by one of capture, derivation, or materialization. flowctl logs --tenant acmeCo --task-type capture  "},{"title":"Accessing logs by materialization​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-by-materialization","content":"You can materialize your logs collection to an external system. This is typically the preferred method if you’d like to continuously work with or monitor logs. It's easiest to materialize the whole collection, but you can use a partition selector to only materialize specific tasks, as the logs collection is partitioned on tasks. caution Be sure to add a partition selector to exclude the logs of the materialization itself. Otherwise, you could trigger an infinite loop in which the connector materializes its own logs, logs that event, and so on. acmeCo/anvils/logs: endpoint: connector: image: ghcr.io/estuary/materialize-webhook:dev config: address: my.webhook.com bindings: - resource: relativePath: /log/wordcount source: ops/acmeCo/logs # Exclude the logs of this materialization to avoid an infinite loop. partitions: exclude: name: ['acmeCo/anvils/logs']  "}]